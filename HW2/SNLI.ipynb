{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 3\n",
    "BIDIRECTIONAL = True\n",
    "NUM_DIRECTIONS = 1 if not BIDIRECTIONAL else 2\n",
    "RNN_HIDDEN_SIZE = 200\n",
    "LIN_HIDDEN_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('hw2_data/snli_train.tsv',sep='\\t')\n",
    "label_map = {'contradiction':0, 'entailment':2, 'neutral':1}\n",
    "train_data.replace({'label':label_map},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('hw2_data/snli_val.tsv',sep='\\t')\n",
    "label_map = {'contradiction':0, 'entailment':2, 'neutral':1}\n",
    "val_data.replace({'label':label_map},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data,val_data = train_test_split(data,test_size=0.2,random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df['sentence1'] = df['sentence1'].str.split()\n",
    "    df['sentence2'] = df['sentence2'].str.split()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(train_data)\n",
    "val_data = prepare_data(val_data)\n",
    "#emb_weights = pkl.load(open('emb_weights.pkl','rb'))\n",
    "#token2id = pkl.load(open('token2id.pkl','rb'))\n",
    "#id2token = pkl.load(open('id2token.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = int(train_data['sentence1'].str.len().quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname,vocab):  \n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    print(n)\n",
    "    print(d)\n",
    "    data = {}\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocab:\n",
    "            #convert map object to numpy ndarray\n",
    "            count+=1\n",
    "            print(count)\n",
    "            data[tokens[0]] = np.fromiter(map(float, tokens[1:]),dtype=np.float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_data,max_vocab_size=50000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    all_tokens = []\n",
    "    for i in range(len(train_data)):\n",
    "        all_tokens.extend(train_data.iloc[i]['sentence1'])\n",
    "        all_tokens.extend(train_data.iloc[i]['sentence2'])\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    print('Done building vocab')\n",
    "    vocab = list(vocab)\n",
    "    emb_weights = load_vectors('wiki-news-300d-1M.vec',vocab)\n",
    "    print('Done getting embedding weights')\n",
    "    vocab = [word for word in vocab if word in emb_weights] \n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token, emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building vocab\n",
      "999994\n",
      "300\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n",
      "5010\n",
      "5011\n",
      "5012\n",
      "5013\n",
      "5014\n",
      "5015\n",
      "5016\n",
      "5017\n",
      "5018\n",
      "5019\n",
      "5020\n",
      "5021\n",
      "5022\n",
      "5023\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5040\n",
      "5041\n",
      "5042\n",
      "5043\n",
      "5044\n",
      "5045\n",
      "5046\n",
      "5047\n",
      "5048\n",
      "5049\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5054\n",
      "5055\n",
      "5056\n",
      "5057\n",
      "5058\n",
      "5059\n",
      "5060\n",
      "5061\n",
      "5062\n",
      "5063\n",
      "5064\n",
      "5065\n",
      "5066\n",
      "5067\n",
      "5068\n",
      "5069\n",
      "5070\n",
      "5071\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5088\n",
      "5089\n",
      "5090\n",
      "5091\n",
      "5092\n",
      "5093\n",
      "5094\n",
      "5095\n",
      "5096\n",
      "5097\n",
      "5098\n",
      "5099\n",
      "5100\n",
      "5101\n",
      "5102\n",
      "5103\n",
      "5104\n",
      "5105\n",
      "5106\n",
      "5107\n",
      "5108\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5112\n",
      "5113\n",
      "5114\n",
      "5115\n",
      "5116\n",
      "5117\n",
      "5118\n",
      "5119\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5128\n",
      "5129\n",
      "5130\n",
      "5131\n",
      "5132\n",
      "5133\n",
      "5134\n",
      "5135\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "5144\n",
      "5145\n",
      "5146\n",
      "5147\n",
      "5148\n",
      "5149\n",
      "5150\n",
      "5151\n",
      "5152\n",
      "5153\n",
      "5154\n",
      "5155\n",
      "5156\n",
      "5157\n",
      "5158\n",
      "5159\n",
      "5160\n",
      "5161\n",
      "5162\n",
      "5163\n",
      "5164\n",
      "5165\n",
      "5166\n",
      "5167\n",
      "5168\n",
      "5169\n",
      "5170\n",
      "5171\n",
      "5172\n",
      "5173\n",
      "5174\n",
      "5175\n",
      "5176\n",
      "5177\n",
      "5178\n",
      "5179\n",
      "5180\n",
      "5181\n",
      "5182\n",
      "5183\n",
      "5184\n",
      "5185\n",
      "5186\n",
      "5187\n",
      "5188\n",
      "5189\n",
      "5190\n",
      "5191\n",
      "5192\n",
      "5193\n",
      "5194\n",
      "5195\n",
      "5196\n",
      "5197\n",
      "5198\n",
      "5199\n",
      "5200\n",
      "5201\n",
      "5202\n",
      "5203\n",
      "5204\n",
      "5205\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5209\n",
      "5210\n",
      "5211\n",
      "5212\n",
      "5213\n",
      "5214\n",
      "5215\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5232\n",
      "5233\n",
      "5234\n",
      "5235\n",
      "5236\n",
      "5237\n",
      "5238\n",
      "5239\n",
      "5240\n",
      "5241\n",
      "5242\n",
      "5243\n",
      "5244\n",
      "5245\n",
      "5246\n",
      "5247\n",
      "5248\n",
      "5249\n",
      "5250\n",
      "5251\n",
      "5252\n",
      "5253\n",
      "5254\n",
      "5255\n",
      "5256\n",
      "5257\n",
      "5258\n",
      "5259\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5263\n",
      "5264\n",
      "5265\n",
      "5266\n",
      "5267\n",
      "5268\n",
      "5269\n",
      "5270\n",
      "5271\n",
      "5272\n",
      "5273\n",
      "5274\n",
      "5275\n",
      "5276\n",
      "5277\n",
      "5278\n",
      "5279\n",
      "5280\n",
      "5281\n",
      "5282\n",
      "5283\n",
      "5284\n",
      "5285\n",
      "5286\n",
      "5287\n",
      "5288\n",
      "5289\n",
      "5290\n",
      "5291\n",
      "5292\n",
      "5293\n",
      "5294\n",
      "5295\n",
      "5296\n",
      "5297\n",
      "5298\n",
      "5299\n",
      "5300\n",
      "5301\n",
      "5302\n",
      "5303\n",
      "5304\n",
      "5305\n",
      "5306\n",
      "5307\n",
      "5308\n",
      "5309\n",
      "5310\n",
      "5311\n",
      "5312\n",
      "5313\n",
      "5314\n",
      "5315\n",
      "5316\n",
      "5317\n",
      "5318\n",
      "5319\n",
      "5320\n",
      "5321\n",
      "5322\n",
      "5323\n",
      "5324\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "5328\n",
      "5329\n",
      "5330\n",
      "5331\n",
      "5332\n",
      "5333\n",
      "5334\n",
      "5335\n",
      "5336\n",
      "5337\n",
      "5338\n",
      "5339\n",
      "5340\n",
      "5341\n",
      "5342\n",
      "5343\n",
      "5344\n",
      "5345\n",
      "5346\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5350\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5355\n",
      "5356\n",
      "5357\n",
      "5358\n",
      "5359\n",
      "5360\n",
      "5361\n",
      "5362\n",
      "5363\n",
      "5364\n",
      "5365\n",
      "5366\n",
      "5367\n",
      "5368\n",
      "5369\n",
      "5370\n",
      "5371\n",
      "5372\n",
      "5373\n",
      "5374\n",
      "5375\n",
      "5376\n",
      "5377\n",
      "5378\n",
      "5379\n",
      "5380\n",
      "5381\n",
      "5382\n",
      "5383\n",
      "5384\n",
      "5385\n",
      "5386\n",
      "5387\n",
      "5388\n",
      "5389\n",
      "5390\n",
      "5391\n",
      "5392\n",
      "5393\n",
      "5394\n",
      "5395\n",
      "5396\n",
      "5397\n",
      "5398\n",
      "5399\n",
      "5400\n",
      "5401\n",
      "5402\n",
      "5403\n",
      "5404\n",
      "5405\n",
      "5406\n",
      "5407\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5424\n",
      "5425\n",
      "5426\n",
      "5427\n",
      "5428\n",
      "5429\n",
      "5430\n",
      "5431\n",
      "5432\n",
      "5433\n",
      "5434\n",
      "5435\n",
      "5436\n",
      "5437\n",
      "5438\n",
      "5439\n",
      "5440\n",
      "5441\n",
      "5442\n",
      "5443\n",
      "5444\n",
      "5445\n",
      "5446\n",
      "5447\n",
      "5448\n",
      "5449\n",
      "5450\n",
      "5451\n",
      "5452\n",
      "5453\n",
      "5454\n",
      "5455\n",
      "5456\n",
      "5457\n",
      "5458\n",
      "5459\n",
      "5460\n",
      "5461\n",
      "5462\n",
      "5463\n",
      "5464\n",
      "5465\n",
      "5466\n",
      "5467\n",
      "5468\n",
      "5469\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5473\n",
      "5474\n",
      "5475\n",
      "5476\n",
      "5477\n",
      "5478\n",
      "5479\n",
      "5480\n",
      "5481\n",
      "5482\n",
      "5483\n",
      "5484\n",
      "5485\n",
      "5486\n",
      "5487\n",
      "5488\n",
      "5489\n",
      "5490\n",
      "5491\n",
      "5492\n",
      "5493\n",
      "5494\n",
      "5495\n",
      "5496\n",
      "5497\n",
      "5498\n",
      "5499\n",
      "5500\n",
      "5501\n",
      "5502\n",
      "5503\n",
      "5504\n",
      "5505\n",
      "5506\n",
      "5507\n",
      "5508\n",
      "5509\n",
      "5510\n",
      "5511\n",
      "5512\n",
      "5513\n",
      "5514\n",
      "5515\n",
      "5516\n",
      "5517\n",
      "5518\n",
      "5519\n",
      "5520\n",
      "5521\n",
      "5522\n",
      "5523\n",
      "5524\n",
      "5525\n",
      "5526\n",
      "5527\n",
      "5528\n",
      "5529\n",
      "5530\n",
      "5531\n",
      "5532\n",
      "5533\n",
      "5534\n",
      "5535\n",
      "5536\n",
      "5537\n",
      "5538\n",
      "5539\n",
      "5540\n",
      "5541\n",
      "5542\n",
      "5543\n",
      "5544\n",
      "5545\n",
      "5546\n",
      "5547\n",
      "5548\n",
      "5549\n",
      "5550\n",
      "5551\n",
      "5552\n",
      "5553\n",
      "5554\n",
      "5555\n",
      "5556\n",
      "5557\n",
      "5558\n",
      "5559\n",
      "5560\n",
      "5561\n",
      "5562\n",
      "5563\n",
      "5564\n",
      "5565\n",
      "5566\n",
      "5567\n",
      "5568\n",
      "5569\n",
      "5570\n",
      "5571\n",
      "5572\n",
      "5573\n",
      "5574\n",
      "5575\n",
      "5576\n",
      "5577\n",
      "5578\n",
      "5579\n",
      "5580\n",
      "5581\n",
      "5582\n",
      "5583\n",
      "5584\n",
      "5585\n",
      "5586\n",
      "5587\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5594\n",
      "5595\n",
      "5596\n",
      "5597\n",
      "5598\n",
      "5599\n",
      "5600\n",
      "5601\n",
      "5602\n",
      "5603\n",
      "5604\n",
      "5605\n",
      "5606\n",
      "5607\n",
      "5608\n",
      "5609\n",
      "5610\n",
      "5611\n",
      "5612\n",
      "5613\n",
      "5614\n",
      "5615\n",
      "5616\n",
      "5617\n",
      "5618\n",
      "5619\n",
      "5620\n",
      "5621\n",
      "5622\n",
      "5623\n",
      "5624\n",
      "5625\n",
      "5626\n",
      "5627\n",
      "5628\n",
      "5629\n",
      "5630\n",
      "5631\n",
      "5632\n",
      "5633\n",
      "5634\n",
      "5635\n",
      "5636\n",
      "5637\n",
      "5638\n",
      "5639\n",
      "5640\n",
      "5641\n",
      "5642\n",
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6032\n",
      "6033\n",
      "6034\n",
      "6035\n",
      "6036\n",
      "6037\n",
      "6038\n",
      "6039\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6043\n",
      "6044\n",
      "6045\n",
      "6046\n",
      "6047\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6064\n",
      "6065\n",
      "6066\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6071\n",
      "6072\n",
      "6073\n",
      "6074\n",
      "6075\n",
      "6076\n",
      "6077\n",
      "6078\n",
      "6079\n",
      "6080\n",
      "6081\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6086\n",
      "6087\n",
      "6088\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6094\n",
      "6095\n",
      "6096\n",
      "6097\n",
      "6098\n",
      "6099\n",
      "6100\n",
      "6101\n",
      "6102\n",
      "6103\n",
      "6104\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6108\n",
      "6109\n",
      "6110\n",
      "6111\n",
      "6112\n",
      "6113\n",
      "6114\n",
      "6115\n",
      "6116\n",
      "6117\n",
      "6118\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6122\n",
      "6123\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6127\n",
      "6128\n",
      "6129\n",
      "6130\n",
      "6131\n",
      "6132\n",
      "6133\n",
      "6134\n",
      "6135\n",
      "6136\n",
      "6137\n",
      "6138\n",
      "6139\n",
      "6140\n",
      "6141\n",
      "6142\n",
      "6143\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6152\n",
      "6153\n",
      "6154\n",
      "6155\n",
      "6156\n",
      "6157\n",
      "6158\n",
      "6159\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "6168\n",
      "6169\n",
      "6170\n",
      "6171\n",
      "6172\n",
      "6173\n",
      "6174\n",
      "6175\n",
      "6176\n",
      "6177\n",
      "6178\n",
      "6179\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6183\n",
      "6184\n",
      "6185\n",
      "6186\n",
      "6187\n",
      "6188\n",
      "6189\n",
      "6190\n",
      "6191\n",
      "6192\n",
      "6193\n",
      "6194\n",
      "6195\n",
      "6196\n",
      "6197\n",
      "6198\n",
      "6199\n",
      "6200\n",
      "6201\n",
      "6202\n",
      "6203\n",
      "6204\n",
      "6205\n",
      "6206\n",
      "6207\n",
      "6208\n",
      "6209\n",
      "6210\n",
      "6211\n",
      "6212\n",
      "6213\n",
      "6214\n",
      "6215\n",
      "6216\n",
      "6217\n",
      "6218\n",
      "6219\n",
      "6220\n",
      "6221\n",
      "6222\n",
      "6223\n",
      "6224\n",
      "6225\n",
      "6226\n",
      "6227\n",
      "6228\n",
      "6229\n",
      "6230\n",
      "6231\n",
      "6232\n",
      "6233\n",
      "6234\n",
      "6235\n",
      "6236\n",
      "6237\n",
      "6238\n",
      "6239\n",
      "6240\n",
      "6241\n",
      "6242\n",
      "6243\n",
      "6244\n",
      "6245\n",
      "6246\n",
      "6247\n",
      "6248\n",
      "6249\n",
      "6250\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6255\n",
      "6256\n",
      "6257\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6264\n",
      "6265\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6274\n",
      "6275\n",
      "6276\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6281\n",
      "6282\n",
      "6283\n",
      "6284\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6288\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6293\n",
      "6294\n",
      "6295\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6304\n",
      "6305\n",
      "6306\n",
      "6307\n",
      "6308\n",
      "6309\n",
      "6310\n",
      "6311\n",
      "6312\n",
      "6313\n",
      "6314\n",
      "6315\n",
      "6316\n",
      "6317\n",
      "6318\n",
      "6319\n",
      "6320\n",
      "6321\n",
      "6322\n",
      "6323\n",
      "6324\n",
      "6325\n",
      "6326\n",
      "6327\n",
      "6328\n",
      "6329\n",
      "6330\n",
      "6331\n",
      "6332\n",
      "6333\n",
      "6334\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6338\n",
      "6339\n",
      "6340\n",
      "6341\n",
      "6342\n",
      "6343\n",
      "6344\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6348\n",
      "6349\n",
      "6350\n",
      "6351\n",
      "6352\n",
      "6353\n",
      "6354\n",
      "6355\n",
      "6356\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6360\n",
      "6361\n",
      "6362\n",
      "6363\n",
      "6364\n",
      "6365\n",
      "6366\n",
      "6367\n",
      "6368\n",
      "6369\n",
      "6370\n",
      "6371\n",
      "6372\n",
      "6373\n",
      "6374\n",
      "6375\n",
      "6376\n",
      "6377\n",
      "6378\n",
      "6379\n",
      "6380\n",
      "6381\n",
      "6382\n",
      "6383\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6388\n",
      "6389\n",
      "6390\n",
      "6391\n",
      "6392\n",
      "6393\n",
      "6394\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6398\n",
      "6399\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6403\n",
      "6404\n",
      "6405\n",
      "6406\n",
      "6407\n",
      "6408\n",
      "6409\n",
      "6410\n",
      "6411\n",
      "6412\n",
      "6413\n",
      "6414\n",
      "6415\n",
      "6416\n",
      "6417\n",
      "6418\n",
      "6419\n",
      "6420\n",
      "6421\n",
      "6422\n",
      "6423\n",
      "6424\n",
      "6425\n",
      "6426\n",
      "6427\n",
      "6428\n",
      "6429\n",
      "6430\n",
      "6431\n",
      "6432\n",
      "6433\n",
      "6434\n",
      "6435\n",
      "6436\n",
      "6437\n",
      "6438\n",
      "6439\n",
      "6440\n",
      "6441\n",
      "6442\n",
      "6443\n",
      "6444\n",
      "6445\n",
      "6446\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6450\n",
      "6451\n",
      "6452\n",
      "6453\n",
      "6454\n",
      "6455\n",
      "6456\n",
      "6457\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6463\n",
      "6464\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6468\n",
      "6469\n",
      "6470\n",
      "6471\n",
      "6472\n",
      "6473\n",
      "6474\n",
      "6475\n",
      "6476\n",
      "6477\n",
      "6478\n",
      "6479\n",
      "6480\n",
      "6481\n",
      "6482\n",
      "6483\n",
      "6484\n",
      "6485\n",
      "6486\n",
      "6487\n",
      "6488\n",
      "6489\n",
      "6490\n",
      "6491\n",
      "6492\n",
      "6493\n",
      "6494\n",
      "6495\n",
      "6496\n",
      "6497\n",
      "6498\n",
      "6499\n",
      "6500\n",
      "6501\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6505\n",
      "6506\n",
      "6507\n",
      "6508\n",
      "6509\n",
      "6510\n",
      "6511\n",
      "6512\n",
      "6513\n",
      "6514\n",
      "6515\n",
      "6516\n",
      "6517\n",
      "6518\n",
      "6519\n",
      "6520\n",
      "6521\n",
      "6522\n",
      "6523\n",
      "6524\n",
      "6525\n",
      "6526\n",
      "6527\n",
      "6528\n",
      "6529\n",
      "6530\n",
      "6531\n",
      "6532\n",
      "6533\n",
      "6534\n",
      "6535\n",
      "6536\n",
      "6537\n",
      "6538\n",
      "6539\n",
      "6540\n",
      "6541\n",
      "6542\n",
      "6543\n",
      "6544\n",
      "6545\n",
      "6546\n",
      "6547\n",
      "6548\n",
      "6549\n",
      "6550\n",
      "6551\n",
      "6552\n",
      "6553\n",
      "6554\n",
      "6555\n",
      "6556\n",
      "6557\n",
      "6558\n",
      "6559\n",
      "6560\n",
      "6561\n",
      "6562\n",
      "6563\n",
      "6564\n",
      "6565\n",
      "6566\n",
      "6567\n",
      "6568\n",
      "6569\n",
      "6570\n",
      "6571\n",
      "6572\n",
      "6573\n",
      "6574\n",
      "6575\n",
      "6576\n",
      "6577\n",
      "6578\n",
      "6579\n",
      "6580\n",
      "6581\n",
      "6582\n",
      "6583\n",
      "6584\n",
      "6585\n",
      "6586\n",
      "6587\n",
      "6588\n",
      "6589\n",
      "6590\n",
      "6591\n",
      "6592\n",
      "6593\n",
      "6594\n",
      "6595\n",
      "6596\n",
      "6597\n",
      "6598\n",
      "6599\n",
      "6600\n",
      "6601\n",
      "6602\n",
      "6603\n",
      "6604\n",
      "6605\n",
      "6606\n",
      "6607\n",
      "6608\n",
      "6609\n",
      "6610\n",
      "6611\n",
      "6612\n",
      "6613\n",
      "6614\n",
      "6615\n",
      "6616\n",
      "6617\n",
      "6618\n",
      "6619\n",
      "6620\n",
      "6621\n",
      "6622\n",
      "6623\n",
      "6624\n",
      "6625\n",
      "6626\n",
      "6627\n",
      "6628\n",
      "6629\n",
      "6630\n",
      "6631\n",
      "6632\n",
      "6633\n",
      "6634\n",
      "6635\n",
      "6636\n",
      "6637\n",
      "6638\n",
      "6639\n",
      "6640\n",
      "6641\n",
      "6642\n",
      "6643\n",
      "6644\n",
      "6645\n",
      "6646\n",
      "6647\n",
      "6648\n",
      "6649\n",
      "6650\n",
      "6651\n",
      "6652\n",
      "6653\n",
      "6654\n",
      "6655\n",
      "6656\n",
      "6657\n",
      "6658\n",
      "6659\n",
      "6660\n",
      "6661\n",
      "6662\n",
      "6663\n",
      "6664\n",
      "6665\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6670\n",
      "6671\n",
      "6672\n",
      "6673\n",
      "6674\n",
      "6675\n",
      "6676\n",
      "6677\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "6689\n",
      "6690\n",
      "6691\n",
      "6692\n",
      "6693\n",
      "6694\n",
      "6695\n",
      "6696\n",
      "6697\n",
      "6698\n",
      "6699\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6703\n",
      "6704\n",
      "6705\n",
      "6706\n",
      "6707\n",
      "6708\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6712\n",
      "6713\n",
      "6714\n",
      "6715\n",
      "6716\n",
      "6717\n",
      "6718\n",
      "6719\n",
      "6720\n",
      "6721\n",
      "6722\n",
      "6723\n",
      "6724\n",
      "6725\n",
      "6726\n",
      "6727\n",
      "6728\n",
      "6729\n",
      "6730\n",
      "6731\n",
      "6732\n",
      "6733\n",
      "6734\n",
      "6735\n",
      "6736\n",
      "6737\n",
      "6738\n",
      "6739\n",
      "6740\n",
      "6741\n",
      "6742\n",
      "6743\n",
      "6744\n",
      "6745\n",
      "6746\n",
      "6747\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6751\n",
      "6752\n",
      "6753\n",
      "6754\n",
      "6755\n",
      "6756\n",
      "6757\n",
      "6758\n",
      "6759\n",
      "6760\n",
      "6761\n",
      "6762\n",
      "6763\n",
      "6764\n",
      "6765\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6772\n",
      "6773\n",
      "6774\n",
      "6775\n",
      "6776\n",
      "6777\n",
      "6778\n",
      "6779\n",
      "6780\n",
      "6781\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "6791\n",
      "6792\n",
      "6793\n",
      "6794\n",
      "6795\n",
      "6796\n",
      "6797\n",
      "6798\n",
      "6799\n",
      "6800\n",
      "6801\n",
      "6802\n",
      "6803\n",
      "6804\n",
      "6805\n",
      "6806\n",
      "6807\n",
      "6808\n",
      "6809\n",
      "6810\n",
      "6811\n",
      "6812\n",
      "6813\n",
      "6814\n",
      "6815\n",
      "6816\n",
      "6817\n",
      "6818\n",
      "6819\n",
      "6820\n",
      "6821\n",
      "6822\n",
      "6823\n",
      "6824\n",
      "6825\n",
      "6826\n",
      "6827\n",
      "6828\n",
      "6829\n",
      "6830\n",
      "6831\n",
      "6832\n",
      "6833\n",
      "6834\n",
      "6835\n",
      "6836\n",
      "6837\n",
      "6838\n",
      "6839\n",
      "6840\n",
      "6841\n",
      "6842\n",
      "6843\n",
      "6844\n",
      "6845\n",
      "6846\n",
      "6847\n",
      "6848\n",
      "6849\n",
      "6850\n",
      "6851\n",
      "6852\n",
      "6853\n",
      "6854\n",
      "6855\n",
      "6856\n",
      "6857\n",
      "6858\n",
      "6859\n",
      "6860\n",
      "6861\n",
      "6862\n",
      "6863\n",
      "6864\n",
      "6865\n",
      "6866\n",
      "6867\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6871\n",
      "6872\n",
      "6873\n",
      "6874\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6879\n",
      "6880\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6885\n",
      "6886\n",
      "6887\n",
      "6888\n",
      "6889\n",
      "6890\n",
      "6891\n",
      "6892\n",
      "6893\n",
      "6894\n",
      "6895\n",
      "6896\n",
      "6897\n",
      "6898\n",
      "6899\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6903\n",
      "6904\n",
      "6905\n",
      "6906\n",
      "6907\n",
      "6908\n",
      "6909\n",
      "6910\n",
      "6911\n",
      "6912\n",
      "6913\n",
      "6914\n",
      "6915\n",
      "6916\n",
      "6917\n",
      "6918\n",
      "6919\n",
      "6920\n",
      "6921\n",
      "6922\n",
      "6923\n",
      "6924\n",
      "6925\n",
      "6926\n",
      "6927\n",
      "6928\n",
      "6929\n",
      "6930\n",
      "6931\n",
      "6932\n",
      "6933\n",
      "6934\n",
      "6935\n",
      "6936\n",
      "6937\n",
      "6938\n",
      "6939\n",
      "6940\n",
      "6941\n",
      "6942\n",
      "6943\n",
      "6944\n",
      "6945\n",
      "6946\n",
      "6947\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6956\n",
      "6957\n",
      "6958\n",
      "6959\n",
      "6960\n",
      "6961\n",
      "6962\n",
      "6963\n",
      "6964\n",
      "6965\n",
      "6966\n",
      "6967\n",
      "6968\n",
      "6969\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6973\n",
      "6974\n",
      "6975\n",
      "6976\n",
      "6977\n",
      "6978\n",
      "6979\n",
      "6980\n",
      "6981\n",
      "6982\n",
      "6983\n",
      "6984\n",
      "6985\n",
      "6986\n",
      "6987\n",
      "6988\n",
      "6989\n",
      "6990\n",
      "6991\n",
      "6992\n",
      "6993\n",
      "6994\n",
      "6995\n",
      "6996\n",
      "6997\n",
      "6998\n",
      "6999\n",
      "7000\n",
      "7001\n",
      "7002\n",
      "7003\n",
      "7004\n",
      "7005\n",
      "7006\n",
      "7007\n",
      "7008\n",
      "7009\n",
      "7010\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7015\n",
      "7016\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7022\n",
      "7023\n",
      "7024\n",
      "7025\n",
      "7026\n",
      "7027\n",
      "7028\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7033\n",
      "7034\n",
      "7035\n",
      "7036\n",
      "7037\n",
      "7038\n",
      "7039\n",
      "7040\n",
      "7041\n",
      "7042\n",
      "7043\n",
      "7044\n",
      "7045\n",
      "7046\n",
      "7047\n",
      "7048\n",
      "7049\n",
      "7050\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7054\n",
      "7055\n",
      "7056\n",
      "7057\n",
      "7058\n",
      "7059\n",
      "7060\n",
      "7061\n",
      "7062\n",
      "7063\n",
      "7064\n",
      "7065\n",
      "7066\n",
      "7067\n",
      "7068\n",
      "7069\n",
      "7070\n",
      "7071\n",
      "7072\n",
      "7073\n",
      "7074\n",
      "7075\n",
      "7076\n",
      "7077\n",
      "7078\n",
      "7079\n",
      "7080\n",
      "7081\n",
      "7082\n",
      "7083\n",
      "7084\n",
      "7085\n",
      "7086\n",
      "7087\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7127\n",
      "7128\n",
      "7129\n",
      "7130\n",
      "7131\n",
      "7132\n",
      "7133\n",
      "7134\n",
      "7135\n",
      "7136\n",
      "7137\n",
      "7138\n",
      "7139\n",
      "7140\n",
      "7141\n",
      "7142\n",
      "7143\n",
      "7144\n",
      "7145\n",
      "7146\n",
      "7147\n",
      "7148\n",
      "7149\n",
      "7150\n",
      "7151\n",
      "7152\n",
      "7153\n",
      "7154\n",
      "7155\n",
      "7156\n",
      "7157\n",
      "7158\n",
      "7159\n",
      "7160\n",
      "7161\n",
      "7162\n",
      "7163\n",
      "7164\n",
      "7165\n",
      "7166\n",
      "7167\n",
      "7168\n",
      "7169\n",
      "7170\n",
      "7171\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7175\n",
      "7176\n",
      "7177\n",
      "7178\n",
      "7179\n",
      "7180\n",
      "7181\n",
      "7182\n",
      "7183\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7187\n",
      "7188\n",
      "7189\n",
      "7190\n",
      "7191\n",
      "7192\n",
      "7193\n",
      "7194\n",
      "7195\n",
      "7196\n",
      "7197\n",
      "7198\n",
      "7199\n",
      "7200\n",
      "7201\n",
      "7202\n",
      "7203\n",
      "7204\n",
      "7205\n",
      "7206\n",
      "7207\n",
      "7208\n",
      "7209\n",
      "7210\n",
      "7211\n",
      "7212\n",
      "7213\n",
      "7214\n",
      "7215\n",
      "7216\n",
      "7217\n",
      "7218\n",
      "7219\n",
      "7220\n",
      "7221\n",
      "7222\n",
      "7223\n",
      "7224\n",
      "7225\n",
      "7226\n",
      "7227\n",
      "7228\n",
      "7229\n",
      "7230\n",
      "7231\n",
      "7232\n",
      "7233\n",
      "7234\n",
      "7235\n",
      "7236\n",
      "7237\n",
      "7238\n",
      "7239\n",
      "7240\n",
      "7241\n",
      "7242\n",
      "7243\n",
      "7244\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7248\n",
      "7249\n",
      "7250\n",
      "7251\n",
      "7252\n",
      "7253\n",
      "7254\n",
      "7255\n",
      "7256\n",
      "7257\n",
      "7258\n",
      "7259\n",
      "7260\n",
      "7261\n",
      "7262\n",
      "7263\n",
      "7264\n",
      "7265\n",
      "7266\n",
      "7267\n",
      "7268\n",
      "7269\n",
      "7270\n",
      "7271\n",
      "7272\n",
      "7273\n",
      "7274\n",
      "7275\n",
      "7276\n",
      "7277\n",
      "7278\n",
      "7279\n",
      "7280\n",
      "7281\n",
      "7282\n",
      "7283\n",
      "7284\n",
      "7285\n",
      "7286\n",
      "7287\n",
      "7288\n",
      "7289\n",
      "7290\n",
      "7291\n",
      "7292\n",
      "7293\n",
      "7294\n",
      "7295\n",
      "7296\n",
      "7297\n",
      "7298\n",
      "7299\n",
      "7300\n",
      "7301\n",
      "7302\n",
      "7303\n",
      "7304\n",
      "7305\n",
      "7306\n",
      "7307\n",
      "7308\n",
      "7309\n",
      "7310\n",
      "7311\n",
      "7312\n",
      "7313\n",
      "7314\n",
      "7315\n",
      "7316\n",
      "7317\n",
      "7318\n",
      "7319\n",
      "7320\n",
      "7321\n",
      "7322\n",
      "7323\n",
      "7324\n",
      "7325\n",
      "7326\n",
      "7327\n",
      "7328\n",
      "7329\n",
      "7330\n",
      "7331\n",
      "7332\n",
      "7333\n",
      "7334\n",
      "7335\n",
      "7336\n",
      "7337\n",
      "7338\n",
      "7339\n",
      "7340\n",
      "7341\n",
      "7342\n",
      "7343\n",
      "7344\n",
      "7345\n",
      "7346\n",
      "7347\n",
      "7348\n",
      "7349\n",
      "7350\n",
      "7351\n",
      "7352\n",
      "7353\n",
      "7354\n",
      "7355\n",
      "7356\n",
      "7357\n",
      "7358\n",
      "7359\n",
      "7360\n",
      "7361\n",
      "7362\n",
      "7363\n",
      "7364\n",
      "7365\n",
      "7366\n",
      "7367\n",
      "7368\n",
      "7369\n",
      "7370\n",
      "7371\n",
      "7372\n",
      "7373\n",
      "7374\n",
      "7375\n",
      "7376\n",
      "7377\n",
      "7378\n",
      "7379\n",
      "7380\n",
      "7381\n",
      "7382\n",
      "7383\n",
      "7384\n",
      "7385\n",
      "7386\n",
      "7387\n",
      "7388\n",
      "7389\n",
      "7390\n",
      "7391\n",
      "7392\n",
      "7393\n",
      "7394\n",
      "7395\n",
      "7396\n",
      "7397\n",
      "7398\n",
      "7399\n",
      "7400\n",
      "7401\n",
      "7402\n",
      "7403\n",
      "7404\n",
      "7405\n",
      "7406\n",
      "7407\n",
      "7408\n",
      "7409\n",
      "7410\n",
      "7411\n",
      "7412\n",
      "7413\n",
      "7414\n",
      "7415\n",
      "7416\n",
      "7417\n",
      "7418\n",
      "7419\n",
      "7420\n",
      "7421\n",
      "7422\n",
      "7423\n",
      "7424\n",
      "7425\n",
      "7426\n",
      "7427\n",
      "7428\n",
      "7429\n",
      "7430\n",
      "7431\n",
      "7432\n",
      "7433\n",
      "7434\n",
      "7435\n",
      "7436\n",
      "7437\n",
      "7438\n",
      "7439\n",
      "7440\n",
      "7441\n",
      "7442\n",
      "7443\n",
      "7444\n",
      "7445\n",
      "7446\n",
      "7447\n",
      "7448\n",
      "7449\n",
      "7450\n",
      "7451\n",
      "7452\n",
      "7453\n",
      "7454\n",
      "7455\n",
      "7456\n",
      "7457\n",
      "7458\n",
      "7459\n",
      "7460\n",
      "7461\n",
      "7462\n",
      "7463\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7467\n",
      "7468\n",
      "7469\n",
      "7470\n",
      "7471\n",
      "7472\n",
      "7473\n",
      "7474\n",
      "7475\n",
      "7476\n",
      "7477\n",
      "7478\n",
      "7479\n",
      "7480\n",
      "7481\n",
      "7482\n",
      "7483\n",
      "7484\n",
      "7485\n",
      "7486\n",
      "7487\n",
      "7488\n",
      "7489\n",
      "7490\n",
      "7491\n",
      "7492\n",
      "7493\n",
      "7494\n",
      "7495\n",
      "7496\n",
      "7497\n",
      "7498\n",
      "7499\n",
      "7500\n",
      "7501\n",
      "7502\n",
      "7503\n",
      "7504\n",
      "7505\n",
      "7506\n",
      "7507\n",
      "7508\n",
      "7509\n",
      "7510\n",
      "7511\n",
      "7512\n",
      "7513\n",
      "7514\n",
      "7515\n",
      "7516\n",
      "7517\n",
      "7518\n",
      "7519\n",
      "7520\n",
      "7521\n",
      "7522\n",
      "7523\n",
      "7524\n",
      "7525\n",
      "7526\n",
      "7527\n",
      "7528\n",
      "7529\n",
      "7530\n",
      "7531\n",
      "7532\n",
      "7533\n",
      "7534\n",
      "7535\n",
      "7536\n",
      "7537\n",
      "7538\n",
      "7539\n",
      "7540\n",
      "7541\n",
      "7542\n",
      "7543\n",
      "7544\n",
      "7545\n",
      "7546\n",
      "7547\n",
      "7548\n",
      "7549\n",
      "7550\n",
      "7551\n",
      "7552\n",
      "7553\n",
      "7554\n",
      "7555\n",
      "7556\n",
      "7557\n",
      "7558\n",
      "7559\n",
      "7560\n",
      "7561\n",
      "7562\n",
      "7563\n",
      "7564\n",
      "7565\n",
      "7566\n",
      "7567\n",
      "7568\n",
      "7569\n",
      "7570\n",
      "7571\n",
      "7572\n",
      "7573\n",
      "7574\n",
      "7575\n",
      "7576\n",
      "7577\n",
      "7578\n",
      "7579\n",
      "7580\n",
      "7581\n",
      "7582\n",
      "7583\n",
      "7584\n",
      "7585\n",
      "7586\n",
      "7587\n",
      "7588\n",
      "7589\n",
      "7590\n",
      "7591\n",
      "7592\n",
      "7593\n",
      "7594\n",
      "7595\n",
      "7596\n",
      "7597\n",
      "7598\n",
      "7599\n",
      "7600\n",
      "7601\n",
      "7602\n",
      "7603\n",
      "7604\n",
      "7605\n",
      "7606\n",
      "7607\n",
      "7608\n",
      "7609\n",
      "7610\n",
      "7611\n",
      "7612\n",
      "7613\n",
      "7614\n",
      "7615\n",
      "7616\n",
      "7617\n",
      "7618\n",
      "7619\n",
      "7620\n",
      "7621\n",
      "7622\n",
      "7623\n",
      "7624\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7629\n",
      "7630\n",
      "7631\n",
      "7632\n",
      "7633\n",
      "7634\n",
      "7635\n",
      "7636\n",
      "7637\n",
      "7638\n",
      "7639\n",
      "7640\n",
      "7641\n",
      "7642\n",
      "7643\n",
      "7644\n",
      "7645\n",
      "7646\n",
      "7647\n",
      "7648\n",
      "7649\n",
      "7650\n",
      "7651\n",
      "7652\n",
      "7653\n",
      "7654\n",
      "7655\n",
      "7656\n",
      "7657\n",
      "7658\n",
      "7659\n",
      "7660\n",
      "7661\n",
      "7662\n",
      "7663\n",
      "7664\n",
      "7665\n",
      "7666\n",
      "7667\n",
      "7668\n",
      "7669\n",
      "7670\n",
      "7671\n",
      "7672\n",
      "7673\n",
      "7674\n",
      "7675\n",
      "7676\n",
      "7677\n",
      "7678\n",
      "7679\n",
      "7680\n",
      "7681\n",
      "7682\n",
      "7683\n",
      "7684\n",
      "7685\n",
      "7686\n",
      "7687\n",
      "7688\n",
      "7689\n",
      "7690\n",
      "7691\n",
      "7692\n",
      "7693\n",
      "7694\n",
      "7695\n",
      "7696\n",
      "7697\n",
      "7698\n",
      "7699\n",
      "7700\n",
      "7701\n",
      "7702\n",
      "7703\n",
      "7704\n",
      "7705\n",
      "7706\n",
      "7707\n",
      "7708\n",
      "7709\n",
      "7710\n",
      "7711\n",
      "7712\n",
      "7713\n",
      "7714\n",
      "7715\n",
      "7716\n",
      "7717\n",
      "7718\n",
      "7719\n",
      "7720\n",
      "7721\n",
      "7722\n",
      "7723\n",
      "7724\n",
      "7725\n",
      "7726\n",
      "7727\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7732\n",
      "7733\n",
      "7734\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7738\n",
      "7739\n",
      "7740\n",
      "7741\n",
      "7742\n",
      "7743\n",
      "7744\n",
      "7745\n",
      "7746\n",
      "7747\n",
      "7748\n",
      "7749\n",
      "7750\n",
      "7751\n",
      "7752\n",
      "7753\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7762\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7766\n",
      "7767\n",
      "7768\n",
      "7769\n",
      "7770\n",
      "7771\n",
      "7772\n",
      "7773\n",
      "7774\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7778\n",
      "7779\n",
      "7780\n",
      "7781\n",
      "7782\n",
      "7783\n",
      "7784\n",
      "7785\n",
      "7786\n",
      "7787\n",
      "7788\n",
      "7789\n",
      "7790\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7795\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7799\n",
      "7800\n",
      "7801\n",
      "7802\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7808\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7812\n",
      "7813\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7820\n",
      "7821\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7828\n",
      "7829\n",
      "7830\n",
      "7831\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7839\n",
      "7840\n",
      "7841\n",
      "7842\n",
      "7843\n",
      "7844\n",
      "7845\n",
      "7846\n",
      "7847\n",
      "7848\n",
      "7849\n",
      "7850\n",
      "7851\n",
      "7852\n",
      "7853\n",
      "7854\n",
      "7855\n",
      "7856\n",
      "7857\n",
      "7858\n",
      "7859\n",
      "7860\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "7871\n",
      "7872\n",
      "7873\n",
      "7874\n",
      "7875\n",
      "7876\n",
      "7877\n",
      "7878\n",
      "7879\n",
      "7880\n",
      "7881\n",
      "7882\n",
      "7883\n",
      "7884\n",
      "7885\n",
      "7886\n",
      "7887\n",
      "7888\n",
      "7889\n",
      "7890\n",
      "7891\n",
      "7892\n",
      "7893\n",
      "7894\n",
      "7895\n",
      "7896\n",
      "7897\n",
      "7898\n",
      "7899\n",
      "7900\n",
      "7901\n",
      "7902\n",
      "7903\n",
      "7904\n",
      "7905\n",
      "7906\n",
      "7907\n",
      "7908\n",
      "7909\n",
      "7910\n",
      "7911\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7915\n",
      "7916\n",
      "7917\n",
      "7918\n",
      "7919\n",
      "7920\n",
      "7921\n",
      "7922\n",
      "7923\n",
      "7924\n",
      "7925\n",
      "7926\n",
      "7927\n",
      "7928\n",
      "7929\n",
      "7930\n",
      "7931\n",
      "7932\n",
      "7933\n",
      "7934\n",
      "7935\n",
      "7936\n",
      "7937\n",
      "7938\n",
      "7939\n",
      "7940\n",
      "7941\n",
      "7942\n",
      "7943\n",
      "7944\n",
      "7945\n",
      "7946\n",
      "7947\n",
      "7948\n",
      "7949\n",
      "7950\n",
      "7951\n",
      "7952\n",
      "7953\n",
      "7954\n",
      "7955\n",
      "7956\n",
      "7957\n",
      "7958\n",
      "7959\n",
      "7960\n",
      "7961\n",
      "7962\n",
      "7963\n",
      "7964\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7968\n",
      "7969\n",
      "7970\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7975\n",
      "7976\n",
      "7977\n",
      "7978\n",
      "7979\n",
      "7980\n",
      "7981\n",
      "7982\n",
      "7983\n",
      "7984\n",
      "7985\n",
      "7986\n",
      "7987\n",
      "7988\n",
      "7989\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7993\n",
      "7994\n",
      "7995\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8001\n",
      "8002\n",
      "8003\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8010\n",
      "8011\n",
      "8012\n",
      "8013\n",
      "8014\n",
      "8015\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8025\n",
      "8026\n",
      "8027\n",
      "8028\n",
      "8029\n",
      "8030\n",
      "8031\n",
      "8032\n",
      "8033\n",
      "8034\n",
      "8035\n",
      "8036\n",
      "8037\n",
      "8038\n",
      "8039\n",
      "8040\n",
      "8041\n",
      "8042\n",
      "8043\n",
      "8044\n",
      "8045\n",
      "8046\n",
      "8047\n",
      "8048\n",
      "8049\n",
      "8050\n",
      "8051\n",
      "8052\n",
      "8053\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8057\n",
      "8058\n",
      "8059\n",
      "8060\n",
      "8061\n",
      "8062\n",
      "8063\n",
      "8064\n",
      "8065\n",
      "8066\n",
      "8067\n",
      "8068\n",
      "8069\n",
      "8070\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8074\n",
      "8075\n",
      "8076\n",
      "8077\n",
      "8078\n",
      "8079\n",
      "8080\n",
      "8081\n",
      "8082\n",
      "8083\n",
      "8084\n",
      "8085\n",
      "8086\n",
      "8087\n",
      "8088\n",
      "8089\n",
      "8090\n",
      "8091\n",
      "8092\n",
      "8093\n",
      "8094\n",
      "8095\n",
      "8096\n",
      "8097\n",
      "8098\n",
      "8099\n",
      "8100\n",
      "8101\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "8105\n",
      "8106\n",
      "8107\n",
      "8108\n",
      "8109\n",
      "8110\n",
      "8111\n",
      "8112\n",
      "8113\n",
      "8114\n",
      "8115\n",
      "8116\n",
      "8117\n",
      "8118\n",
      "8119\n",
      "8120\n",
      "8121\n",
      "8122\n",
      "8123\n",
      "8124\n",
      "8125\n",
      "8126\n",
      "8127\n",
      "8128\n",
      "8129\n",
      "8130\n",
      "8131\n",
      "8132\n",
      "8133\n",
      "8134\n",
      "8135\n",
      "8136\n",
      "8137\n",
      "8138\n",
      "8139\n",
      "8140\n",
      "8141\n",
      "8142\n",
      "8143\n",
      "8144\n",
      "8145\n",
      "8146\n",
      "8147\n",
      "8148\n",
      "8149\n",
      "8150\n",
      "8151\n",
      "8152\n",
      "8153\n",
      "8154\n",
      "8155\n",
      "8156\n",
      "8157\n",
      "8158\n",
      "8159\n",
      "8160\n",
      "8161\n",
      "8162\n",
      "8163\n",
      "8164\n",
      "8165\n",
      "8166\n",
      "8167\n",
      "8168\n",
      "8169\n",
      "8170\n",
      "8171\n",
      "8172\n",
      "8173\n",
      "8174\n",
      "8175\n",
      "8176\n",
      "8177\n",
      "8178\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8182\n",
      "8183\n",
      "8184\n",
      "8185\n",
      "8186\n",
      "8187\n",
      "8188\n",
      "8189\n",
      "8190\n",
      "8191\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8202\n",
      "8203\n",
      "8204\n",
      "8205\n",
      "8206\n",
      "8207\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "8216\n",
      "8217\n",
      "8218\n",
      "8219\n",
      "8220\n",
      "8221\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8240\n",
      "8241\n",
      "8242\n",
      "8243\n",
      "8244\n",
      "8245\n",
      "8246\n",
      "8247\n",
      "8248\n",
      "8249\n",
      "8250\n",
      "8251\n",
      "8252\n",
      "8253\n",
      "8254\n",
      "8255\n",
      "8256\n",
      "8257\n",
      "8258\n",
      "8259\n",
      "8260\n",
      "8261\n",
      "8262\n",
      "8263\n",
      "8264\n",
      "8265\n",
      "8266\n",
      "8267\n",
      "8268\n",
      "8269\n",
      "8270\n",
      "8271\n",
      "8272\n",
      "8273\n",
      "8274\n",
      "8275\n",
      "8276\n",
      "8277\n",
      "8278\n",
      "8279\n",
      "8280\n",
      "8281\n",
      "8282\n",
      "8283\n",
      "8284\n",
      "8285\n",
      "8286\n",
      "8287\n",
      "8288\n",
      "8289\n",
      "8290\n",
      "8291\n",
      "8292\n",
      "8293\n",
      "8294\n",
      "8295\n",
      "8296\n",
      "8297\n",
      "8298\n",
      "8299\n",
      "8300\n",
      "8301\n",
      "8302\n",
      "8303\n",
      "8304\n",
      "8305\n",
      "8306\n",
      "8307\n",
      "8308\n",
      "8309\n",
      "8310\n",
      "8311\n",
      "8312\n",
      "8313\n",
      "8314\n",
      "8315\n",
      "8316\n",
      "8317\n",
      "8318\n",
      "8319\n",
      "8320\n",
      "8321\n",
      "8322\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8326\n",
      "8327\n",
      "8328\n",
      "8329\n",
      "8330\n",
      "8331\n",
      "8332\n",
      "8333\n",
      "8334\n",
      "8335\n",
      "8336\n",
      "8337\n",
      "8338\n",
      "8339\n",
      "8340\n",
      "8341\n",
      "8342\n",
      "8343\n",
      "8344\n",
      "8345\n",
      "8346\n",
      "8347\n",
      "8348\n",
      "8349\n",
      "8350\n",
      "8351\n",
      "8352\n",
      "8353\n",
      "8354\n",
      "8355\n",
      "8356\n",
      "8357\n",
      "8358\n",
      "8359\n",
      "8360\n",
      "8361\n",
      "8362\n",
      "8363\n",
      "8364\n",
      "8365\n",
      "8366\n",
      "8367\n",
      "8368\n",
      "8369\n",
      "8370\n",
      "8371\n",
      "8372\n",
      "8373\n",
      "8374\n",
      "8375\n",
      "8376\n",
      "8377\n",
      "8378\n",
      "8379\n",
      "8380\n",
      "8381\n",
      "8382\n",
      "8383\n",
      "8384\n",
      "8385\n",
      "8386\n",
      "8387\n",
      "8388\n",
      "8389\n",
      "8390\n",
      "8391\n",
      "8392\n",
      "8393\n",
      "8394\n",
      "8395\n",
      "8396\n",
      "8397\n",
      "8398\n",
      "8399\n",
      "8400\n",
      "8401\n",
      "8402\n",
      "8403\n",
      "8404\n",
      "8405\n",
      "8406\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8411\n",
      "8412\n",
      "8413\n",
      "8414\n",
      "8415\n",
      "8416\n",
      "8417\n",
      "8418\n",
      "8419\n",
      "8420\n",
      "8421\n",
      "8422\n",
      "8423\n",
      "8424\n",
      "8425\n",
      "8426\n",
      "8427\n",
      "8428\n",
      "8429\n",
      "8430\n",
      "8431\n",
      "8432\n",
      "8433\n",
      "8434\n",
      "8435\n",
      "8436\n",
      "8437\n",
      "8438\n",
      "8439\n",
      "8440\n",
      "8441\n",
      "8442\n",
      "8443\n",
      "8444\n",
      "8445\n",
      "8446\n",
      "8447\n",
      "8448\n",
      "8449\n",
      "8450\n",
      "8451\n",
      "8452\n",
      "8453\n",
      "8454\n",
      "8455\n",
      "8456\n",
      "8457\n",
      "8458\n",
      "8459\n",
      "8460\n",
      "8461\n",
      "8462\n",
      "8463\n",
      "8464\n",
      "8465\n",
      "8466\n",
      "8467\n",
      "8468\n",
      "8469\n",
      "8470\n",
      "8471\n",
      "8472\n",
      "8473\n",
      "8474\n",
      "8475\n",
      "8476\n",
      "8477\n",
      "8478\n",
      "8479\n",
      "8480\n",
      "8481\n",
      "8482\n",
      "8483\n",
      "8484\n",
      "8485\n",
      "8486\n",
      "8487\n",
      "8488\n",
      "8489\n",
      "8490\n",
      "8491\n",
      "8492\n",
      "8493\n",
      "8494\n",
      "8495\n",
      "8496\n",
      "8497\n",
      "8498\n",
      "8499\n",
      "8500\n",
      "8501\n",
      "8502\n",
      "8503\n",
      "8504\n",
      "8505\n",
      "8506\n",
      "8507\n",
      "8508\n",
      "8509\n",
      "8510\n",
      "8511\n",
      "8512\n",
      "8513\n",
      "8514\n",
      "8515\n",
      "8516\n",
      "8517\n",
      "8518\n",
      "8519\n",
      "8520\n",
      "8521\n",
      "8522\n",
      "8523\n",
      "8524\n",
      "8525\n",
      "8526\n",
      "8527\n",
      "8528\n",
      "8529\n",
      "8530\n",
      "8531\n",
      "8532\n",
      "8533\n",
      "8534\n",
      "8535\n",
      "8536\n",
      "8537\n",
      "8538\n",
      "8539\n",
      "8540\n",
      "8541\n",
      "8542\n",
      "8543\n",
      "8544\n",
      "8545\n",
      "8546\n",
      "8547\n",
      "8548\n",
      "8549\n",
      "8550\n",
      "8551\n",
      "8552\n",
      "8553\n",
      "8554\n",
      "8555\n",
      "8556\n",
      "8557\n",
      "8558\n",
      "8559\n",
      "8560\n",
      "8561\n",
      "8562\n",
      "8563\n",
      "8564\n",
      "8565\n",
      "8566\n",
      "8567\n",
      "8568\n",
      "8569\n",
      "8570\n",
      "8571\n",
      "8572\n",
      "8573\n",
      "8574\n",
      "8575\n",
      "8576\n",
      "8577\n",
      "8578\n",
      "8579\n",
      "8580\n",
      "8581\n",
      "8582\n",
      "8583\n",
      "8584\n",
      "8585\n",
      "8586\n",
      "8587\n",
      "8588\n",
      "8589\n",
      "8590\n",
      "8591\n",
      "8592\n",
      "8593\n",
      "8594\n",
      "8595\n",
      "8596\n",
      "8597\n",
      "8598\n",
      "8599\n",
      "8600\n",
      "8601\n",
      "8602\n",
      "8603\n",
      "8604\n",
      "8605\n",
      "8606\n",
      "8607\n",
      "8608\n",
      "8609\n",
      "8610\n",
      "8611\n",
      "8612\n",
      "8613\n",
      "8614\n",
      "8615\n",
      "8616\n",
      "8617\n",
      "8618\n",
      "8619\n",
      "8620\n",
      "8621\n",
      "8622\n",
      "8623\n",
      "8624\n",
      "8625\n",
      "8626\n",
      "8627\n",
      "8628\n",
      "8629\n",
      "8630\n",
      "8631\n",
      "8632\n",
      "8633\n",
      "8634\n",
      "8635\n",
      "8636\n",
      "8637\n",
      "8638\n",
      "8639\n",
      "8640\n",
      "8641\n",
      "8642\n",
      "8643\n",
      "8644\n",
      "8645\n",
      "8646\n",
      "8647\n",
      "8648\n",
      "8649\n",
      "8650\n",
      "8651\n",
      "8652\n",
      "8653\n",
      "8654\n",
      "8655\n",
      "8656\n",
      "8657\n",
      "8658\n",
      "8659\n",
      "8660\n",
      "8661\n",
      "8662\n",
      "8663\n",
      "8664\n",
      "8665\n",
      "8666\n",
      "8667\n",
      "8668\n",
      "8669\n",
      "8670\n",
      "8671\n",
      "8672\n",
      "8673\n",
      "8674\n",
      "8675\n",
      "8676\n",
      "8677\n",
      "8678\n",
      "8679\n",
      "8680\n",
      "8681\n",
      "8682\n",
      "8683\n",
      "8684\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8688\n",
      "8689\n",
      "8690\n",
      "8691\n",
      "8692\n",
      "8693\n",
      "8694\n",
      "8695\n",
      "8696\n",
      "8697\n",
      "8698\n",
      "8699\n",
      "8700\n",
      "8701\n",
      "8702\n",
      "8703\n",
      "8704\n",
      "8705\n",
      "8706\n",
      "8707\n",
      "8708\n",
      "8709\n",
      "8710\n",
      "8711\n",
      "8712\n",
      "8713\n",
      "8714\n",
      "8715\n",
      "8716\n",
      "8717\n",
      "8718\n",
      "8719\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8723\n",
      "8724\n",
      "8725\n",
      "8726\n",
      "8727\n",
      "8728\n",
      "8729\n",
      "8730\n",
      "8731\n",
      "8732\n",
      "8733\n",
      "8734\n",
      "8735\n",
      "8736\n",
      "8737\n",
      "8738\n",
      "8739\n",
      "8740\n",
      "8741\n",
      "8742\n",
      "8743\n",
      "8744\n",
      "8745\n",
      "8746\n",
      "8747\n",
      "8748\n",
      "8749\n",
      "8750\n",
      "8751\n",
      "8752\n",
      "8753\n",
      "8754\n",
      "8755\n",
      "8756\n",
      "8757\n",
      "8758\n",
      "8759\n",
      "8760\n",
      "8761\n",
      "8762\n",
      "8763\n",
      "8764\n",
      "8765\n",
      "8766\n",
      "8767\n",
      "8768\n",
      "8769\n",
      "8770\n",
      "8771\n",
      "8772\n",
      "8773\n",
      "8774\n",
      "8775\n",
      "8776\n",
      "8777\n",
      "8778\n",
      "8779\n",
      "8780\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8785\n",
      "8786\n",
      "8787\n",
      "8788\n",
      "8789\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8797\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8805\n",
      "8806\n",
      "8807\n",
      "8808\n",
      "8809\n",
      "8810\n",
      "8811\n",
      "8812\n",
      "8813\n",
      "8814\n",
      "8815\n",
      "8816\n",
      "8817\n",
      "8818\n",
      "8819\n",
      "8820\n",
      "8821\n",
      "8822\n",
      "8823\n",
      "8824\n",
      "8825\n",
      "8826\n",
      "8827\n",
      "8828\n",
      "8829\n",
      "8830\n",
      "8831\n",
      "8832\n",
      "8833\n",
      "8834\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8842\n",
      "8843\n",
      "8844\n",
      "8845\n",
      "8846\n",
      "8847\n",
      "8848\n",
      "8849\n",
      "8850\n",
      "8851\n",
      "8852\n",
      "8853\n",
      "8854\n",
      "8855\n",
      "8856\n",
      "8857\n",
      "8858\n",
      "8859\n",
      "8860\n",
      "8861\n",
      "8862\n",
      "8863\n",
      "8864\n",
      "8865\n",
      "8866\n",
      "8867\n",
      "8868\n",
      "8869\n",
      "8870\n",
      "8871\n",
      "8872\n",
      "8873\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8878\n",
      "8879\n",
      "8880\n",
      "8881\n",
      "8882\n",
      "8883\n",
      "8884\n",
      "8885\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8889\n",
      "8890\n",
      "8891\n",
      "8892\n",
      "8893\n",
      "8894\n",
      "8895\n",
      "8896\n",
      "8897\n",
      "8898\n",
      "8899\n",
      "8900\n",
      "8901\n",
      "8902\n",
      "8903\n",
      "8904\n",
      "8905\n",
      "8906\n",
      "8907\n",
      "8908\n",
      "8909\n",
      "8910\n",
      "8911\n",
      "8912\n",
      "8913\n",
      "8914\n",
      "8915\n",
      "8916\n",
      "8917\n",
      "8918\n",
      "8919\n",
      "8920\n",
      "8921\n",
      "8922\n",
      "8923\n",
      "8924\n",
      "8925\n",
      "8926\n",
      "8927\n",
      "8928\n",
      "8929\n",
      "8930\n",
      "8931\n",
      "8932\n",
      "8933\n",
      "8934\n",
      "8935\n",
      "8936\n",
      "8937\n",
      "8938\n",
      "8939\n",
      "8940\n",
      "8941\n",
      "8942\n",
      "8943\n",
      "8944\n",
      "8945\n",
      "8946\n",
      "8947\n",
      "8948\n",
      "8949\n",
      "8950\n",
      "8951\n",
      "8952\n",
      "8953\n",
      "8954\n",
      "8955\n",
      "8956\n",
      "8957\n",
      "8958\n",
      "8959\n",
      "8960\n",
      "8961\n",
      "8962\n",
      "8963\n",
      "8964\n",
      "8965\n",
      "8966\n",
      "8967\n",
      "8968\n",
      "8969\n",
      "8970\n",
      "8971\n",
      "8972\n",
      "8973\n",
      "8974\n",
      "8975\n",
      "8976\n",
      "8977\n",
      "8978\n",
      "8979\n",
      "8980\n",
      "8981\n",
      "8982\n",
      "8983\n",
      "8984\n",
      "8985\n",
      "8986\n",
      "8987\n",
      "8988\n",
      "8989\n",
      "8990\n",
      "8991\n",
      "8992\n",
      "8993\n",
      "8994\n",
      "8995\n",
      "8996\n",
      "8997\n",
      "8998\n",
      "8999\n",
      "9000\n",
      "9001\n",
      "9002\n",
      "9003\n",
      "9004\n",
      "9005\n",
      "9006\n",
      "9007\n",
      "9008\n",
      "9009\n",
      "9010\n",
      "9011\n",
      "9012\n",
      "9013\n",
      "9014\n",
      "9015\n",
      "9016\n",
      "9017\n",
      "9018\n",
      "9019\n",
      "9020\n",
      "9021\n",
      "9022\n",
      "9023\n",
      "9024\n",
      "9025\n",
      "9026\n",
      "9027\n",
      "9028\n",
      "9029\n",
      "9030\n",
      "9031\n",
      "9032\n",
      "9033\n",
      "9034\n",
      "9035\n",
      "9036\n",
      "9037\n",
      "9038\n",
      "9039\n",
      "9040\n",
      "9041\n",
      "9042\n",
      "9043\n",
      "9044\n",
      "9045\n",
      "9046\n",
      "9047\n",
      "9048\n",
      "9049\n",
      "9050\n",
      "9051\n",
      "9052\n",
      "9053\n",
      "9054\n",
      "9055\n",
      "9056\n",
      "9057\n",
      "9058\n",
      "9059\n",
      "9060\n",
      "9061\n",
      "9062\n",
      "9063\n",
      "9064\n",
      "9065\n",
      "9066\n",
      "9067\n",
      "9068\n",
      "9069\n",
      "9070\n",
      "9071\n",
      "9072\n",
      "9073\n",
      "9074\n",
      "9075\n",
      "9076\n",
      "9077\n",
      "9078\n",
      "9079\n",
      "9080\n",
      "9081\n",
      "9082\n",
      "9083\n",
      "9084\n",
      "9085\n",
      "9086\n",
      "9087\n",
      "9088\n",
      "9089\n",
      "9090\n",
      "9091\n",
      "9092\n",
      "9093\n",
      "9094\n",
      "9095\n",
      "9096\n",
      "9097\n",
      "9098\n",
      "9099\n",
      "9100\n",
      "9101\n",
      "9102\n",
      "9103\n",
      "9104\n",
      "9105\n",
      "9106\n",
      "9107\n",
      "9108\n",
      "9109\n",
      "9110\n",
      "9111\n",
      "9112\n",
      "9113\n",
      "9114\n",
      "9115\n",
      "9116\n",
      "9117\n",
      "9118\n",
      "9119\n",
      "9120\n",
      "9121\n",
      "9122\n",
      "9123\n",
      "9124\n",
      "9125\n",
      "9126\n",
      "9127\n",
      "9128\n",
      "9129\n",
      "9130\n",
      "9131\n",
      "9132\n",
      "9133\n",
      "9134\n",
      "9135\n",
      "9136\n",
      "9137\n",
      "9138\n",
      "9139\n",
      "9140\n",
      "9141\n",
      "9142\n",
      "9143\n",
      "9144\n",
      "9145\n",
      "9146\n",
      "9147\n",
      "9148\n",
      "9149\n",
      "9150\n",
      "9151\n",
      "9152\n",
      "9153\n",
      "9154\n",
      "9155\n",
      "9156\n",
      "9157\n",
      "9158\n",
      "9159\n",
      "9160\n",
      "9161\n",
      "9162\n",
      "9163\n",
      "9164\n",
      "9165\n",
      "9166\n",
      "9167\n",
      "9168\n",
      "9169\n",
      "9170\n",
      "9171\n",
      "9172\n",
      "9173\n",
      "9174\n",
      "9175\n",
      "9176\n",
      "9177\n",
      "9178\n",
      "9179\n",
      "9180\n",
      "9181\n",
      "9182\n",
      "9183\n",
      "9184\n",
      "9185\n",
      "9186\n",
      "9187\n",
      "9188\n",
      "9189\n",
      "9190\n",
      "9191\n",
      "9192\n",
      "9193\n",
      "9194\n",
      "9195\n",
      "9196\n",
      "9197\n",
      "9198\n",
      "9199\n",
      "9200\n",
      "9201\n",
      "9202\n",
      "9203\n",
      "9204\n",
      "9205\n",
      "9206\n",
      "9207\n",
      "9208\n",
      "9209\n",
      "9210\n",
      "9211\n",
      "9212\n",
      "9213\n",
      "9214\n",
      "9215\n",
      "9216\n",
      "9217\n",
      "9218\n",
      "9219\n",
      "9220\n",
      "9221\n",
      "9222\n",
      "9223\n",
      "9224\n",
      "9225\n",
      "9226\n",
      "9227\n",
      "9228\n",
      "9229\n",
      "9230\n",
      "9231\n",
      "9232\n",
      "9233\n",
      "9234\n",
      "9235\n",
      "9236\n",
      "9237\n",
      "9238\n",
      "9239\n",
      "9240\n",
      "9241\n",
      "9242\n",
      "9243\n",
      "9244\n",
      "9245\n",
      "9246\n",
      "9247\n",
      "9248\n",
      "9249\n",
      "9250\n",
      "9251\n",
      "9252\n",
      "9253\n",
      "9254\n",
      "9255\n",
      "9256\n",
      "9257\n",
      "9258\n",
      "9259\n",
      "9260\n",
      "9261\n",
      "9262\n",
      "9263\n",
      "9264\n",
      "9265\n",
      "9266\n",
      "9267\n",
      "9268\n",
      "9269\n",
      "9270\n",
      "9271\n",
      "9272\n",
      "9273\n",
      "9274\n",
      "9275\n",
      "9276\n",
      "9277\n",
      "9278\n",
      "9279\n",
      "9280\n",
      "9281\n",
      "9282\n",
      "9283\n",
      "9284\n",
      "9285\n",
      "9286\n",
      "9287\n",
      "9288\n",
      "9289\n",
      "9290\n",
      "9291\n",
      "9292\n",
      "9293\n",
      "9294\n",
      "9295\n",
      "9296\n",
      "9297\n",
      "9298\n",
      "9299\n",
      "9300\n",
      "9301\n",
      "9302\n",
      "9303\n",
      "9304\n",
      "9305\n",
      "9306\n",
      "9307\n",
      "9308\n",
      "9309\n",
      "9310\n",
      "9311\n",
      "9312\n",
      "9313\n",
      "9314\n",
      "9315\n",
      "9316\n",
      "9317\n",
      "9318\n",
      "9319\n",
      "9320\n",
      "9321\n",
      "9322\n",
      "9323\n",
      "9324\n",
      "9325\n",
      "9326\n",
      "9327\n",
      "9328\n",
      "9329\n",
      "9330\n",
      "9331\n",
      "9332\n",
      "9333\n",
      "9334\n",
      "9335\n",
      "9336\n",
      "9337\n",
      "9338\n",
      "9339\n",
      "9340\n",
      "9341\n",
      "9342\n",
      "9343\n",
      "9344\n",
      "9345\n",
      "9346\n",
      "9347\n",
      "9348\n",
      "9349\n",
      "9350\n",
      "9351\n",
      "9352\n",
      "9353\n",
      "9354\n",
      "9355\n",
      "9356\n",
      "9357\n",
      "9358\n",
      "9359\n",
      "9360\n",
      "9361\n",
      "9362\n",
      "9363\n",
      "9364\n",
      "9365\n",
      "9366\n",
      "9367\n",
      "9368\n",
      "9369\n",
      "9370\n",
      "9371\n",
      "9372\n",
      "9373\n",
      "9374\n",
      "9375\n",
      "9376\n",
      "9377\n",
      "9378\n",
      "9379\n",
      "9380\n",
      "9381\n",
      "9382\n",
      "9383\n",
      "9384\n",
      "9385\n",
      "9386\n",
      "9387\n",
      "9388\n",
      "9389\n",
      "9390\n",
      "9391\n",
      "9392\n",
      "9393\n",
      "9394\n",
      "9395\n",
      "9396\n",
      "9397\n",
      "9398\n",
      "9399\n",
      "9400\n",
      "9401\n",
      "9402\n",
      "9403\n",
      "9404\n",
      "9405\n",
      "9406\n",
      "9407\n",
      "9408\n",
      "9409\n",
      "9410\n",
      "9411\n",
      "9412\n",
      "9413\n",
      "9414\n",
      "9415\n",
      "9416\n",
      "9417\n",
      "9418\n",
      "9419\n",
      "9420\n",
      "9421\n",
      "9422\n",
      "9423\n",
      "9424\n",
      "9425\n",
      "9426\n",
      "9427\n",
      "9428\n",
      "9429\n",
      "9430\n",
      "9431\n",
      "9432\n",
      "9433\n",
      "9434\n",
      "9435\n",
      "9436\n",
      "9437\n",
      "9438\n",
      "9439\n",
      "9440\n",
      "9441\n",
      "9442\n",
      "9443\n",
      "9444\n",
      "9445\n",
      "9446\n",
      "9447\n",
      "9448\n",
      "9449\n",
      "9450\n",
      "9451\n",
      "9452\n",
      "9453\n",
      "9454\n",
      "9455\n",
      "9456\n",
      "9457\n",
      "9458\n",
      "9459\n",
      "9460\n",
      "9461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9462\n",
      "9463\n",
      "9464\n",
      "9465\n",
      "9466\n",
      "9467\n",
      "9468\n",
      "9469\n",
      "9470\n",
      "9471\n",
      "9472\n",
      "9473\n",
      "9474\n",
      "9475\n",
      "9476\n",
      "9477\n",
      "9478\n",
      "9479\n",
      "9480\n",
      "9481\n",
      "9482\n",
      "9483\n",
      "9484\n",
      "9485\n",
      "9486\n",
      "9487\n",
      "9488\n",
      "9489\n",
      "9490\n",
      "9491\n",
      "9492\n",
      "9493\n",
      "9494\n",
      "9495\n",
      "9496\n",
      "9497\n",
      "9498\n",
      "9499\n",
      "9500\n",
      "9501\n",
      "9502\n",
      "9503\n",
      "9504\n",
      "9505\n",
      "9506\n",
      "9507\n",
      "9508\n",
      "9509\n",
      "9510\n",
      "9511\n",
      "9512\n",
      "9513\n",
      "9514\n",
      "9515\n",
      "9516\n",
      "9517\n",
      "9518\n",
      "9519\n",
      "9520\n",
      "9521\n",
      "9522\n",
      "9523\n",
      "9524\n",
      "9525\n",
      "9526\n",
      "9527\n",
      "9528\n",
      "9529\n",
      "9530\n",
      "9531\n",
      "9532\n",
      "9533\n",
      "9534\n",
      "9535\n",
      "9536\n",
      "9537\n",
      "9538\n",
      "9539\n",
      "9540\n",
      "9541\n",
      "9542\n",
      "9543\n",
      "9544\n",
      "9545\n",
      "9546\n",
      "9547\n",
      "9548\n",
      "9549\n",
      "9550\n",
      "9551\n",
      "9552\n",
      "9553\n",
      "9554\n",
      "9555\n",
      "9556\n",
      "9557\n",
      "9558\n",
      "9559\n",
      "9560\n",
      "9561\n",
      "9562\n",
      "9563\n",
      "9564\n",
      "9565\n",
      "9566\n",
      "9567\n",
      "9568\n",
      "9569\n",
      "9570\n",
      "9571\n",
      "9572\n",
      "9573\n",
      "9574\n",
      "9575\n",
      "9576\n",
      "9577\n",
      "9578\n",
      "9579\n",
      "9580\n",
      "9581\n",
      "9582\n",
      "9583\n",
      "9584\n",
      "9585\n",
      "9586\n",
      "9587\n",
      "9588\n",
      "9589\n",
      "9590\n",
      "9591\n",
      "9592\n",
      "9593\n",
      "9594\n",
      "9595\n",
      "9596\n",
      "9597\n",
      "9598\n",
      "9599\n",
      "9600\n",
      "9601\n",
      "9602\n",
      "9603\n",
      "9604\n",
      "9605\n",
      "9606\n",
      "9607\n",
      "9608\n",
      "9609\n",
      "9610\n",
      "9611\n",
      "9612\n",
      "9613\n",
      "9614\n",
      "9615\n",
      "9616\n",
      "9617\n",
      "9618\n",
      "9619\n",
      "9620\n",
      "9621\n",
      "9622\n",
      "9623\n",
      "9624\n",
      "9625\n",
      "9626\n",
      "9627\n",
      "9628\n",
      "9629\n",
      "9630\n",
      "9631\n",
      "9632\n",
      "9633\n",
      "9634\n",
      "9635\n",
      "9636\n",
      "9637\n",
      "9638\n",
      "9639\n",
      "9640\n",
      "9641\n",
      "9642\n",
      "9643\n",
      "9644\n",
      "9645\n",
      "9646\n",
      "9647\n",
      "9648\n",
      "9649\n",
      "9650\n",
      "9651\n",
      "9652\n",
      "9653\n",
      "9654\n",
      "9655\n",
      "9656\n",
      "9657\n",
      "9658\n",
      "9659\n",
      "9660\n",
      "9661\n",
      "9662\n",
      "9663\n",
      "9664\n",
      "9665\n",
      "9666\n",
      "9667\n",
      "9668\n",
      "9669\n",
      "9670\n",
      "9671\n",
      "9672\n",
      "9673\n",
      "9674\n",
      "9675\n",
      "9676\n",
      "9677\n",
      "9678\n",
      "9679\n",
      "9680\n",
      "9681\n",
      "9682\n",
      "9683\n",
      "9684\n",
      "9685\n",
      "9686\n",
      "9687\n",
      "9688\n",
      "9689\n",
      "9690\n",
      "9691\n",
      "9692\n",
      "9693\n",
      "9694\n",
      "9695\n",
      "9696\n",
      "9697\n",
      "9698\n",
      "9699\n",
      "9700\n",
      "9701\n",
      "9702\n",
      "9703\n",
      "9704\n",
      "9705\n",
      "9706\n",
      "9707\n",
      "9708\n",
      "9709\n",
      "9710\n",
      "9711\n",
      "9712\n",
      "9713\n",
      "9714\n",
      "9715\n",
      "9716\n",
      "9717\n",
      "9718\n",
      "9719\n",
      "9720\n",
      "9721\n",
      "9722\n",
      "9723\n",
      "9724\n",
      "9725\n",
      "9726\n",
      "9727\n",
      "9728\n",
      "9729\n",
      "9730\n",
      "9731\n",
      "9732\n",
      "9733\n",
      "9734\n",
      "9735\n",
      "9736\n",
      "9737\n",
      "9738\n",
      "9739\n",
      "9740\n",
      "9741\n",
      "9742\n",
      "9743\n",
      "9744\n",
      "9745\n",
      "9746\n",
      "9747\n",
      "9748\n",
      "9749\n",
      "9750\n",
      "9751\n",
      "9752\n",
      "9753\n",
      "9754\n",
      "9755\n",
      "9756\n",
      "9757\n",
      "9758\n",
      "9759\n",
      "9760\n",
      "9761\n",
      "9762\n",
      "9763\n",
      "9764\n",
      "9765\n",
      "9766\n",
      "9767\n",
      "9768\n",
      "9769\n",
      "9770\n",
      "9771\n",
      "9772\n",
      "9773\n",
      "9774\n",
      "9775\n",
      "9776\n",
      "9777\n",
      "9778\n",
      "9779\n",
      "9780\n",
      "9781\n",
      "9782\n",
      "9783\n",
      "9784\n",
      "9785\n",
      "9786\n",
      "9787\n",
      "9788\n",
      "9789\n",
      "9790\n",
      "9791\n",
      "9792\n",
      "9793\n",
      "9794\n",
      "9795\n",
      "9796\n",
      "9797\n",
      "9798\n",
      "9799\n",
      "9800\n",
      "9801\n",
      "9802\n",
      "9803\n",
      "9804\n",
      "9805\n",
      "9806\n",
      "9807\n",
      "9808\n",
      "9809\n",
      "9810\n",
      "9811\n",
      "9812\n",
      "9813\n",
      "9814\n",
      "9815\n",
      "9816\n",
      "9817\n",
      "9818\n",
      "9819\n",
      "9820\n",
      "9821\n",
      "9822\n",
      "9823\n",
      "9824\n",
      "9825\n",
      "9826\n",
      "9827\n",
      "9828\n",
      "9829\n",
      "9830\n",
      "9831\n",
      "9832\n",
      "9833\n",
      "9834\n",
      "9835\n",
      "9836\n",
      "9837\n",
      "9838\n",
      "9839\n",
      "9840\n",
      "9841\n",
      "9842\n",
      "9843\n",
      "9844\n",
      "9845\n",
      "9846\n",
      "9847\n",
      "9848\n",
      "9849\n",
      "9850\n",
      "9851\n",
      "9852\n",
      "9853\n",
      "9854\n",
      "9855\n",
      "9856\n",
      "9857\n",
      "9858\n",
      "9859\n",
      "9860\n",
      "9861\n",
      "9862\n",
      "9863\n",
      "9864\n",
      "9865\n",
      "9866\n",
      "9867\n",
      "9868\n",
      "9869\n",
      "9870\n",
      "9871\n",
      "9872\n",
      "9873\n",
      "9874\n",
      "9875\n",
      "9876\n",
      "9877\n",
      "9878\n",
      "9879\n",
      "9880\n",
      "9881\n",
      "9882\n",
      "9883\n",
      "9884\n",
      "9885\n",
      "9886\n",
      "9887\n",
      "9888\n",
      "9889\n",
      "9890\n",
      "9891\n",
      "9892\n",
      "9893\n",
      "9894\n",
      "9895\n",
      "9896\n",
      "9897\n",
      "9898\n",
      "9899\n",
      "9900\n",
      "9901\n",
      "9902\n",
      "9903\n",
      "9904\n",
      "9905\n",
      "9906\n",
      "9907\n",
      "9908\n",
      "9909\n",
      "9910\n",
      "9911\n",
      "9912\n",
      "9913\n",
      "9914\n",
      "9915\n",
      "9916\n",
      "9917\n",
      "9918\n",
      "9919\n",
      "9920\n",
      "9921\n",
      "9922\n",
      "9923\n",
      "9924\n",
      "9925\n",
      "9926\n",
      "9927\n",
      "9928\n",
      "9929\n",
      "9930\n",
      "9931\n",
      "9932\n",
      "9933\n",
      "9934\n",
      "9935\n",
      "9936\n",
      "9937\n",
      "9938\n",
      "9939\n",
      "9940\n",
      "9941\n",
      "9942\n",
      "9943\n",
      "9944\n",
      "9945\n",
      "9946\n",
      "9947\n",
      "9948\n",
      "9949\n",
      "9950\n",
      "9951\n",
      "9952\n",
      "9953\n",
      "9954\n",
      "9955\n",
      "9956\n",
      "9957\n",
      "9958\n",
      "9959\n",
      "9960\n",
      "9961\n",
      "9962\n",
      "9963\n",
      "9964\n",
      "9965\n",
      "9966\n",
      "9967\n",
      "9968\n",
      "9969\n",
      "9970\n",
      "9971\n",
      "9972\n",
      "9973\n",
      "9974\n",
      "9975\n",
      "9976\n",
      "9977\n",
      "9978\n",
      "9979\n",
      "9980\n",
      "9981\n",
      "9982\n",
      "9983\n",
      "9984\n",
      "9985\n",
      "9986\n",
      "9987\n",
      "9988\n",
      "9989\n",
      "9990\n",
      "9991\n",
      "9992\n",
      "9993\n",
      "9994\n",
      "9995\n",
      "9996\n",
      "9997\n",
      "9998\n",
      "9999\n",
      "10000\n",
      "10001\n",
      "10002\n",
      "10003\n",
      "10004\n",
      "10005\n",
      "10006\n",
      "10007\n",
      "10008\n",
      "10009\n",
      "10010\n",
      "10011\n",
      "10012\n",
      "10013\n",
      "10014\n",
      "10015\n",
      "10016\n",
      "10017\n",
      "10018\n",
      "10019\n",
      "10020\n",
      "10021\n",
      "10022\n",
      "10023\n",
      "10024\n",
      "10025\n",
      "10026\n",
      "10027\n",
      "10028\n",
      "10029\n",
      "10030\n",
      "10031\n",
      "10032\n",
      "10033\n",
      "10034\n",
      "10035\n",
      "10036\n",
      "10037\n",
      "10038\n",
      "10039\n",
      "10040\n",
      "10041\n",
      "10042\n",
      "10043\n",
      "10044\n",
      "10045\n",
      "10046\n",
      "10047\n",
      "10048\n",
      "10049\n",
      "10050\n",
      "10051\n",
      "10052\n",
      "10053\n",
      "10054\n",
      "10055\n",
      "10056\n",
      "10057\n",
      "10058\n",
      "10059\n",
      "10060\n",
      "10061\n",
      "10062\n",
      "10063\n",
      "10064\n",
      "10065\n",
      "10066\n",
      "10067\n",
      "10068\n",
      "10069\n",
      "10070\n",
      "10071\n",
      "10072\n",
      "10073\n",
      "10074\n",
      "10075\n",
      "10076\n",
      "10077\n",
      "10078\n",
      "10079\n",
      "10080\n",
      "10081\n",
      "10082\n",
      "10083\n",
      "10084\n",
      "10085\n",
      "10086\n",
      "10087\n",
      "10088\n",
      "10089\n",
      "10090\n",
      "10091\n",
      "10092\n",
      "10093\n",
      "10094\n",
      "10095\n",
      "10096\n",
      "10097\n",
      "10098\n",
      "10099\n",
      "10100\n",
      "10101\n",
      "10102\n",
      "10103\n",
      "10104\n",
      "10105\n",
      "10106\n",
      "10107\n",
      "10108\n",
      "10109\n",
      "10110\n",
      "10111\n",
      "10112\n",
      "10113\n",
      "10114\n",
      "10115\n",
      "10116\n",
      "10117\n",
      "10118\n",
      "10119\n",
      "10120\n",
      "10121\n",
      "10122\n",
      "10123\n",
      "10124\n",
      "10125\n",
      "10126\n",
      "10127\n",
      "10128\n",
      "10129\n",
      "10130\n",
      "10131\n",
      "10132\n",
      "10133\n",
      "10134\n",
      "10135\n",
      "10136\n",
      "10137\n",
      "10138\n",
      "10139\n",
      "10140\n",
      "10141\n",
      "10142\n",
      "10143\n",
      "10144\n",
      "10145\n",
      "10146\n",
      "10147\n",
      "10148\n",
      "10149\n",
      "10150\n",
      "10151\n",
      "10152\n",
      "10153\n",
      "10154\n",
      "10155\n",
      "10156\n",
      "10157\n",
      "10158\n",
      "10159\n",
      "10160\n",
      "10161\n",
      "10162\n",
      "10163\n",
      "10164\n",
      "10165\n",
      "10166\n",
      "10167\n",
      "10168\n",
      "10169\n",
      "10170\n",
      "10171\n",
      "10172\n",
      "10173\n",
      "10174\n",
      "10175\n",
      "10176\n",
      "10177\n",
      "10178\n",
      "10179\n",
      "10180\n",
      "10181\n",
      "10182\n",
      "10183\n",
      "10184\n",
      "10185\n",
      "10186\n",
      "10187\n",
      "10188\n",
      "10189\n",
      "10190\n",
      "10191\n",
      "10192\n",
      "10193\n",
      "10194\n",
      "10195\n",
      "10196\n",
      "10197\n",
      "10198\n",
      "10199\n",
      "10200\n",
      "10201\n",
      "10202\n",
      "10203\n",
      "10204\n",
      "10205\n",
      "10206\n",
      "10207\n",
      "10208\n",
      "10209\n",
      "10210\n",
      "10211\n",
      "10212\n",
      "10213\n",
      "10214\n",
      "10215\n",
      "10216\n",
      "10217\n",
      "10218\n",
      "10219\n",
      "10220\n",
      "10221\n",
      "10222\n",
      "10223\n",
      "10224\n",
      "10225\n",
      "10226\n",
      "10227\n",
      "10228\n",
      "10229\n",
      "10230\n",
      "10231\n",
      "10232\n",
      "10233\n",
      "10234\n",
      "10235\n",
      "10236\n",
      "10237\n",
      "10238\n",
      "10239\n",
      "10240\n",
      "10241\n",
      "10242\n",
      "10243\n",
      "10244\n",
      "10245\n",
      "10246\n",
      "10247\n",
      "10248\n",
      "10249\n",
      "10250\n",
      "10251\n",
      "10252\n",
      "10253\n",
      "10254\n",
      "10255\n",
      "10256\n",
      "10257\n",
      "10258\n",
      "10259\n",
      "10260\n",
      "10261\n",
      "10262\n",
      "10263\n",
      "10264\n",
      "10265\n",
      "10266\n",
      "10267\n",
      "10268\n",
      "10269\n",
      "10270\n",
      "10271\n",
      "10272\n",
      "10273\n",
      "10274\n",
      "10275\n",
      "10276\n",
      "10277\n",
      "10278\n",
      "10279\n",
      "10280\n",
      "10281\n",
      "10282\n",
      "10283\n",
      "10284\n",
      "10285\n",
      "10286\n",
      "10287\n",
      "10288\n",
      "10289\n",
      "10290\n",
      "10291\n",
      "10292\n",
      "10293\n",
      "10294\n",
      "10295\n",
      "10296\n",
      "10297\n",
      "10298\n",
      "10299\n",
      "10300\n",
      "10301\n",
      "10302\n",
      "10303\n",
      "10304\n",
      "10305\n",
      "10306\n",
      "10307\n",
      "10308\n",
      "10309\n",
      "10310\n",
      "10311\n",
      "10312\n",
      "10313\n",
      "10314\n",
      "10315\n",
      "10316\n",
      "10317\n",
      "10318\n",
      "10319\n",
      "10320\n",
      "10321\n",
      "10322\n",
      "10323\n",
      "10324\n",
      "10325\n",
      "10326\n",
      "10327\n",
      "10328\n",
      "10329\n",
      "10330\n",
      "10331\n",
      "10332\n",
      "10333\n",
      "10334\n",
      "10335\n",
      "10336\n",
      "10337\n",
      "10338\n",
      "10339\n",
      "10340\n",
      "10341\n",
      "10342\n",
      "10343\n",
      "10344\n",
      "10345\n",
      "10346\n",
      "10347\n",
      "10348\n",
      "10349\n",
      "10350\n",
      "10351\n",
      "10352\n",
      "10353\n",
      "10354\n",
      "10355\n",
      "10356\n",
      "10357\n",
      "10358\n",
      "10359\n",
      "10360\n",
      "10361\n",
      "10362\n",
      "10363\n",
      "10364\n",
      "10365\n",
      "10366\n",
      "10367\n",
      "10368\n",
      "10369\n",
      "10370\n",
      "10371\n",
      "10372\n",
      "10373\n",
      "10374\n",
      "10375\n",
      "10376\n",
      "10377\n",
      "10378\n",
      "10379\n",
      "10380\n",
      "10381\n",
      "10382\n",
      "10383\n",
      "10384\n",
      "10385\n",
      "10386\n",
      "10387\n",
      "10388\n",
      "10389\n",
      "10390\n",
      "10391\n",
      "10392\n",
      "10393\n",
      "10394\n",
      "10395\n",
      "10396\n",
      "10397\n",
      "10398\n",
      "10399\n",
      "10400\n",
      "10401\n",
      "10402\n",
      "10403\n",
      "10404\n",
      "10405\n",
      "10406\n",
      "10407\n",
      "10408\n",
      "10409\n",
      "10410\n",
      "10411\n",
      "10412\n",
      "10413\n",
      "10414\n",
      "10415\n",
      "10416\n",
      "10417\n",
      "10418\n",
      "10419\n",
      "10420\n",
      "10421\n",
      "10422\n",
      "10423\n",
      "10424\n",
      "10425\n",
      "10426\n",
      "10427\n",
      "10428\n",
      "10429\n",
      "10430\n",
      "10431\n",
      "10432\n",
      "10433\n",
      "10434\n",
      "10435\n",
      "10436\n",
      "10437\n",
      "10438\n",
      "10439\n",
      "10440\n",
      "10441\n",
      "10442\n",
      "10443\n",
      "10444\n",
      "10445\n",
      "10446\n",
      "10447\n",
      "10448\n",
      "10449\n",
      "10450\n",
      "10451\n",
      "10452\n",
      "10453\n",
      "10454\n",
      "10455\n",
      "10456\n",
      "10457\n",
      "10458\n",
      "10459\n",
      "10460\n",
      "10461\n",
      "10462\n",
      "10463\n",
      "10464\n",
      "10465\n",
      "10466\n",
      "10467\n",
      "10468\n",
      "10469\n",
      "10470\n",
      "10471\n",
      "10472\n",
      "10473\n",
      "10474\n",
      "10475\n",
      "10476\n",
      "10477\n",
      "10478\n",
      "10479\n",
      "10480\n",
      "10481\n",
      "10482\n",
      "10483\n",
      "10484\n",
      "10485\n",
      "10486\n",
      "10487\n",
      "10488\n",
      "10489\n",
      "10490\n",
      "10491\n",
      "10492\n",
      "10493\n",
      "10494\n",
      "10495\n",
      "10496\n",
      "10497\n",
      "10498\n",
      "10499\n",
      "10500\n",
      "10501\n",
      "10502\n",
      "10503\n",
      "10504\n",
      "10505\n",
      "10506\n",
      "10507\n",
      "10508\n",
      "10509\n",
      "10510\n",
      "10511\n",
      "10512\n",
      "10513\n",
      "10514\n",
      "10515\n",
      "10516\n",
      "10517\n",
      "10518\n",
      "10519\n",
      "10520\n",
      "10521\n",
      "10522\n",
      "10523\n",
      "10524\n",
      "10525\n",
      "10526\n",
      "10527\n",
      "10528\n",
      "10529\n",
      "10530\n",
      "10531\n",
      "10532\n",
      "10533\n",
      "10534\n",
      "10535\n",
      "10536\n",
      "10537\n",
      "10538\n",
      "10539\n",
      "10540\n",
      "10541\n",
      "10542\n",
      "10543\n",
      "10544\n",
      "10545\n",
      "10546\n",
      "10547\n",
      "10548\n",
      "10549\n",
      "10550\n",
      "10551\n",
      "10552\n",
      "10553\n",
      "10554\n",
      "10555\n",
      "10556\n",
      "10557\n",
      "10558\n",
      "10559\n",
      "10560\n",
      "10561\n",
      "10562\n",
      "10563\n",
      "10564\n",
      "10565\n",
      "10566\n",
      "10567\n",
      "10568\n",
      "10569\n",
      "10570\n",
      "10571\n",
      "10572\n",
      "10573\n",
      "10574\n",
      "10575\n",
      "10576\n",
      "10577\n",
      "10578\n",
      "10579\n",
      "10580\n",
      "10581\n",
      "10582\n",
      "10583\n",
      "10584\n",
      "10585\n",
      "10586\n",
      "10587\n",
      "10588\n",
      "10589\n",
      "10590\n",
      "10591\n",
      "10592\n",
      "10593\n",
      "10594\n",
      "10595\n",
      "10596\n",
      "10597\n",
      "10598\n",
      "10599\n",
      "10600\n",
      "10601\n",
      "10602\n",
      "10603\n",
      "10604\n",
      "10605\n",
      "10606\n",
      "10607\n",
      "10608\n",
      "10609\n",
      "10610\n",
      "10611\n",
      "10612\n",
      "10613\n",
      "10614\n",
      "10615\n",
      "10616\n",
      "10617\n",
      "10618\n",
      "10619\n",
      "10620\n",
      "10621\n",
      "10622\n",
      "10623\n",
      "10624\n",
      "10625\n",
      "10626\n",
      "10627\n",
      "10628\n",
      "10629\n",
      "10630\n",
      "10631\n",
      "10632\n",
      "10633\n",
      "10634\n",
      "10635\n",
      "10636\n",
      "10637\n",
      "10638\n",
      "10639\n",
      "10640\n",
      "10641\n",
      "10642\n",
      "10643\n",
      "10644\n",
      "10645\n",
      "10646\n",
      "10647\n",
      "10648\n",
      "10649\n",
      "10650\n",
      "10651\n",
      "10652\n",
      "10653\n",
      "10654\n",
      "10655\n",
      "10656\n",
      "10657\n",
      "10658\n",
      "10659\n",
      "10660\n",
      "10661\n",
      "10662\n",
      "10663\n",
      "10664\n",
      "10665\n",
      "10666\n",
      "10667\n",
      "10668\n",
      "10669\n",
      "10670\n",
      "10671\n",
      "10672\n",
      "10673\n",
      "10674\n",
      "10675\n",
      "10676\n",
      "10677\n",
      "10678\n",
      "10679\n",
      "10680\n",
      "10681\n",
      "10682\n",
      "10683\n",
      "10684\n",
      "10685\n",
      "10686\n",
      "10687\n",
      "10688\n",
      "10689\n",
      "10690\n",
      "10691\n",
      "10692\n",
      "10693\n",
      "10694\n",
      "10695\n",
      "10696\n",
      "10697\n",
      "10698\n",
      "10699\n",
      "10700\n",
      "10701\n",
      "10702\n",
      "10703\n",
      "10704\n",
      "10705\n",
      "10706\n",
      "10707\n",
      "10708\n",
      "10709\n",
      "10710\n",
      "10711\n",
      "10712\n",
      "10713\n",
      "10714\n",
      "10715\n",
      "10716\n",
      "10717\n",
      "10718\n",
      "10719\n",
      "10720\n",
      "10721\n",
      "10722\n",
      "10723\n",
      "10724\n",
      "10725\n",
      "10726\n",
      "10727\n",
      "10728\n",
      "10729\n",
      "10730\n",
      "10731\n",
      "10732\n",
      "10733\n",
      "10734\n",
      "10735\n",
      "10736\n",
      "10737\n",
      "10738\n",
      "10739\n",
      "10740\n",
      "10741\n",
      "10742\n",
      "10743\n",
      "10744\n",
      "10745\n",
      "10746\n",
      "10747\n",
      "10748\n",
      "10749\n",
      "10750\n",
      "10751\n",
      "10752\n",
      "10753\n",
      "10754\n",
      "10755\n",
      "10756\n",
      "10757\n",
      "10758\n",
      "10759\n",
      "10760\n",
      "10761\n",
      "10762\n",
      "10763\n",
      "10764\n",
      "10765\n",
      "10766\n",
      "10767\n",
      "10768\n",
      "10769\n",
      "10770\n",
      "10771\n",
      "10772\n",
      "10773\n",
      "10774\n",
      "10775\n",
      "10776\n",
      "10777\n",
      "10778\n",
      "10779\n",
      "10780\n",
      "10781\n",
      "10782\n",
      "10783\n",
      "10784\n",
      "10785\n",
      "10786\n",
      "10787\n",
      "10788\n",
      "10789\n",
      "10790\n",
      "10791\n",
      "10792\n",
      "10793\n",
      "10794\n",
      "10795\n",
      "10796\n",
      "10797\n",
      "10798\n",
      "10799\n",
      "10800\n",
      "10801\n",
      "10802\n",
      "10803\n",
      "10804\n",
      "10805\n",
      "10806\n",
      "10807\n",
      "10808\n",
      "10809\n",
      "10810\n",
      "10811\n",
      "10812\n",
      "10813\n",
      "10814\n",
      "10815\n",
      "10816\n",
      "10817\n",
      "10818\n",
      "10819\n",
      "10820\n",
      "10821\n",
      "10822\n",
      "10823\n",
      "10824\n",
      "10825\n",
      "10826\n",
      "10827\n",
      "10828\n",
      "10829\n",
      "10830\n",
      "10831\n",
      "10832\n",
      "10833\n",
      "10834\n",
      "10835\n",
      "10836\n",
      "10837\n",
      "10838\n",
      "10839\n",
      "10840\n",
      "10841\n",
      "10842\n",
      "10843\n",
      "10844\n",
      "10845\n",
      "10846\n",
      "10847\n",
      "10848\n",
      "10849\n",
      "10850\n",
      "10851\n",
      "10852\n",
      "10853\n",
      "10854\n",
      "10855\n",
      "10856\n",
      "10857\n",
      "10858\n",
      "10859\n",
      "10860\n",
      "10861\n",
      "10862\n",
      "10863\n",
      "10864\n",
      "10865\n",
      "10866\n",
      "10867\n",
      "10868\n",
      "10869\n",
      "10870\n",
      "10871\n",
      "10872\n",
      "10873\n",
      "10874\n",
      "10875\n",
      "10876\n",
      "10877\n",
      "10878\n",
      "10879\n",
      "10880\n",
      "10881\n",
      "10882\n",
      "10883\n",
      "10884\n",
      "10885\n",
      "10886\n",
      "10887\n",
      "10888\n",
      "10889\n",
      "10890\n",
      "10891\n",
      "10892\n",
      "10893\n",
      "10894\n",
      "10895\n",
      "10896\n",
      "10897\n",
      "10898\n",
      "10899\n",
      "10900\n",
      "10901\n",
      "10902\n",
      "10903\n",
      "10904\n",
      "10905\n",
      "10906\n",
      "10907\n",
      "10908\n",
      "10909\n",
      "10910\n",
      "10911\n",
      "10912\n",
      "10913\n",
      "10914\n",
      "10915\n",
      "10916\n",
      "10917\n",
      "10918\n",
      "10919\n",
      "10920\n",
      "10921\n",
      "10922\n",
      "10923\n",
      "10924\n",
      "10925\n",
      "10926\n",
      "10927\n",
      "10928\n",
      "10929\n",
      "10930\n",
      "10931\n",
      "10932\n",
      "10933\n",
      "10934\n",
      "10935\n",
      "10936\n",
      "10937\n",
      "10938\n",
      "10939\n",
      "10940\n",
      "10941\n",
      "10942\n",
      "10943\n",
      "10944\n",
      "10945\n",
      "10946\n",
      "10947\n",
      "10948\n",
      "10949\n",
      "10950\n",
      "10951\n",
      "10952\n",
      "10953\n",
      "10954\n",
      "10955\n",
      "10956\n",
      "10957\n",
      "10958\n",
      "10959\n",
      "10960\n",
      "10961\n",
      "10962\n",
      "10963\n",
      "10964\n",
      "10965\n",
      "10966\n",
      "10967\n",
      "10968\n",
      "10969\n",
      "10970\n",
      "10971\n",
      "10972\n",
      "10973\n",
      "10974\n",
      "10975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976\n",
      "10977\n",
      "10978\n",
      "10979\n",
      "10980\n",
      "10981\n",
      "10982\n",
      "10983\n",
      "10984\n",
      "10985\n",
      "10986\n",
      "10987\n",
      "10988\n",
      "10989\n",
      "10990\n",
      "10991\n",
      "10992\n",
      "10993\n",
      "10994\n",
      "10995\n",
      "10996\n",
      "10997\n",
      "10998\n",
      "10999\n",
      "11000\n",
      "11001\n",
      "11002\n",
      "11003\n",
      "11004\n",
      "11005\n",
      "11006\n",
      "11007\n",
      "11008\n",
      "11009\n",
      "11010\n",
      "11011\n",
      "11012\n",
      "11013\n",
      "11014\n",
      "11015\n",
      "11016\n",
      "11017\n",
      "11018\n",
      "11019\n",
      "11020\n",
      "11021\n",
      "11022\n",
      "11023\n",
      "11024\n",
      "11025\n",
      "11026\n",
      "11027\n",
      "11028\n",
      "11029\n",
      "11030\n",
      "11031\n",
      "11032\n",
      "11033\n",
      "11034\n",
      "11035\n",
      "11036\n",
      "11037\n",
      "11038\n",
      "11039\n",
      "11040\n",
      "11041\n",
      "11042\n",
      "11043\n",
      "11044\n",
      "11045\n",
      "11046\n",
      "11047\n",
      "11048\n",
      "11049\n",
      "11050\n",
      "11051\n",
      "11052\n",
      "11053\n",
      "11054\n",
      "11055\n",
      "11056\n",
      "11057\n",
      "11058\n",
      "11059\n",
      "11060\n",
      "11061\n",
      "11062\n",
      "11063\n",
      "11064\n",
      "11065\n",
      "11066\n",
      "11067\n",
      "11068\n",
      "11069\n",
      "11070\n",
      "11071\n",
      "11072\n",
      "11073\n",
      "11074\n",
      "11075\n",
      "11076\n",
      "11077\n",
      "11078\n",
      "11079\n",
      "11080\n",
      "11081\n",
      "11082\n",
      "11083\n",
      "11084\n",
      "11085\n",
      "11086\n",
      "11087\n",
      "11088\n",
      "11089\n",
      "11090\n",
      "11091\n",
      "11092\n",
      "11093\n",
      "11094\n",
      "11095\n",
      "11096\n",
      "11097\n",
      "11098\n",
      "11099\n",
      "11100\n",
      "11101\n",
      "11102\n",
      "11103\n",
      "11104\n",
      "11105\n",
      "11106\n",
      "11107\n",
      "11108\n",
      "11109\n",
      "11110\n",
      "11111\n",
      "11112\n",
      "11113\n",
      "11114\n",
      "11115\n",
      "11116\n",
      "11117\n",
      "11118\n",
      "11119\n",
      "11120\n",
      "11121\n",
      "11122\n",
      "11123\n",
      "11124\n",
      "11125\n",
      "11126\n",
      "11127\n",
      "11128\n",
      "11129\n",
      "11130\n",
      "11131\n",
      "11132\n",
      "11133\n",
      "11134\n",
      "11135\n",
      "11136\n",
      "11137\n",
      "11138\n",
      "11139\n",
      "11140\n",
      "11141\n",
      "11142\n",
      "11143\n",
      "11144\n",
      "11145\n",
      "11146\n",
      "11147\n",
      "11148\n",
      "11149\n",
      "11150\n",
      "11151\n",
      "11152\n",
      "11153\n",
      "11154\n",
      "11155\n",
      "11156\n",
      "11157\n",
      "11158\n",
      "11159\n",
      "11160\n",
      "11161\n",
      "11162\n",
      "11163\n",
      "11164\n",
      "11165\n",
      "11166\n",
      "11167\n",
      "11168\n",
      "11169\n",
      "11170\n",
      "11171\n",
      "11172\n",
      "11173\n",
      "11174\n",
      "11175\n",
      "11176\n",
      "11177\n",
      "11178\n",
      "11179\n",
      "11180\n",
      "11181\n",
      "11182\n",
      "11183\n",
      "11184\n",
      "11185\n",
      "11186\n",
      "11187\n",
      "11188\n",
      "11189\n",
      "11190\n",
      "11191\n",
      "11192\n",
      "11193\n",
      "11194\n",
      "11195\n",
      "11196\n",
      "11197\n",
      "11198\n",
      "11199\n",
      "11200\n",
      "11201\n",
      "11202\n",
      "11203\n",
      "11204\n",
      "11205\n",
      "11206\n",
      "11207\n",
      "11208\n",
      "11209\n",
      "11210\n",
      "11211\n",
      "11212\n",
      "11213\n",
      "11214\n",
      "11215\n",
      "11216\n",
      "11217\n",
      "11218\n",
      "11219\n",
      "11220\n",
      "11221\n",
      "11222\n",
      "11223\n",
      "11224\n",
      "11225\n",
      "11226\n",
      "11227\n",
      "11228\n",
      "11229\n",
      "11230\n",
      "11231\n",
      "11232\n",
      "11233\n",
      "11234\n",
      "11235\n",
      "11236\n",
      "11237\n",
      "11238\n",
      "11239\n",
      "11240\n",
      "11241\n",
      "11242\n",
      "11243\n",
      "11244\n",
      "11245\n",
      "11246\n",
      "11247\n",
      "11248\n",
      "11249\n",
      "11250\n",
      "11251\n",
      "11252\n",
      "11253\n",
      "11254\n",
      "11255\n",
      "11256\n",
      "11257\n",
      "11258\n",
      "11259\n",
      "11260\n",
      "11261\n",
      "11262\n",
      "11263\n",
      "11264\n",
      "11265\n",
      "11266\n",
      "11267\n",
      "11268\n",
      "11269\n",
      "11270\n",
      "11271\n",
      "11272\n",
      "11273\n",
      "11274\n",
      "11275\n",
      "11276\n",
      "11277\n",
      "11278\n",
      "11279\n",
      "11280\n",
      "11281\n",
      "11282\n",
      "11283\n",
      "11284\n",
      "11285\n",
      "11286\n",
      "11287\n",
      "11288\n",
      "11289\n",
      "11290\n",
      "11291\n",
      "11292\n",
      "11293\n",
      "11294\n",
      "11295\n",
      "11296\n",
      "11297\n",
      "11298\n",
      "11299\n",
      "11300\n",
      "11301\n",
      "11302\n",
      "11303\n",
      "11304\n",
      "11305\n",
      "11306\n",
      "11307\n",
      "11308\n",
      "11309\n",
      "11310\n",
      "11311\n",
      "11312\n",
      "11313\n",
      "11314\n",
      "11315\n",
      "11316\n",
      "11317\n",
      "11318\n",
      "11319\n",
      "11320\n",
      "11321\n",
      "11322\n",
      "11323\n",
      "11324\n",
      "11325\n",
      "11326\n",
      "11327\n",
      "11328\n",
      "11329\n",
      "11330\n",
      "11331\n",
      "11332\n",
      "11333\n",
      "11334\n",
      "11335\n",
      "11336\n",
      "11337\n",
      "11338\n",
      "11339\n",
      "11340\n",
      "11341\n",
      "11342\n",
      "11343\n",
      "11344\n",
      "11345\n",
      "11346\n",
      "11347\n",
      "11348\n",
      "11349\n",
      "11350\n",
      "11351\n",
      "11352\n",
      "11353\n",
      "11354\n",
      "11355\n",
      "11356\n",
      "11357\n",
      "11358\n",
      "11359\n",
      "11360\n",
      "11361\n",
      "11362\n",
      "11363\n",
      "11364\n",
      "11365\n",
      "11366\n",
      "11367\n",
      "11368\n",
      "11369\n",
      "11370\n",
      "11371\n",
      "11372\n",
      "11373\n",
      "11374\n",
      "11375\n",
      "11376\n",
      "11377\n",
      "11378\n",
      "11379\n",
      "11380\n",
      "11381\n",
      "11382\n",
      "11383\n",
      "11384\n",
      "11385\n",
      "11386\n",
      "11387\n",
      "11388\n",
      "11389\n",
      "11390\n",
      "11391\n",
      "11392\n",
      "11393\n",
      "11394\n",
      "11395\n",
      "11396\n",
      "11397\n",
      "11398\n",
      "11399\n",
      "11400\n",
      "11401\n",
      "11402\n",
      "11403\n",
      "11404\n",
      "11405\n",
      "11406\n",
      "11407\n",
      "11408\n",
      "11409\n",
      "11410\n",
      "11411\n",
      "11412\n",
      "11413\n",
      "11414\n",
      "11415\n",
      "11416\n",
      "11417\n",
      "11418\n",
      "11419\n",
      "11420\n",
      "11421\n",
      "11422\n",
      "11423\n",
      "11424\n",
      "11425\n",
      "11426\n",
      "11427\n",
      "11428\n",
      "11429\n",
      "11430\n",
      "11431\n",
      "11432\n",
      "11433\n",
      "11434\n",
      "11435\n",
      "11436\n",
      "11437\n",
      "11438\n",
      "11439\n",
      "11440\n",
      "11441\n",
      "11442\n",
      "11443\n",
      "11444\n",
      "11445\n",
      "11446\n",
      "11447\n",
      "11448\n",
      "11449\n",
      "11450\n",
      "11451\n",
      "11452\n",
      "11453\n",
      "11454\n",
      "11455\n",
      "11456\n",
      "11457\n",
      "11458\n",
      "11459\n",
      "11460\n",
      "11461\n",
      "11462\n",
      "11463\n",
      "11464\n",
      "11465\n",
      "11466\n",
      "11467\n",
      "11468\n",
      "11469\n",
      "11470\n",
      "11471\n",
      "11472\n",
      "11473\n",
      "11474\n",
      "11475\n",
      "11476\n",
      "11477\n",
      "11478\n",
      "11479\n",
      "11480\n",
      "11481\n",
      "11482\n",
      "11483\n",
      "11484\n",
      "11485\n",
      "11486\n",
      "11487\n",
      "11488\n",
      "11489\n",
      "11490\n",
      "11491\n",
      "11492\n",
      "11493\n",
      "11494\n",
      "11495\n",
      "11496\n",
      "11497\n",
      "11498\n",
      "11499\n",
      "11500\n",
      "11501\n",
      "11502\n",
      "11503\n",
      "11504\n",
      "11505\n",
      "11506\n",
      "11507\n",
      "11508\n",
      "11509\n",
      "11510\n",
      "11511\n",
      "11512\n",
      "11513\n",
      "11514\n",
      "11515\n",
      "11516\n",
      "11517\n",
      "11518\n",
      "11519\n",
      "11520\n",
      "11521\n",
      "11522\n",
      "11523\n",
      "11524\n",
      "11525\n",
      "11526\n",
      "11527\n",
      "11528\n",
      "11529\n",
      "11530\n",
      "11531\n",
      "11532\n",
      "11533\n",
      "11534\n",
      "11535\n",
      "11536\n",
      "11537\n",
      "11538\n",
      "11539\n",
      "11540\n",
      "11541\n",
      "11542\n",
      "11543\n",
      "11544\n",
      "11545\n",
      "11546\n",
      "11547\n",
      "11548\n",
      "11549\n",
      "11550\n",
      "11551\n",
      "11552\n",
      "11553\n",
      "11554\n",
      "11555\n",
      "11556\n",
      "11557\n",
      "11558\n",
      "11559\n",
      "11560\n",
      "11561\n",
      "11562\n",
      "11563\n",
      "11564\n",
      "11565\n",
      "11566\n",
      "11567\n",
      "11568\n",
      "11569\n",
      "11570\n",
      "11571\n",
      "11572\n",
      "11573\n",
      "11574\n",
      "11575\n",
      "11576\n",
      "11577\n",
      "11578\n",
      "11579\n",
      "11580\n",
      "11581\n",
      "11582\n",
      "11583\n",
      "11584\n",
      "11585\n",
      "11586\n",
      "11587\n",
      "11588\n",
      "11589\n",
      "11590\n",
      "11591\n",
      "11592\n",
      "11593\n",
      "11594\n",
      "11595\n",
      "11596\n",
      "11597\n",
      "11598\n",
      "11599\n",
      "11600\n",
      "11601\n",
      "11602\n",
      "11603\n",
      "11604\n",
      "11605\n",
      "11606\n",
      "11607\n",
      "11608\n",
      "11609\n",
      "11610\n",
      "11611\n",
      "11612\n",
      "11613\n",
      "11614\n",
      "11615\n",
      "11616\n",
      "11617\n",
      "11618\n",
      "11619\n",
      "11620\n",
      "11621\n",
      "11622\n",
      "11623\n",
      "11624\n",
      "11625\n",
      "11626\n",
      "11627\n",
      "11628\n",
      "11629\n",
      "11630\n",
      "11631\n",
      "11632\n",
      "11633\n",
      "11634\n",
      "11635\n",
      "11636\n",
      "11637\n",
      "11638\n",
      "11639\n",
      "11640\n",
      "11641\n",
      "11642\n",
      "11643\n",
      "11644\n",
      "11645\n",
      "11646\n",
      "11647\n",
      "11648\n",
      "11649\n",
      "11650\n",
      "11651\n",
      "11652\n",
      "11653\n",
      "11654\n",
      "11655\n",
      "11656\n",
      "11657\n",
      "11658\n",
      "11659\n",
      "11660\n",
      "11661\n",
      "11662\n",
      "11663\n",
      "11664\n",
      "11665\n",
      "11666\n",
      "11667\n",
      "11668\n",
      "11669\n",
      "11670\n",
      "11671\n",
      "11672\n",
      "11673\n",
      "11674\n",
      "11675\n",
      "11676\n",
      "11677\n",
      "11678\n",
      "11679\n",
      "11680\n",
      "11681\n",
      "11682\n",
      "11683\n",
      "11684\n",
      "11685\n",
      "11686\n",
      "11687\n",
      "11688\n",
      "11689\n",
      "11690\n",
      "11691\n",
      "11692\n",
      "11693\n",
      "11694\n",
      "11695\n",
      "11696\n",
      "11697\n",
      "11698\n",
      "11699\n",
      "11700\n",
      "11701\n",
      "11702\n",
      "11703\n",
      "11704\n",
      "11705\n",
      "11706\n",
      "11707\n",
      "11708\n",
      "11709\n",
      "11710\n",
      "11711\n",
      "11712\n",
      "11713\n",
      "11714\n",
      "11715\n",
      "11716\n",
      "11717\n",
      "11718\n",
      "11719\n",
      "11720\n",
      "11721\n",
      "11722\n",
      "11723\n",
      "11724\n",
      "11725\n",
      "11726\n",
      "11727\n",
      "11728\n",
      "11729\n",
      "11730\n",
      "11731\n",
      "11732\n",
      "11733\n",
      "11734\n",
      "11735\n",
      "11736\n",
      "11737\n",
      "11738\n",
      "11739\n",
      "11740\n",
      "11741\n",
      "11742\n",
      "11743\n",
      "11744\n",
      "11745\n",
      "11746\n",
      "11747\n",
      "11748\n",
      "11749\n",
      "11750\n",
      "11751\n",
      "11752\n",
      "11753\n",
      "11754\n",
      "11755\n",
      "11756\n",
      "11757\n",
      "11758\n",
      "11759\n",
      "11760\n",
      "11761\n",
      "11762\n",
      "11763\n",
      "11764\n",
      "11765\n",
      "11766\n",
      "11767\n",
      "11768\n",
      "11769\n",
      "11770\n",
      "11771\n",
      "11772\n",
      "11773\n",
      "11774\n",
      "11775\n",
      "11776\n",
      "11777\n",
      "11778\n",
      "11779\n",
      "11780\n",
      "11781\n",
      "11782\n",
      "11783\n",
      "11784\n",
      "11785\n",
      "11786\n",
      "11787\n",
      "11788\n",
      "11789\n",
      "11790\n",
      "11791\n",
      "11792\n",
      "11793\n",
      "11794\n",
      "11795\n",
      "11796\n",
      "11797\n",
      "11798\n",
      "11799\n",
      "11800\n",
      "11801\n",
      "11802\n",
      "11803\n",
      "11804\n",
      "11805\n",
      "11806\n",
      "11807\n",
      "11808\n",
      "11809\n",
      "11810\n",
      "11811\n",
      "11812\n",
      "11813\n",
      "11814\n",
      "11815\n",
      "11816\n",
      "11817\n",
      "11818\n",
      "11819\n",
      "11820\n",
      "11821\n",
      "11822\n",
      "11823\n",
      "11824\n",
      "11825\n",
      "11826\n",
      "11827\n",
      "11828\n",
      "11829\n",
      "11830\n",
      "11831\n",
      "11832\n",
      "11833\n",
      "11834\n",
      "11835\n",
      "11836\n",
      "11837\n",
      "11838\n",
      "11839\n",
      "11840\n",
      "11841\n",
      "11842\n",
      "11843\n",
      "11844\n",
      "11845\n",
      "11846\n",
      "11847\n",
      "11848\n",
      "11849\n",
      "11850\n",
      "11851\n",
      "11852\n",
      "11853\n",
      "11854\n",
      "11855\n",
      "11856\n",
      "11857\n",
      "11858\n",
      "11859\n",
      "11860\n",
      "11861\n",
      "11862\n",
      "11863\n",
      "11864\n",
      "11865\n",
      "11866\n",
      "11867\n",
      "11868\n",
      "11869\n",
      "11870\n",
      "11871\n",
      "11872\n",
      "11873\n",
      "11874\n",
      "11875\n",
      "11876\n",
      "11877\n",
      "11878\n",
      "11879\n",
      "11880\n",
      "11881\n",
      "11882\n",
      "11883\n",
      "11884\n",
      "11885\n",
      "11886\n",
      "11887\n",
      "11888\n",
      "11889\n",
      "11890\n",
      "11891\n",
      "11892\n",
      "11893\n",
      "11894\n",
      "11895\n",
      "11896\n",
      "11897\n",
      "11898\n",
      "11899\n",
      "11900\n",
      "11901\n",
      "11902\n",
      "11903\n",
      "11904\n",
      "11905\n",
      "11906\n",
      "11907\n",
      "11908\n",
      "11909\n",
      "11910\n",
      "11911\n",
      "11912\n",
      "11913\n",
      "11914\n",
      "11915\n",
      "11916\n",
      "11917\n",
      "11918\n",
      "11919\n",
      "11920\n",
      "11921\n",
      "11922\n",
      "11923\n",
      "11924\n",
      "11925\n",
      "11926\n",
      "11927\n",
      "11928\n",
      "11929\n",
      "11930\n",
      "11931\n",
      "11932\n",
      "11933\n",
      "11934\n",
      "11935\n",
      "11936\n",
      "11937\n",
      "11938\n",
      "11939\n",
      "11940\n",
      "11941\n",
      "11942\n",
      "11943\n",
      "11944\n",
      "11945\n",
      "11946\n",
      "11947\n",
      "11948\n",
      "11949\n",
      "11950\n",
      "11951\n",
      "11952\n",
      "11953\n",
      "11954\n",
      "11955\n",
      "11956\n",
      "11957\n",
      "11958\n",
      "11959\n",
      "11960\n",
      "11961\n",
      "11962\n",
      "11963\n",
      "11964\n",
      "11965\n",
      "11966\n",
      "11967\n",
      "11968\n",
      "11969\n",
      "11970\n",
      "11971\n",
      "11972\n",
      "11973\n",
      "11974\n",
      "11975\n",
      "11976\n",
      "11977\n",
      "11978\n",
      "11979\n",
      "11980\n",
      "11981\n",
      "11982\n",
      "11983\n",
      "11984\n",
      "11985\n",
      "11986\n",
      "11987\n",
      "11988\n",
      "11989\n",
      "11990\n",
      "11991\n",
      "11992\n",
      "11993\n",
      "11994\n",
      "11995\n",
      "11996\n",
      "11997\n",
      "11998\n",
      "11999\n",
      "12000\n",
      "12001\n",
      "12002\n",
      "12003\n",
      "12004\n",
      "12005\n",
      "12006\n",
      "12007\n",
      "12008\n",
      "12009\n",
      "12010\n",
      "12011\n",
      "12012\n",
      "12013\n",
      "12014\n",
      "12015\n",
      "12016\n",
      "12017\n",
      "12018\n",
      "12019\n",
      "12020\n",
      "12021\n",
      "12022\n",
      "12023\n",
      "12024\n",
      "12025\n",
      "12026\n",
      "12027\n",
      "12028\n",
      "12029\n",
      "12030\n",
      "12031\n",
      "12032\n",
      "12033\n",
      "12034\n",
      "12035\n",
      "12036\n",
      "12037\n",
      "12038\n",
      "12039\n",
      "12040\n",
      "12041\n",
      "12042\n",
      "12043\n",
      "12044\n",
      "12045\n",
      "12046\n",
      "12047\n",
      "12048\n",
      "12049\n",
      "12050\n",
      "12051\n",
      "12052\n",
      "12053\n",
      "12054\n",
      "12055\n",
      "12056\n",
      "12057\n",
      "12058\n",
      "12059\n",
      "12060\n",
      "12061\n",
      "12062\n",
      "12063\n",
      "12064\n",
      "12065\n",
      "12066\n",
      "12067\n",
      "12068\n",
      "12069\n",
      "12070\n",
      "12071\n",
      "12072\n",
      "12073\n",
      "12074\n",
      "12075\n",
      "12076\n",
      "12077\n",
      "12078\n",
      "12079\n",
      "12080\n",
      "12081\n",
      "12082\n",
      "12083\n",
      "12084\n",
      "12085\n",
      "12086\n",
      "12087\n",
      "12088\n",
      "12089\n",
      "12090\n",
      "12091\n",
      "12092\n",
      "12093\n",
      "12094\n",
      "12095\n",
      "12096\n",
      "12097\n",
      "12098\n",
      "12099\n",
      "12100\n",
      "12101\n",
      "12102\n",
      "12103\n",
      "12104\n",
      "12105\n",
      "12106\n",
      "12107\n",
      "12108\n",
      "12109\n",
      "12110\n",
      "12111\n",
      "12112\n",
      "12113\n",
      "12114\n",
      "12115\n",
      "12116\n",
      "12117\n",
      "12118\n",
      "12119\n",
      "12120\n",
      "12121\n",
      "12122\n",
      "12123\n",
      "12124\n",
      "12125\n",
      "12126\n",
      "12127\n",
      "12128\n",
      "12129\n",
      "12130\n",
      "12131\n",
      "12132\n",
      "12133\n",
      "12134\n",
      "12135\n",
      "12136\n",
      "12137\n",
      "12138\n",
      "12139\n",
      "12140\n",
      "12141\n",
      "12142\n",
      "12143\n",
      "12144\n",
      "12145\n",
      "12146\n",
      "12147\n",
      "12148\n",
      "12149\n",
      "12150\n",
      "12151\n",
      "12152\n",
      "12153\n",
      "12154\n",
      "12155\n",
      "12156\n",
      "12157\n",
      "12158\n",
      "12159\n",
      "12160\n",
      "12161\n",
      "12162\n",
      "12163\n",
      "12164\n",
      "12165\n",
      "12166\n",
      "12167\n",
      "12168\n",
      "12169\n",
      "12170\n",
      "12171\n",
      "12172\n",
      "12173\n",
      "12174\n",
      "12175\n",
      "12176\n",
      "12177\n",
      "12178\n",
      "12179\n",
      "12180\n",
      "12181\n",
      "12182\n",
      "12183\n",
      "12184\n",
      "12185\n",
      "12186\n",
      "12187\n",
      "12188\n",
      "12189\n",
      "12190\n",
      "12191\n",
      "12192\n",
      "12193\n",
      "12194\n",
      "12195\n",
      "12196\n",
      "12197\n",
      "12198\n",
      "12199\n",
      "12200\n",
      "12201\n",
      "12202\n",
      "12203\n",
      "12204\n",
      "12205\n",
      "12206\n",
      "12207\n",
      "12208\n",
      "12209\n",
      "12210\n",
      "12211\n",
      "12212\n",
      "12213\n",
      "12214\n",
      "12215\n",
      "12216\n",
      "12217\n",
      "12218\n",
      "12219\n",
      "12220\n",
      "12221\n",
      "12222\n",
      "12223\n",
      "12224\n",
      "12225\n",
      "12226\n",
      "12227\n",
      "12228\n",
      "12229\n",
      "12230\n",
      "12231\n",
      "12232\n",
      "12233\n",
      "12234\n",
      "12235\n",
      "12236\n",
      "12237\n",
      "12238\n",
      "12239\n",
      "12240\n",
      "12241\n",
      "12242\n",
      "12243\n",
      "12244\n",
      "12245\n",
      "12246\n",
      "12247\n",
      "12248\n",
      "12249\n",
      "12250\n",
      "12251\n",
      "12252\n",
      "12253\n",
      "12254\n",
      "12255\n",
      "12256\n",
      "12257\n",
      "12258\n",
      "12259\n",
      "12260\n",
      "12261\n",
      "12262\n",
      "12263\n",
      "12264\n",
      "12265\n",
      "12266\n",
      "12267\n",
      "12268\n",
      "12269\n",
      "12270\n",
      "12271\n",
      "12272\n",
      "12273\n",
      "12274\n",
      "12275\n",
      "12276\n",
      "12277\n",
      "12278\n",
      "12279\n",
      "12280\n",
      "12281\n",
      "12282\n",
      "12283\n",
      "12284\n",
      "12285\n",
      "12286\n",
      "12287\n",
      "12288\n",
      "12289\n",
      "12290\n",
      "12291\n",
      "12292\n",
      "12293\n",
      "12294\n",
      "12295\n",
      "12296\n",
      "12297\n",
      "12298\n",
      "12299\n",
      "12300\n",
      "12301\n",
      "12302\n",
      "12303\n",
      "12304\n",
      "12305\n",
      "12306\n",
      "12307\n",
      "12308\n",
      "12309\n",
      "12310\n",
      "12311\n",
      "12312\n",
      "12313\n",
      "12314\n",
      "12315\n",
      "12316\n",
      "12317\n",
      "12318\n",
      "12319\n",
      "12320\n",
      "12321\n",
      "12322\n",
      "12323\n",
      "12324\n",
      "12325\n",
      "12326\n",
      "12327\n",
      "12328\n",
      "12329\n",
      "12330\n",
      "12331\n",
      "12332\n",
      "12333\n",
      "12334\n",
      "12335\n",
      "12336\n",
      "12337\n",
      "12338\n",
      "12339\n",
      "12340\n",
      "12341\n",
      "12342\n",
      "12343\n",
      "12344\n",
      "12345\n",
      "12346\n",
      "12347\n",
      "12348\n",
      "12349\n",
      "12350\n",
      "12351\n",
      "12352\n",
      "12353\n",
      "12354\n",
      "12355\n",
      "12356\n",
      "12357\n",
      "12358\n",
      "12359\n",
      "12360\n",
      "12361\n",
      "12362\n",
      "12363\n",
      "12364\n",
      "12365\n",
      "12366\n",
      "12367\n",
      "12368\n",
      "12369\n",
      "12370\n",
      "12371\n",
      "12372\n",
      "12373\n",
      "12374\n",
      "12375\n",
      "12376\n",
      "12377\n",
      "12378\n",
      "12379\n",
      "12380\n",
      "12381\n",
      "12382\n",
      "12383\n",
      "12384\n",
      "12385\n",
      "12386\n",
      "12387\n",
      "12388\n",
      "12389\n",
      "12390\n",
      "12391\n",
      "12392\n",
      "12393\n",
      "12394\n",
      "12395\n",
      "12396\n",
      "12397\n",
      "12398\n",
      "12399\n",
      "12400\n",
      "12401\n",
      "12402\n",
      "12403\n",
      "12404\n",
      "12405\n",
      "12406\n",
      "12407\n",
      "12408\n",
      "12409\n",
      "12410\n",
      "12411\n",
      "12412\n",
      "12413\n",
      "12414\n",
      "12415\n",
      "12416\n",
      "12417\n",
      "12418\n",
      "12419\n",
      "12420\n",
      "12421\n",
      "12422\n",
      "12423\n",
      "12424\n",
      "12425\n",
      "12426\n",
      "12427\n",
      "12428\n",
      "12429\n",
      "12430\n",
      "12431\n",
      "12432\n",
      "12433\n",
      "12434\n",
      "12435\n",
      "12436\n",
      "12437\n",
      "12438\n",
      "12439\n",
      "12440\n",
      "12441\n",
      "12442\n",
      "12443\n",
      "12444\n",
      "12445\n",
      "12446\n",
      "12447\n",
      "12448\n",
      "12449\n",
      "12450\n",
      "12451\n",
      "12452\n",
      "12453\n",
      "12454\n",
      "12455\n",
      "12456\n",
      "12457\n",
      "12458\n",
      "12459\n",
      "12460\n",
      "12461\n",
      "12462\n",
      "12463\n",
      "12464\n",
      "12465\n",
      "12466\n",
      "12467\n",
      "12468\n",
      "12469\n",
      "12470\n",
      "12471\n",
      "12472\n",
      "12473\n",
      "12474\n",
      "12475\n",
      "12476\n",
      "12477\n",
      "12478\n",
      "12479\n",
      "12480\n",
      "12481\n",
      "12482\n",
      "12483\n",
      "12484\n",
      "12485\n",
      "12486\n",
      "12487\n",
      "12488\n",
      "12489\n",
      "12490\n",
      "12491\n",
      "12492\n",
      "12493\n",
      "12494\n",
      "12495\n",
      "12496\n",
      "12497\n",
      "12498\n",
      "12499\n",
      "12500\n",
      "12501\n",
      "12502\n",
      "12503\n",
      "12504\n",
      "12505\n",
      "12506\n",
      "12507\n",
      "12508\n",
      "12509\n",
      "12510\n",
      "12511\n",
      "12512\n",
      "12513\n",
      "12514\n",
      "12515\n",
      "12516\n",
      "12517\n",
      "12518\n",
      "12519\n",
      "12520\n",
      "12521\n",
      "12522\n",
      "12523\n",
      "12524\n",
      "12525\n",
      "12526\n",
      "12527\n",
      "12528\n",
      "12529\n",
      "12530\n",
      "12531\n",
      "12532\n",
      "12533\n",
      "12534\n",
      "12535\n",
      "12536\n",
      "12537\n",
      "12538\n",
      "12539\n",
      "12540\n",
      "12541\n",
      "12542\n",
      "12543\n",
      "12544\n",
      "12545\n",
      "12546\n",
      "12547\n",
      "12548\n",
      "12549\n",
      "12550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12551\n",
      "12552\n",
      "12553\n",
      "12554\n",
      "12555\n",
      "12556\n",
      "12557\n",
      "12558\n",
      "12559\n",
      "12560\n",
      "12561\n",
      "12562\n",
      "12563\n",
      "12564\n",
      "12565\n",
      "12566\n",
      "12567\n",
      "12568\n",
      "12569\n",
      "12570\n",
      "12571\n",
      "12572\n",
      "12573\n",
      "12574\n",
      "12575\n",
      "12576\n",
      "12577\n",
      "12578\n",
      "12579\n",
      "12580\n",
      "12581\n",
      "12582\n",
      "12583\n",
      "12584\n",
      "12585\n",
      "12586\n",
      "12587\n",
      "12588\n",
      "12589\n",
      "12590\n",
      "12591\n",
      "12592\n",
      "12593\n",
      "12594\n",
      "12595\n",
      "12596\n",
      "12597\n",
      "12598\n",
      "12599\n",
      "12600\n",
      "12601\n",
      "12602\n",
      "12603\n",
      "12604\n",
      "12605\n",
      "12606\n",
      "12607\n",
      "12608\n",
      "12609\n",
      "12610\n",
      "12611\n",
      "12612\n",
      "12613\n",
      "12614\n",
      "12615\n",
      "12616\n",
      "12617\n",
      "12618\n",
      "12619\n",
      "12620\n",
      "12621\n",
      "12622\n",
      "12623\n",
      "12624\n",
      "12625\n",
      "12626\n",
      "12627\n",
      "12628\n",
      "12629\n",
      "12630\n",
      "12631\n",
      "12632\n",
      "12633\n",
      "12634\n",
      "12635\n",
      "12636\n",
      "12637\n",
      "12638\n",
      "12639\n",
      "12640\n",
      "12641\n",
      "12642\n",
      "12643\n",
      "12644\n",
      "12645\n",
      "12646\n",
      "12647\n",
      "12648\n",
      "12649\n",
      "12650\n",
      "12651\n",
      "12652\n",
      "12653\n",
      "12654\n",
      "12655\n",
      "12656\n",
      "12657\n",
      "12658\n",
      "12659\n",
      "12660\n",
      "12661\n",
      "12662\n",
      "12663\n",
      "12664\n",
      "12665\n",
      "12666\n",
      "12667\n",
      "12668\n",
      "12669\n",
      "12670\n",
      "12671\n",
      "12672\n",
      "12673\n",
      "12674\n",
      "12675\n",
      "12676\n",
      "12677\n",
      "12678\n",
      "12679\n",
      "12680\n",
      "12681\n",
      "12682\n",
      "12683\n",
      "12684\n",
      "12685\n",
      "12686\n",
      "12687\n",
      "12688\n",
      "12689\n",
      "12690\n",
      "12691\n",
      "12692\n",
      "12693\n",
      "12694\n",
      "12695\n",
      "12696\n",
      "12697\n",
      "12698\n",
      "12699\n",
      "12700\n",
      "12701\n",
      "12702\n",
      "12703\n",
      "12704\n",
      "12705\n",
      "12706\n",
      "12707\n",
      "12708\n",
      "12709\n",
      "12710\n",
      "12711\n",
      "12712\n",
      "12713\n",
      "12714\n",
      "12715\n",
      "12716\n",
      "12717\n",
      "12718\n",
      "12719\n",
      "12720\n",
      "12721\n",
      "12722\n",
      "12723\n",
      "12724\n",
      "12725\n",
      "12726\n",
      "12727\n",
      "12728\n",
      "12729\n",
      "12730\n",
      "12731\n",
      "12732\n",
      "12733\n",
      "12734\n",
      "12735\n",
      "12736\n",
      "12737\n",
      "12738\n",
      "12739\n",
      "12740\n",
      "12741\n",
      "12742\n",
      "12743\n",
      "12744\n",
      "12745\n",
      "12746\n",
      "12747\n",
      "12748\n",
      "12749\n",
      "12750\n",
      "12751\n",
      "12752\n",
      "12753\n",
      "12754\n",
      "12755\n",
      "12756\n",
      "12757\n",
      "12758\n",
      "12759\n",
      "12760\n",
      "12761\n",
      "12762\n",
      "12763\n",
      "12764\n",
      "12765\n",
      "12766\n",
      "12767\n",
      "12768\n",
      "12769\n",
      "12770\n",
      "12771\n",
      "12772\n",
      "12773\n",
      "12774\n",
      "12775\n",
      "12776\n",
      "12777\n",
      "12778\n",
      "12779\n",
      "12780\n",
      "12781\n",
      "12782\n",
      "12783\n",
      "12784\n",
      "12785\n",
      "12786\n",
      "12787\n",
      "12788\n",
      "12789\n",
      "12790\n",
      "12791\n",
      "12792\n",
      "12793\n",
      "12794\n",
      "12795\n",
      "12796\n",
      "12797\n",
      "12798\n",
      "12799\n",
      "12800\n",
      "12801\n",
      "12802\n",
      "12803\n",
      "12804\n",
      "12805\n",
      "12806\n",
      "12807\n",
      "12808\n",
      "12809\n",
      "12810\n",
      "12811\n",
      "12812\n",
      "12813\n",
      "12814\n",
      "12815\n",
      "12816\n",
      "12817\n",
      "12818\n",
      "12819\n",
      "12820\n",
      "12821\n",
      "12822\n",
      "12823\n",
      "12824\n",
      "12825\n",
      "12826\n",
      "12827\n",
      "12828\n",
      "12829\n",
      "12830\n",
      "12831\n",
      "12832\n",
      "12833\n",
      "12834\n",
      "12835\n",
      "12836\n",
      "12837\n",
      "12838\n",
      "12839\n",
      "12840\n",
      "12841\n",
      "12842\n",
      "12843\n",
      "12844\n",
      "12845\n",
      "12846\n",
      "12847\n",
      "12848\n",
      "12849\n",
      "12850\n",
      "12851\n",
      "12852\n",
      "12853\n",
      "12854\n",
      "12855\n",
      "12856\n",
      "12857\n",
      "12858\n",
      "12859\n",
      "12860\n",
      "12861\n",
      "12862\n",
      "12863\n",
      "12864\n",
      "12865\n",
      "12866\n",
      "12867\n",
      "12868\n",
      "12869\n",
      "12870\n",
      "12871\n",
      "12872\n",
      "12873\n",
      "12874\n",
      "12875\n",
      "12876\n",
      "12877\n",
      "12878\n",
      "12879\n",
      "12880\n",
      "12881\n",
      "12882\n",
      "12883\n",
      "12884\n",
      "12885\n",
      "12886\n",
      "12887\n",
      "12888\n",
      "12889\n",
      "12890\n",
      "12891\n",
      "12892\n",
      "12893\n",
      "12894\n",
      "12895\n",
      "12896\n",
      "12897\n",
      "12898\n",
      "12899\n",
      "12900\n",
      "12901\n",
      "12902\n",
      "12903\n",
      "12904\n",
      "12905\n",
      "12906\n",
      "12907\n",
      "12908\n",
      "12909\n",
      "12910\n",
      "12911\n",
      "12912\n",
      "12913\n",
      "12914\n",
      "12915\n",
      "12916\n",
      "12917\n",
      "12918\n",
      "12919\n",
      "12920\n",
      "12921\n",
      "12922\n",
      "12923\n",
      "12924\n",
      "12925\n",
      "12926\n",
      "12927\n",
      "12928\n",
      "12929\n",
      "12930\n",
      "12931\n",
      "12932\n",
      "12933\n",
      "12934\n",
      "12935\n",
      "12936\n",
      "12937\n",
      "12938\n",
      "12939\n",
      "12940\n",
      "12941\n",
      "12942\n",
      "12943\n",
      "12944\n",
      "12945\n",
      "12946\n",
      "12947\n",
      "12948\n",
      "12949\n",
      "12950\n",
      "12951\n",
      "12952\n",
      "12953\n",
      "12954\n",
      "12955\n",
      "12956\n",
      "12957\n",
      "12958\n",
      "12959\n",
      "12960\n",
      "12961\n",
      "12962\n",
      "12963\n",
      "12964\n",
      "12965\n",
      "12966\n",
      "12967\n",
      "12968\n",
      "12969\n",
      "12970\n",
      "12971\n",
      "12972\n",
      "12973\n",
      "12974\n",
      "12975\n",
      "12976\n",
      "12977\n",
      "12978\n",
      "12979\n",
      "12980\n",
      "12981\n",
      "12982\n",
      "12983\n",
      "12984\n",
      "12985\n",
      "12986\n",
      "12987\n",
      "12988\n",
      "12989\n",
      "12990\n",
      "12991\n",
      "12992\n",
      "12993\n",
      "12994\n",
      "12995\n",
      "12996\n",
      "12997\n",
      "12998\n",
      "12999\n",
      "13000\n",
      "13001\n",
      "13002\n",
      "13003\n",
      "13004\n",
      "13005\n",
      "13006\n",
      "13007\n",
      "13008\n",
      "13009\n",
      "13010\n",
      "13011\n",
      "13012\n",
      "13013\n",
      "13014\n",
      "13015\n",
      "13016\n",
      "13017\n",
      "13018\n",
      "13019\n",
      "13020\n",
      "13021\n",
      "13022\n",
      "13023\n",
      "13024\n",
      "13025\n",
      "13026\n",
      "13027\n",
      "13028\n",
      "13029\n",
      "13030\n",
      "13031\n",
      "13032\n",
      "13033\n",
      "13034\n",
      "13035\n",
      "13036\n",
      "13037\n",
      "13038\n",
      "13039\n",
      "13040\n",
      "13041\n",
      "13042\n",
      "13043\n",
      "13044\n",
      "13045\n",
      "13046\n",
      "13047\n",
      "13048\n",
      "13049\n",
      "13050\n",
      "13051\n",
      "13052\n",
      "13053\n",
      "13054\n",
      "13055\n",
      "13056\n",
      "13057\n",
      "13058\n",
      "13059\n",
      "13060\n",
      "13061\n",
      "13062\n",
      "13063\n",
      "13064\n",
      "13065\n",
      "13066\n",
      "13067\n",
      "13068\n",
      "13069\n",
      "13070\n",
      "13071\n",
      "13072\n",
      "13073\n",
      "13074\n",
      "13075\n",
      "13076\n",
      "13077\n",
      "13078\n",
      "13079\n",
      "13080\n",
      "13081\n",
      "13082\n",
      "13083\n",
      "13084\n",
      "13085\n",
      "13086\n",
      "13087\n",
      "13088\n",
      "13089\n",
      "13090\n",
      "13091\n",
      "13092\n",
      "13093\n",
      "13094\n",
      "13095\n",
      "13096\n",
      "13097\n",
      "13098\n",
      "13099\n",
      "13100\n",
      "13101\n",
      "13102\n",
      "13103\n",
      "13104\n",
      "13105\n",
      "13106\n",
      "13107\n",
      "13108\n",
      "13109\n",
      "13110\n",
      "13111\n",
      "13112\n",
      "13113\n",
      "13114\n",
      "13115\n",
      "13116\n",
      "13117\n",
      "13118\n",
      "13119\n",
      "13120\n",
      "13121\n",
      "13122\n",
      "13123\n",
      "13124\n",
      "13125\n",
      "13126\n",
      "13127\n",
      "13128\n",
      "13129\n",
      "13130\n",
      "13131\n",
      "13132\n",
      "13133\n",
      "13134\n",
      "13135\n",
      "13136\n",
      "13137\n",
      "13138\n",
      "13139\n",
      "13140\n",
      "13141\n",
      "13142\n",
      "13143\n",
      "13144\n",
      "13145\n",
      "13146\n",
      "13147\n",
      "13148\n",
      "13149\n",
      "13150\n",
      "13151\n",
      "13152\n",
      "13153\n",
      "13154\n",
      "13155\n",
      "13156\n",
      "13157\n",
      "13158\n",
      "13159\n",
      "13160\n",
      "13161\n",
      "13162\n",
      "13163\n",
      "13164\n",
      "13165\n",
      "13166\n",
      "13167\n",
      "13168\n",
      "13169\n",
      "13170\n",
      "13171\n",
      "13172\n",
      "13173\n",
      "13174\n",
      "13175\n",
      "13176\n",
      "13177\n",
      "13178\n",
      "13179\n",
      "13180\n",
      "13181\n",
      "13182\n",
      "13183\n",
      "13184\n",
      "13185\n",
      "13186\n",
      "13187\n",
      "13188\n",
      "13189\n",
      "13190\n",
      "13191\n",
      "13192\n",
      "13193\n",
      "13194\n",
      "13195\n",
      "13196\n",
      "13197\n",
      "13198\n",
      "13199\n",
      "13200\n",
      "13201\n",
      "13202\n",
      "13203\n",
      "13204\n",
      "13205\n",
      "13206\n",
      "13207\n",
      "13208\n",
      "13209\n",
      "13210\n",
      "13211\n",
      "13212\n",
      "13213\n",
      "13214\n",
      "13215\n",
      "13216\n",
      "13217\n",
      "13218\n",
      "13219\n",
      "13220\n",
      "13221\n",
      "13222\n",
      "13223\n",
      "13224\n",
      "13225\n",
      "13226\n",
      "13227\n",
      "13228\n",
      "13229\n",
      "13230\n",
      "13231\n",
      "13232\n",
      "13233\n",
      "13234\n",
      "13235\n",
      "13236\n",
      "13237\n",
      "13238\n",
      "13239\n",
      "13240\n",
      "13241\n",
      "13242\n",
      "13243\n",
      "13244\n",
      "13245\n",
      "13246\n",
      "13247\n",
      "13248\n",
      "13249\n",
      "13250\n",
      "13251\n",
      "13252\n",
      "13253\n",
      "13254\n",
      "13255\n",
      "13256\n",
      "13257\n",
      "13258\n",
      "13259\n",
      "13260\n",
      "13261\n",
      "13262\n",
      "13263\n",
      "13264\n",
      "13265\n",
      "13266\n",
      "13267\n",
      "13268\n",
      "13269\n",
      "13270\n",
      "13271\n",
      "13272\n",
      "13273\n",
      "13274\n",
      "13275\n",
      "13276\n",
      "13277\n",
      "13278\n",
      "13279\n",
      "13280\n",
      "13281\n",
      "13282\n",
      "13283\n",
      "13284\n",
      "13285\n",
      "13286\n",
      "13287\n",
      "13288\n",
      "13289\n",
      "13290\n",
      "13291\n",
      "13292\n",
      "13293\n",
      "13294\n",
      "13295\n",
      "13296\n",
      "13297\n",
      "13298\n",
      "13299\n",
      "13300\n",
      "13301\n",
      "13302\n",
      "13303\n",
      "13304\n",
      "13305\n",
      "13306\n",
      "13307\n",
      "13308\n",
      "13309\n",
      "13310\n",
      "13311\n",
      "13312\n",
      "13313\n",
      "13314\n",
      "13315\n",
      "13316\n",
      "13317\n",
      "13318\n",
      "13319\n",
      "13320\n",
      "13321\n",
      "13322\n",
      "13323\n",
      "13324\n",
      "13325\n",
      "13326\n",
      "13327\n",
      "13328\n",
      "13329\n",
      "13330\n",
      "13331\n",
      "13332\n",
      "13333\n",
      "13334\n",
      "13335\n",
      "13336\n",
      "13337\n",
      "13338\n",
      "13339\n",
      "13340\n",
      "13341\n",
      "13342\n",
      "13343\n",
      "13344\n",
      "13345\n",
      "13346\n",
      "13347\n",
      "13348\n",
      "13349\n",
      "13350\n",
      "13351\n",
      "13352\n",
      "13353\n",
      "13354\n",
      "13355\n",
      "13356\n",
      "13357\n",
      "13358\n",
      "13359\n",
      "13360\n",
      "13361\n",
      "13362\n",
      "13363\n",
      "13364\n",
      "13365\n",
      "13366\n",
      "13367\n",
      "13368\n",
      "13369\n",
      "13370\n",
      "13371\n",
      "13372\n",
      "13373\n",
      "13374\n",
      "13375\n",
      "13376\n",
      "13377\n",
      "13378\n",
      "13379\n",
      "13380\n",
      "13381\n",
      "13382\n",
      "13383\n",
      "13384\n",
      "13385\n",
      "13386\n",
      "13387\n",
      "13388\n",
      "13389\n",
      "13390\n",
      "13391\n",
      "13392\n",
      "13393\n",
      "13394\n",
      "13395\n",
      "13396\n",
      "13397\n",
      "13398\n",
      "13399\n",
      "13400\n",
      "13401\n",
      "13402\n",
      "13403\n",
      "13404\n",
      "13405\n",
      "13406\n",
      "13407\n",
      "13408\n",
      "13409\n",
      "13410\n",
      "13411\n",
      "13412\n",
      "13413\n",
      "13414\n",
      "13415\n",
      "13416\n",
      "13417\n",
      "13418\n",
      "13419\n",
      "13420\n",
      "13421\n",
      "13422\n",
      "13423\n",
      "13424\n",
      "13425\n",
      "13426\n",
      "13427\n",
      "13428\n",
      "13429\n",
      "13430\n",
      "13431\n",
      "13432\n",
      "13433\n",
      "13434\n",
      "13435\n",
      "13436\n",
      "13437\n",
      "13438\n",
      "13439\n",
      "13440\n",
      "13441\n",
      "13442\n",
      "13443\n",
      "13444\n",
      "13445\n",
      "13446\n",
      "13447\n",
      "13448\n",
      "13449\n",
      "13450\n",
      "13451\n",
      "13452\n",
      "13453\n",
      "13454\n",
      "13455\n",
      "13456\n",
      "13457\n",
      "13458\n",
      "13459\n",
      "13460\n",
      "13461\n",
      "13462\n",
      "13463\n",
      "13464\n",
      "13465\n",
      "13466\n",
      "13467\n",
      "13468\n",
      "13469\n",
      "13470\n",
      "13471\n",
      "13472\n",
      "13473\n",
      "13474\n",
      "13475\n",
      "13476\n",
      "13477\n",
      "13478\n",
      "13479\n",
      "13480\n",
      "13481\n",
      "13482\n",
      "13483\n",
      "13484\n",
      "13485\n",
      "13486\n",
      "13487\n",
      "13488\n",
      "13489\n",
      "13490\n",
      "13491\n",
      "13492\n",
      "13493\n",
      "13494\n",
      "13495\n",
      "13496\n",
      "13497\n",
      "13498\n",
      "13499\n",
      "13500\n",
      "13501\n",
      "13502\n",
      "13503\n",
      "13504\n",
      "13505\n",
      "13506\n",
      "13507\n",
      "13508\n",
      "13509\n",
      "13510\n",
      "13511\n",
      "13512\n",
      "13513\n",
      "13514\n",
      "13515\n",
      "13516\n",
      "13517\n",
      "13518\n",
      "13519\n",
      "13520\n",
      "13521\n",
      "13522\n",
      "13523\n",
      "13524\n",
      "13525\n",
      "13526\n",
      "13527\n",
      "13528\n",
      "13529\n",
      "13530\n",
      "13531\n",
      "13532\n",
      "13533\n",
      "13534\n",
      "13535\n",
      "13536\n",
      "13537\n",
      "13538\n",
      "13539\n",
      "13540\n",
      "13541\n",
      "13542\n",
      "13543\n",
      "13544\n",
      "13545\n",
      "13546\n",
      "13547\n",
      "13548\n",
      "13549\n",
      "13550\n",
      "13551\n",
      "13552\n",
      "13553\n",
      "13554\n",
      "13555\n",
      "13556\n",
      "13557\n",
      "13558\n",
      "13559\n",
      "13560\n",
      "13561\n",
      "13562\n",
      "13563\n",
      "13564\n",
      "13565\n",
      "13566\n",
      "13567\n",
      "13568\n",
      "13569\n",
      "13570\n",
      "13571\n",
      "13572\n",
      "13573\n",
      "13574\n",
      "13575\n",
      "13576\n",
      "13577\n",
      "13578\n",
      "13579\n",
      "13580\n",
      "13581\n",
      "13582\n",
      "13583\n",
      "13584\n",
      "13585\n",
      "13586\n",
      "13587\n",
      "13588\n",
      "13589\n",
      "13590\n",
      "13591\n",
      "13592\n",
      "13593\n",
      "13594\n",
      "13595\n",
      "13596\n",
      "13597\n",
      "13598\n",
      "13599\n",
      "13600\n",
      "13601\n",
      "13602\n",
      "13603\n",
      "13604\n",
      "13605\n",
      "13606\n",
      "13607\n",
      "13608\n",
      "13609\n",
      "13610\n",
      "13611\n",
      "13612\n",
      "13613\n",
      "13614\n",
      "13615\n",
      "13616\n",
      "13617\n",
      "13618\n",
      "13619\n",
      "13620\n",
      "13621\n",
      "13622\n",
      "13623\n",
      "13624\n",
      "13625\n",
      "13626\n",
      "13627\n",
      "13628\n",
      "13629\n",
      "13630\n",
      "13631\n",
      "13632\n",
      "13633\n",
      "13634\n",
      "13635\n",
      "13636\n",
      "13637\n",
      "13638\n",
      "13639\n",
      "13640\n",
      "13641\n",
      "13642\n",
      "13643\n",
      "13644\n",
      "13645\n",
      "13646\n",
      "13647\n",
      "13648\n",
      "13649\n",
      "13650\n",
      "13651\n",
      "13652\n",
      "13653\n",
      "13654\n",
      "13655\n",
      "13656\n",
      "13657\n",
      "13658\n",
      "13659\n",
      "13660\n",
      "13661\n",
      "13662\n",
      "13663\n",
      "13664\n",
      "13665\n",
      "13666\n",
      "13667\n",
      "13668\n",
      "13669\n",
      "13670\n",
      "13671\n",
      "13672\n",
      "13673\n",
      "13674\n",
      "13675\n",
      "13676\n",
      "13677\n",
      "13678\n",
      "13679\n",
      "13680\n",
      "13681\n",
      "13682\n",
      "13683\n",
      "13684\n",
      "13685\n",
      "13686\n",
      "13687\n",
      "13688\n",
      "13689\n",
      "13690\n",
      "13691\n",
      "13692\n",
      "13693\n",
      "13694\n",
      "13695\n",
      "13696\n",
      "13697\n",
      "13698\n",
      "13699\n",
      "13700\n",
      "13701\n",
      "13702\n",
      "13703\n",
      "13704\n",
      "13705\n",
      "13706\n",
      "13707\n",
      "13708\n",
      "13709\n",
      "13710\n",
      "13711\n",
      "13712\n",
      "13713\n",
      "13714\n",
      "13715\n",
      "13716\n",
      "13717\n",
      "13718\n",
      "13719\n",
      "13720\n",
      "13721\n",
      "13722\n",
      "13723\n",
      "13724\n",
      "13725\n",
      "13726\n",
      "13727\n",
      "13728\n",
      "13729\n",
      "13730\n",
      "13731\n",
      "13732\n",
      "13733\n",
      "13734\n",
      "13735\n",
      "13736\n",
      "13737\n",
      "13738\n",
      "13739\n",
      "13740\n",
      "13741\n",
      "13742\n",
      "13743\n",
      "13744\n",
      "13745\n",
      "13746\n",
      "13747\n",
      "13748\n",
      "13749\n",
      "13750\n",
      "13751\n",
      "13752\n",
      "13753\n",
      "13754\n",
      "13755\n",
      "13756\n",
      "13757\n",
      "13758\n",
      "13759\n",
      "13760\n",
      "13761\n",
      "13762\n",
      "13763\n",
      "13764\n",
      "13765\n",
      "13766\n",
      "13767\n",
      "13768\n",
      "13769\n",
      "13770\n",
      "13771\n",
      "13772\n",
      "13773\n",
      "13774\n",
      "13775\n",
      "13776\n",
      "13777\n",
      "13778\n",
      "13779\n",
      "13780\n",
      "13781\n",
      "13782\n",
      "13783\n",
      "13784\n",
      "13785\n",
      "13786\n",
      "13787\n",
      "13788\n",
      "13789\n",
      "13790\n",
      "13791\n",
      "13792\n",
      "13793\n",
      "13794\n",
      "13795\n",
      "13796\n",
      "13797\n",
      "13798\n",
      "13799\n",
      "13800\n",
      "13801\n",
      "13802\n",
      "13803\n",
      "13804\n",
      "13805\n",
      "13806\n",
      "13807\n",
      "13808\n",
      "13809\n",
      "13810\n",
      "13811\n",
      "13812\n",
      "13813\n",
      "13814\n",
      "13815\n",
      "13816\n",
      "13817\n",
      "13818\n",
      "13819\n",
      "13820\n",
      "13821\n",
      "13822\n",
      "13823\n",
      "13824\n",
      "13825\n",
      "13826\n",
      "13827\n",
      "13828\n",
      "13829\n",
      "13830\n",
      "13831\n",
      "13832\n",
      "13833\n",
      "13834\n",
      "13835\n",
      "13836\n",
      "13837\n",
      "13838\n",
      "13839\n",
      "13840\n",
      "13841\n",
      "13842\n",
      "13843\n",
      "13844\n",
      "13845\n",
      "13846\n",
      "13847\n",
      "13848\n",
      "13849\n",
      "13850\n",
      "13851\n",
      "13852\n",
      "13853\n",
      "13854\n",
      "13855\n",
      "13856\n",
      "13857\n",
      "13858\n",
      "13859\n",
      "13860\n",
      "13861\n",
      "13862\n",
      "13863\n",
      "13864\n",
      "13865\n",
      "13866\n",
      "13867\n",
      "13868\n",
      "13869\n",
      "13870\n",
      "13871\n",
      "13872\n",
      "13873\n",
      "13874\n",
      "13875\n",
      "13876\n",
      "13877\n",
      "13878\n",
      "13879\n",
      "13880\n",
      "13881\n",
      "13882\n",
      "13883\n",
      "13884\n",
      "13885\n",
      "13886\n",
      "13887\n",
      "13888\n",
      "13889\n",
      "13890\n",
      "13891\n",
      "13892\n",
      "13893\n",
      "13894\n",
      "13895\n",
      "13896\n",
      "13897\n",
      "13898\n",
      "13899\n",
      "13900\n",
      "13901\n",
      "13902\n",
      "13903\n",
      "13904\n",
      "13905\n",
      "13906\n",
      "13907\n",
      "13908\n",
      "13909\n",
      "13910\n",
      "13911\n",
      "13912\n",
      "13913\n",
      "13914\n",
      "13915\n",
      "13916\n",
      "13917\n",
      "13918\n",
      "13919\n",
      "13920\n",
      "13921\n",
      "13922\n",
      "13923\n",
      "13924\n",
      "13925\n",
      "13926\n",
      "13927\n",
      "13928\n",
      "13929\n",
      "13930\n",
      "13931\n",
      "13932\n",
      "13933\n",
      "13934\n",
      "13935\n",
      "13936\n",
      "13937\n",
      "13938\n",
      "13939\n",
      "13940\n",
      "13941\n",
      "13942\n",
      "13943\n",
      "13944\n",
      "13945\n",
      "13946\n",
      "13947\n",
      "13948\n",
      "13949\n",
      "13950\n",
      "13951\n",
      "13952\n",
      "13953\n",
      "13954\n",
      "13955\n",
      "13956\n",
      "13957\n",
      "13958\n",
      "13959\n",
      "13960\n",
      "13961\n",
      "13962\n",
      "13963\n",
      "13964\n",
      "13965\n",
      "13966\n",
      "13967\n",
      "13968\n",
      "13969\n",
      "13970\n",
      "13971\n",
      "13972\n",
      "13973\n",
      "13974\n",
      "13975\n",
      "13976\n",
      "13977\n",
      "13978\n",
      "13979\n",
      "13980\n",
      "13981\n",
      "13982\n",
      "13983\n",
      "13984\n",
      "13985\n",
      "13986\n",
      "13987\n",
      "13988\n",
      "13989\n",
      "13990\n",
      "13991\n",
      "13992\n",
      "13993\n",
      "13994\n",
      "13995\n",
      "13996\n",
      "13997\n",
      "13998\n",
      "13999\n",
      "14000\n",
      "14001\n",
      "14002\n",
      "14003\n",
      "14004\n",
      "14005\n",
      "14006\n",
      "14007\n",
      "14008\n",
      "14009\n",
      "14010\n",
      "14011\n",
      "14012\n",
      "14013\n",
      "14014\n",
      "14015\n",
      "14016\n",
      "14017\n",
      "14018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14019\n",
      "14020\n",
      "14021\n",
      "14022\n",
      "14023\n",
      "14024\n",
      "14025\n",
      "14026\n",
      "14027\n",
      "14028\n",
      "14029\n",
      "14030\n",
      "14031\n",
      "14032\n",
      "14033\n",
      "14034\n",
      "14035\n",
      "14036\n",
      "14037\n",
      "14038\n",
      "14039\n",
      "14040\n",
      "14041\n",
      "14042\n",
      "14043\n",
      "14044\n",
      "14045\n",
      "14046\n",
      "14047\n",
      "14048\n",
      "14049\n",
      "14050\n",
      "14051\n",
      "14052\n",
      "14053\n",
      "14054\n",
      "14055\n",
      "14056\n",
      "14057\n",
      "14058\n",
      "14059\n",
      "14060\n",
      "14061\n",
      "14062\n",
      "14063\n",
      "14064\n",
      "14065\n",
      "14066\n",
      "14067\n",
      "14068\n",
      "14069\n",
      "14070\n",
      "14071\n",
      "14072\n",
      "14073\n",
      "14074\n",
      "14075\n",
      "14076\n",
      "14077\n",
      "14078\n",
      "14079\n",
      "14080\n",
      "14081\n",
      "14082\n",
      "14083\n",
      "14084\n",
      "14085\n",
      "14086\n",
      "14087\n",
      "14088\n",
      "14089\n",
      "14090\n",
      "14091\n",
      "14092\n",
      "14093\n",
      "14094\n",
      "14095\n",
      "14096\n",
      "14097\n",
      "14098\n",
      "14099\n",
      "14100\n",
      "14101\n",
      "14102\n",
      "14103\n",
      "14104\n",
      "14105\n",
      "14106\n",
      "14107\n",
      "14108\n",
      "14109\n",
      "14110\n",
      "14111\n",
      "14112\n",
      "14113\n",
      "14114\n",
      "14115\n",
      "14116\n",
      "14117\n",
      "14118\n",
      "14119\n",
      "14120\n",
      "14121\n",
      "14122\n",
      "14123\n",
      "14124\n",
      "14125\n",
      "14126\n",
      "14127\n",
      "14128\n",
      "14129\n",
      "14130\n",
      "14131\n",
      "14132\n",
      "14133\n",
      "14134\n",
      "14135\n",
      "14136\n",
      "14137\n",
      "14138\n",
      "14139\n",
      "14140\n",
      "14141\n",
      "14142\n",
      "14143\n",
      "14144\n",
      "14145\n",
      "14146\n",
      "14147\n",
      "14148\n",
      "14149\n",
      "14150\n",
      "14151\n",
      "14152\n",
      "14153\n",
      "14154\n",
      "14155\n",
      "14156\n",
      "14157\n",
      "14158\n",
      "14159\n",
      "14160\n",
      "14161\n",
      "14162\n",
      "14163\n",
      "14164\n",
      "14165\n",
      "14166\n",
      "14167\n",
      "14168\n",
      "14169\n",
      "14170\n",
      "14171\n",
      "14172\n",
      "14173\n",
      "14174\n",
      "14175\n",
      "14176\n",
      "14177\n",
      "14178\n",
      "14179\n",
      "14180\n",
      "14181\n",
      "14182\n",
      "14183\n",
      "14184\n",
      "14185\n",
      "14186\n",
      "14187\n",
      "14188\n",
      "14189\n",
      "14190\n",
      "14191\n",
      "14192\n",
      "14193\n",
      "14194\n",
      "14195\n",
      "14196\n",
      "14197\n",
      "14198\n",
      "14199\n",
      "14200\n",
      "14201\n",
      "14202\n",
      "14203\n",
      "14204\n",
      "14205\n",
      "14206\n",
      "14207\n",
      "14208\n",
      "14209\n",
      "14210\n",
      "14211\n",
      "14212\n",
      "14213\n",
      "14214\n",
      "14215\n",
      "14216\n",
      "14217\n",
      "14218\n",
      "14219\n",
      "14220\n",
      "14221\n",
      "14222\n",
      "14223\n",
      "14224\n",
      "14225\n",
      "14226\n",
      "14227\n",
      "14228\n",
      "14229\n",
      "14230\n",
      "14231\n",
      "14232\n",
      "14233\n",
      "14234\n",
      "14235\n",
      "14236\n",
      "14237\n",
      "14238\n",
      "14239\n",
      "14240\n",
      "14241\n",
      "14242\n",
      "14243\n",
      "14244\n",
      "14245\n",
      "14246\n",
      "14247\n",
      "14248\n",
      "14249\n",
      "14250\n",
      "14251\n",
      "14252\n",
      "14253\n",
      "14254\n",
      "14255\n",
      "14256\n",
      "14257\n",
      "14258\n",
      "14259\n",
      "14260\n",
      "14261\n",
      "14262\n",
      "14263\n",
      "14264\n",
      "14265\n",
      "14266\n",
      "14267\n",
      "14268\n",
      "14269\n",
      "14270\n",
      "14271\n",
      "14272\n",
      "14273\n",
      "14274\n",
      "14275\n",
      "14276\n",
      "14277\n",
      "14278\n",
      "14279\n",
      "14280\n",
      "14281\n",
      "14282\n",
      "14283\n",
      "14284\n",
      "14285\n",
      "14286\n",
      "14287\n",
      "14288\n",
      "14289\n",
      "14290\n",
      "14291\n",
      "14292\n",
      "14293\n",
      "14294\n",
      "14295\n",
      "14296\n",
      "14297\n",
      "14298\n",
      "14299\n",
      "14300\n",
      "14301\n",
      "14302\n",
      "14303\n",
      "14304\n",
      "14305\n",
      "14306\n",
      "14307\n",
      "14308\n",
      "14309\n",
      "14310\n",
      "14311\n",
      "14312\n",
      "14313\n",
      "14314\n",
      "14315\n",
      "14316\n",
      "14317\n",
      "14318\n",
      "14319\n",
      "14320\n",
      "14321\n",
      "14322\n",
      "14323\n",
      "14324\n",
      "14325\n",
      "14326\n",
      "14327\n",
      "14328\n",
      "14329\n",
      "14330\n",
      "14331\n",
      "14332\n",
      "14333\n",
      "14334\n",
      "14335\n",
      "14336\n",
      "14337\n",
      "14338\n",
      "14339\n",
      "14340\n",
      "14341\n",
      "14342\n",
      "14343\n",
      "14344\n",
      "14345\n",
      "14346\n",
      "14347\n",
      "14348\n",
      "14349\n",
      "14350\n",
      "14351\n",
      "14352\n",
      "14353\n",
      "14354\n",
      "14355\n",
      "14356\n",
      "14357\n",
      "14358\n",
      "14359\n",
      "14360\n",
      "14361\n",
      "14362\n",
      "14363\n",
      "14364\n",
      "14365\n",
      "14366\n",
      "14367\n",
      "14368\n",
      "14369\n",
      "14370\n",
      "14371\n",
      "14372\n",
      "14373\n",
      "14374\n",
      "14375\n",
      "14376\n",
      "14377\n",
      "14378\n",
      "14379\n",
      "14380\n",
      "14381\n",
      "14382\n",
      "14383\n",
      "14384\n",
      "14385\n",
      "14386\n",
      "14387\n",
      "14388\n",
      "14389\n",
      "14390\n",
      "14391\n",
      "14392\n",
      "14393\n",
      "14394\n",
      "14395\n",
      "14396\n",
      "14397\n",
      "14398\n",
      "14399\n",
      "14400\n",
      "14401\n",
      "14402\n",
      "14403\n",
      "14404\n",
      "14405\n",
      "14406\n",
      "14407\n",
      "14408\n",
      "14409\n",
      "14410\n",
      "14411\n",
      "14412\n",
      "14413\n",
      "14414\n",
      "14415\n",
      "14416\n",
      "14417\n",
      "14418\n",
      "14419\n",
      "14420\n",
      "14421\n",
      "14422\n",
      "14423\n",
      "14424\n",
      "14425\n",
      "14426\n",
      "14427\n",
      "14428\n",
      "14429\n",
      "14430\n",
      "14431\n",
      "14432\n",
      "14433\n",
      "14434\n",
      "14435\n",
      "14436\n",
      "14437\n",
      "14438\n",
      "14439\n",
      "14440\n",
      "14441\n",
      "14442\n",
      "14443\n",
      "14444\n",
      "14445\n",
      "14446\n",
      "14447\n",
      "14448\n",
      "14449\n",
      "14450\n",
      "14451\n",
      "14452\n",
      "14453\n",
      "14454\n",
      "14455\n",
      "14456\n",
      "14457\n",
      "14458\n",
      "14459\n",
      "14460\n",
      "14461\n",
      "14462\n",
      "14463\n",
      "14464\n",
      "14465\n",
      "14466\n",
      "14467\n",
      "14468\n",
      "14469\n",
      "14470\n",
      "14471\n",
      "14472\n",
      "14473\n",
      "14474\n",
      "14475\n",
      "14476\n",
      "14477\n",
      "14478\n",
      "14479\n",
      "14480\n",
      "14481\n",
      "14482\n",
      "14483\n",
      "14484\n",
      "14485\n",
      "14486\n",
      "14487\n",
      "14488\n",
      "14489\n",
      "14490\n",
      "14491\n",
      "14492\n",
      "14493\n",
      "14494\n",
      "14495\n",
      "14496\n",
      "14497\n",
      "14498\n",
      "14499\n",
      "14500\n",
      "14501\n",
      "14502\n",
      "14503\n",
      "14504\n",
      "14505\n",
      "14506\n",
      "14507\n",
      "14508\n",
      "14509\n",
      "14510\n",
      "14511\n",
      "14512\n",
      "14513\n",
      "14514\n",
      "14515\n",
      "14516\n",
      "14517\n",
      "14518\n",
      "14519\n",
      "14520\n",
      "14521\n",
      "14522\n",
      "14523\n",
      "14524\n",
      "14525\n",
      "14526\n",
      "14527\n",
      "14528\n",
      "14529\n",
      "14530\n",
      "14531\n",
      "14532\n",
      "14533\n",
      "14534\n",
      "14535\n",
      "14536\n",
      "14537\n",
      "14538\n",
      "14539\n",
      "14540\n",
      "14541\n",
      "14542\n",
      "14543\n",
      "14544\n",
      "14545\n",
      "14546\n",
      "14547\n",
      "14548\n",
      "14549\n",
      "14550\n",
      "14551\n",
      "14552\n",
      "14553\n",
      "14554\n",
      "14555\n",
      "14556\n",
      "14557\n",
      "14558\n",
      "14559\n",
      "14560\n",
      "14561\n",
      "14562\n",
      "14563\n",
      "14564\n",
      "14565\n",
      "14566\n",
      "14567\n",
      "14568\n",
      "14569\n",
      "14570\n",
      "14571\n",
      "14572\n",
      "14573\n",
      "14574\n",
      "14575\n",
      "14576\n",
      "14577\n",
      "14578\n",
      "14579\n",
      "14580\n",
      "14581\n",
      "14582\n",
      "14583\n",
      "14584\n",
      "14585\n",
      "14586\n",
      "14587\n",
      "14588\n",
      "14589\n",
      "14590\n",
      "14591\n",
      "14592\n",
      "14593\n",
      "14594\n",
      "14595\n",
      "14596\n",
      "14597\n",
      "14598\n",
      "14599\n",
      "14600\n",
      "14601\n",
      "14602\n",
      "14603\n",
      "14604\n",
      "14605\n",
      "14606\n",
      "14607\n",
      "14608\n",
      "14609\n",
      "14610\n",
      "14611\n",
      "14612\n",
      "14613\n",
      "14614\n",
      "14615\n",
      "14616\n",
      "14617\n",
      "14618\n",
      "14619\n",
      "14620\n",
      "14621\n",
      "14622\n",
      "14623\n",
      "14624\n",
      "14625\n",
      "14626\n",
      "14627\n",
      "14628\n",
      "14629\n",
      "14630\n",
      "14631\n",
      "14632\n",
      "14633\n",
      "14634\n",
      "14635\n",
      "14636\n",
      "14637\n",
      "14638\n",
      "14639\n",
      "14640\n",
      "14641\n",
      "14642\n",
      "14643\n",
      "14644\n",
      "14645\n",
      "14646\n",
      "14647\n",
      "14648\n",
      "14649\n",
      "14650\n",
      "14651\n",
      "14652\n",
      "14653\n",
      "14654\n",
      "14655\n",
      "14656\n",
      "14657\n",
      "14658\n",
      "14659\n",
      "14660\n",
      "14661\n",
      "14662\n",
      "14663\n",
      "14664\n",
      "14665\n",
      "14666\n",
      "14667\n",
      "14668\n",
      "14669\n",
      "14670\n",
      "14671\n",
      "14672\n",
      "14673\n",
      "14674\n",
      "14675\n",
      "14676\n",
      "14677\n",
      "14678\n",
      "14679\n",
      "14680\n",
      "14681\n",
      "14682\n",
      "14683\n",
      "14684\n",
      "14685\n",
      "14686\n",
      "14687\n",
      "14688\n",
      "14689\n",
      "14690\n",
      "14691\n",
      "14692\n",
      "14693\n",
      "14694\n",
      "14695\n",
      "14696\n",
      "14697\n",
      "14698\n",
      "14699\n",
      "14700\n",
      "14701\n",
      "14702\n",
      "14703\n",
      "14704\n",
      "14705\n",
      "14706\n",
      "14707\n",
      "14708\n",
      "14709\n",
      "14710\n",
      "14711\n",
      "14712\n",
      "14713\n",
      "14714\n",
      "14715\n",
      "14716\n",
      "14717\n",
      "14718\n",
      "14719\n",
      "14720\n",
      "14721\n",
      "14722\n",
      "14723\n",
      "14724\n",
      "14725\n",
      "14726\n",
      "14727\n",
      "14728\n",
      "14729\n",
      "14730\n",
      "14731\n",
      "14732\n",
      "14733\n",
      "14734\n",
      "14735\n",
      "14736\n",
      "14737\n",
      "14738\n",
      "14739\n",
      "14740\n",
      "14741\n",
      "14742\n",
      "14743\n",
      "14744\n",
      "14745\n",
      "14746\n",
      "14747\n",
      "14748\n",
      "14749\n",
      "14750\n",
      "14751\n",
      "14752\n",
      "14753\n",
      "14754\n",
      "14755\n",
      "14756\n",
      "14757\n",
      "14758\n",
      "14759\n",
      "14760\n",
      "14761\n",
      "14762\n",
      "14763\n",
      "14764\n",
      "14765\n",
      "14766\n",
      "14767\n",
      "14768\n",
      "14769\n",
      "14770\n",
      "14771\n",
      "14772\n",
      "14773\n",
      "14774\n",
      "14775\n",
      "14776\n",
      "14777\n",
      "14778\n",
      "14779\n",
      "14780\n",
      "14781\n",
      "14782\n",
      "14783\n",
      "14784\n",
      "14785\n",
      "14786\n",
      "14787\n",
      "14788\n",
      "14789\n",
      "14790\n",
      "14791\n",
      "14792\n",
      "14793\n",
      "14794\n",
      "14795\n",
      "14796\n",
      "14797\n",
      "14798\n",
      "14799\n",
      "14800\n",
      "14801\n",
      "14802\n",
      "14803\n",
      "14804\n",
      "14805\n",
      "14806\n",
      "14807\n",
      "14808\n",
      "14809\n",
      "14810\n",
      "14811\n",
      "14812\n",
      "14813\n",
      "14814\n",
      "14815\n",
      "14816\n",
      "14817\n",
      "14818\n",
      "14819\n",
      "14820\n",
      "14821\n",
      "14822\n",
      "14823\n",
      "14824\n",
      "14825\n",
      "14826\n",
      "14827\n",
      "14828\n",
      "14829\n",
      "14830\n",
      "14831\n",
      "14832\n",
      "14833\n",
      "14834\n",
      "14835\n",
      "14836\n",
      "14837\n",
      "14838\n",
      "14839\n",
      "14840\n",
      "14841\n",
      "14842\n",
      "14843\n",
      "14844\n",
      "14845\n",
      "14846\n",
      "14847\n",
      "14848\n",
      "14849\n",
      "14850\n",
      "14851\n",
      "14852\n",
      "14853\n",
      "14854\n",
      "14855\n",
      "14856\n",
      "14857\n",
      "14858\n",
      "14859\n",
      "14860\n",
      "14861\n",
      "14862\n",
      "14863\n",
      "14864\n",
      "14865\n",
      "14866\n",
      "14867\n",
      "14868\n",
      "14869\n",
      "14870\n",
      "14871\n",
      "14872\n",
      "14873\n",
      "14874\n",
      "14875\n",
      "14876\n",
      "14877\n",
      "14878\n",
      "14879\n",
      "14880\n",
      "14881\n",
      "14882\n",
      "14883\n",
      "14884\n",
      "14885\n",
      "14886\n",
      "14887\n",
      "14888\n",
      "14889\n",
      "14890\n",
      "14891\n",
      "14892\n",
      "14893\n",
      "14894\n",
      "14895\n",
      "14896\n",
      "14897\n",
      "14898\n",
      "14899\n",
      "14900\n",
      "14901\n",
      "14902\n",
      "14903\n",
      "14904\n",
      "14905\n",
      "14906\n",
      "14907\n",
      "14908\n",
      "14909\n",
      "14910\n",
      "14911\n",
      "14912\n",
      "14913\n",
      "14914\n",
      "14915\n",
      "14916\n",
      "14917\n",
      "14918\n",
      "14919\n",
      "14920\n",
      "14921\n",
      "14922\n",
      "14923\n",
      "14924\n",
      "14925\n",
      "14926\n",
      "14927\n",
      "14928\n",
      "14929\n",
      "14930\n",
      "14931\n",
      "14932\n",
      "14933\n",
      "14934\n",
      "14935\n",
      "14936\n",
      "14937\n",
      "14938\n",
      "14939\n",
      "14940\n",
      "14941\n",
      "14942\n",
      "14943\n",
      "14944\n",
      "14945\n",
      "14946\n",
      "14947\n",
      "14948\n",
      "14949\n",
      "14950\n",
      "14951\n",
      "14952\n",
      "14953\n",
      "14954\n",
      "14955\n",
      "14956\n",
      "14957\n",
      "14958\n",
      "14959\n",
      "14960\n",
      "14961\n",
      "14962\n",
      "14963\n",
      "14964\n",
      "14965\n",
      "14966\n",
      "14967\n",
      "14968\n",
      "14969\n",
      "14970\n",
      "14971\n",
      "14972\n",
      "14973\n",
      "14974\n",
      "14975\n",
      "14976\n",
      "14977\n",
      "14978\n",
      "14979\n",
      "14980\n",
      "14981\n",
      "14982\n",
      "14983\n",
      "14984\n",
      "14985\n",
      "14986\n",
      "14987\n",
      "14988\n",
      "14989\n",
      "14990\n",
      "14991\n",
      "14992\n",
      "14993\n",
      "14994\n",
      "14995\n",
      "14996\n",
      "14997\n",
      "14998\n",
      "14999\n",
      "15000\n",
      "15001\n",
      "15002\n",
      "15003\n",
      "15004\n",
      "15005\n",
      "15006\n",
      "15007\n",
      "15008\n",
      "15009\n",
      "15010\n",
      "15011\n",
      "15012\n",
      "15013\n",
      "15014\n",
      "15015\n",
      "15016\n",
      "15017\n",
      "15018\n",
      "15019\n",
      "15020\n",
      "15021\n",
      "15022\n",
      "15023\n",
      "15024\n",
      "15025\n",
      "15026\n",
      "15027\n",
      "15028\n",
      "15029\n",
      "15030\n",
      "15031\n",
      "15032\n",
      "15033\n",
      "15034\n",
      "15035\n",
      "15036\n",
      "15037\n",
      "15038\n",
      "15039\n",
      "15040\n",
      "15041\n",
      "15042\n",
      "15043\n",
      "15044\n",
      "15045\n",
      "15046\n",
      "15047\n",
      "15048\n",
      "15049\n",
      "15050\n",
      "15051\n",
      "15052\n",
      "15053\n",
      "15054\n",
      "15055\n",
      "15056\n",
      "15057\n",
      "15058\n",
      "15059\n",
      "15060\n",
      "15061\n",
      "15062\n",
      "15063\n",
      "15064\n",
      "15065\n",
      "15066\n",
      "15067\n",
      "15068\n",
      "15069\n",
      "15070\n",
      "15071\n",
      "15072\n",
      "15073\n",
      "15074\n",
      "15075\n",
      "15076\n",
      "15077\n",
      "15078\n",
      "15079\n",
      "15080\n",
      "15081\n",
      "15082\n",
      "15083\n",
      "15084\n",
      "15085\n",
      "15086\n",
      "15087\n",
      "15088\n",
      "15089\n",
      "15090\n",
      "15091\n",
      "15092\n",
      "15093\n",
      "15094\n",
      "15095\n",
      "15096\n",
      "15097\n",
      "15098\n",
      "15099\n",
      "15100\n",
      "15101\n",
      "15102\n",
      "15103\n",
      "15104\n",
      "15105\n",
      "15106\n",
      "15107\n",
      "15108\n",
      "15109\n",
      "15110\n",
      "15111\n",
      "15112\n",
      "15113\n",
      "15114\n",
      "15115\n",
      "15116\n",
      "15117\n",
      "15118\n",
      "15119\n",
      "15120\n",
      "15121\n",
      "15122\n",
      "15123\n",
      "15124\n",
      "15125\n",
      "15126\n",
      "15127\n",
      "15128\n",
      "15129\n",
      "15130\n",
      "15131\n",
      "15132\n",
      "15133\n",
      "15134\n",
      "15135\n",
      "15136\n",
      "15137\n",
      "15138\n",
      "15139\n",
      "15140\n",
      "15141\n",
      "15142\n",
      "15143\n",
      "15144\n",
      "15145\n",
      "15146\n",
      "15147\n",
      "15148\n",
      "15149\n",
      "15150\n",
      "15151\n",
      "15152\n",
      "15153\n",
      "15154\n",
      "15155\n",
      "15156\n",
      "15157\n",
      "15158\n",
      "15159\n",
      "15160\n",
      "15161\n",
      "15162\n",
      "15163\n",
      "15164\n",
      "15165\n",
      "15166\n",
      "15167\n",
      "15168\n",
      "15169\n",
      "15170\n",
      "15171\n",
      "15172\n",
      "15173\n",
      "15174\n",
      "15175\n",
      "15176\n",
      "15177\n",
      "15178\n",
      "15179\n",
      "15180\n",
      "15181\n",
      "15182\n",
      "15183\n",
      "15184\n",
      "15185\n",
      "15186\n",
      "15187\n",
      "15188\n",
      "15189\n",
      "15190\n",
      "15191\n",
      "15192\n",
      "15193\n",
      "15194\n",
      "15195\n",
      "15196\n",
      "15197\n",
      "15198\n",
      "15199\n",
      "15200\n",
      "15201\n",
      "15202\n",
      "15203\n",
      "15204\n",
      "15205\n",
      "15206\n",
      "15207\n",
      "15208\n",
      "15209\n",
      "15210\n",
      "15211\n",
      "15212\n",
      "15213\n",
      "15214\n",
      "15215\n",
      "15216\n",
      "15217\n",
      "15218\n",
      "15219\n",
      "15220\n",
      "15221\n",
      "15222\n",
      "15223\n",
      "15224\n",
      "15225\n",
      "15226\n",
      "15227\n",
      "15228\n",
      "15229\n",
      "15230\n",
      "15231\n",
      "15232\n",
      "15233\n",
      "15234\n",
      "15235\n",
      "15236\n",
      "15237\n",
      "15238\n",
      "15239\n",
      "15240\n",
      "15241\n",
      "15242\n",
      "15243\n",
      "15244\n",
      "15245\n",
      "15246\n",
      "15247\n",
      "15248\n",
      "15249\n",
      "15250\n",
      "15251\n",
      "15252\n",
      "15253\n",
      "15254\n",
      "15255\n",
      "15256\n",
      "15257\n",
      "15258\n",
      "15259\n",
      "15260\n",
      "15261\n",
      "15262\n",
      "15263\n",
      "15264\n",
      "15265\n",
      "15266\n",
      "15267\n",
      "15268\n",
      "15269\n",
      "15270\n",
      "15271\n",
      "15272\n",
      "15273\n",
      "15274\n",
      "15275\n",
      "15276\n",
      "15277\n",
      "15278\n",
      "15279\n",
      "15280\n",
      "15281\n",
      "15282\n",
      "15283\n",
      "15284\n",
      "15285\n",
      "15286\n",
      "15287\n",
      "15288\n",
      "15289\n",
      "15290\n",
      "15291\n",
      "15292\n",
      "15293\n",
      "15294\n",
      "15295\n",
      "15296\n",
      "15297\n",
      "15298\n",
      "15299\n",
      "15300\n",
      "15301\n",
      "15302\n",
      "15303\n",
      "15304\n",
      "15305\n",
      "15306\n",
      "15307\n",
      "15308\n",
      "15309\n",
      "15310\n",
      "15311\n",
      "15312\n",
      "15313\n",
      "15314\n",
      "15315\n",
      "15316\n",
      "15317\n",
      "15318\n",
      "15319\n",
      "15320\n",
      "15321\n",
      "15322\n",
      "15323\n",
      "15324\n",
      "15325\n",
      "15326\n",
      "15327\n",
      "15328\n",
      "15329\n",
      "15330\n",
      "15331\n",
      "15332\n",
      "15333\n",
      "15334\n",
      "15335\n",
      "15336\n",
      "15337\n",
      "15338\n",
      "15339\n",
      "15340\n",
      "15341\n",
      "15342\n",
      "15343\n",
      "15344\n",
      "15345\n",
      "15346\n",
      "15347\n",
      "15348\n",
      "15349\n",
      "15350\n",
      "15351\n",
      "15352\n",
      "15353\n",
      "15354\n",
      "15355\n",
      "15356\n",
      "15357\n",
      "15358\n",
      "15359\n",
      "15360\n",
      "15361\n",
      "15362\n",
      "15363\n",
      "15364\n",
      "15365\n",
      "15366\n",
      "15367\n",
      "15368\n",
      "15369\n",
      "15370\n",
      "15371\n",
      "15372\n",
      "15373\n",
      "15374\n",
      "15375\n",
      "15376\n",
      "15377\n",
      "15378\n",
      "15379\n",
      "15380\n",
      "15381\n",
      "15382\n",
      "15383\n",
      "15384\n",
      "15385\n",
      "15386\n",
      "15387\n",
      "15388\n",
      "15389\n",
      "15390\n",
      "15391\n",
      "15392\n",
      "15393\n",
      "15394\n",
      "15395\n",
      "15396\n",
      "15397\n",
      "15398\n",
      "15399\n",
      "15400\n",
      "15401\n",
      "15402\n",
      "15403\n",
      "15404\n",
      "15405\n",
      "15406\n",
      "15407\n",
      "15408\n",
      "15409\n",
      "15410\n",
      "15411\n",
      "15412\n",
      "15413\n",
      "15414\n",
      "15415\n",
      "15416\n",
      "15417\n",
      "15418\n",
      "15419\n",
      "15420\n",
      "15421\n",
      "15422\n",
      "15423\n",
      "15424\n",
      "15425\n",
      "15426\n",
      "15427\n",
      "15428\n",
      "15429\n",
      "15430\n",
      "15431\n",
      "15432\n",
      "15433\n",
      "15434\n",
      "15435\n",
      "15436\n",
      "15437\n",
      "15438\n",
      "15439\n",
      "15440\n",
      "15441\n",
      "15442\n",
      "15443\n",
      "15444\n",
      "15445\n",
      "15446\n",
      "15447\n",
      "15448\n",
      "15449\n",
      "15450\n",
      "15451\n",
      "15452\n",
      "15453\n",
      "15454\n",
      "15455\n",
      "15456\n",
      "15457\n",
      "15458\n",
      "15459\n",
      "15460\n",
      "15461\n",
      "15462\n",
      "15463\n",
      "15464\n",
      "15465\n",
      "15466\n",
      "15467\n",
      "15468\n",
      "15469\n",
      "15470\n",
      "15471\n",
      "15472\n",
      "15473\n",
      "15474\n",
      "15475\n",
      "15476\n",
      "15477\n",
      "15478\n",
      "15479\n",
      "15480\n",
      "15481\n",
      "15482\n",
      "15483\n",
      "15484\n",
      "15485\n",
      "15486\n",
      "15487\n",
      "15488\n",
      "15489\n",
      "15490\n",
      "15491\n",
      "15492\n",
      "15493\n",
      "15494\n",
      "15495\n",
      "15496\n",
      "15497\n",
      "15498\n",
      "15499\n",
      "15500\n",
      "15501\n",
      "15502\n",
      "15503\n",
      "15504\n",
      "15505\n",
      "15506\n",
      "15507\n",
      "15508\n",
      "15509\n",
      "15510\n",
      "15511\n",
      "15512\n",
      "15513\n",
      "15514\n",
      "15515\n",
      "15516\n",
      "15517\n",
      "15518\n",
      "15519\n",
      "15520\n",
      "15521\n",
      "15522\n",
      "15523\n",
      "15524\n",
      "15525\n",
      "15526\n",
      "15527\n",
      "15528\n",
      "15529\n",
      "15530\n",
      "15531\n",
      "15532\n",
      "15533\n",
      "15534\n",
      "15535\n",
      "15536\n",
      "15537\n",
      "15538\n",
      "15539\n",
      "15540\n",
      "15541\n",
      "15542\n",
      "15543\n",
      "15544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15545\n",
      "15546\n",
      "15547\n",
      "15548\n",
      "15549\n",
      "15550\n",
      "15551\n",
      "15552\n",
      "15553\n",
      "15554\n",
      "15555\n",
      "15556\n",
      "15557\n",
      "15558\n",
      "15559\n",
      "15560\n",
      "15561\n",
      "15562\n",
      "15563\n",
      "15564\n",
      "15565\n",
      "15566\n",
      "15567\n",
      "15568\n",
      "15569\n",
      "15570\n",
      "15571\n",
      "15572\n",
      "15573\n",
      "15574\n",
      "15575\n",
      "15576\n",
      "15577\n",
      "15578\n",
      "15579\n",
      "15580\n",
      "15581\n",
      "15582\n",
      "15583\n",
      "15584\n",
      "15585\n",
      "15586\n",
      "15587\n",
      "15588\n",
      "15589\n",
      "15590\n",
      "15591\n",
      "15592\n",
      "15593\n",
      "15594\n",
      "15595\n",
      "15596\n",
      "15597\n",
      "15598\n",
      "15599\n",
      "15600\n",
      "15601\n",
      "15602\n",
      "15603\n",
      "15604\n",
      "15605\n",
      "15606\n",
      "15607\n",
      "15608\n",
      "15609\n",
      "15610\n",
      "15611\n",
      "15612\n",
      "15613\n",
      "15614\n",
      "15615\n",
      "15616\n",
      "15617\n",
      "15618\n",
      "15619\n",
      "15620\n",
      "15621\n",
      "15622\n",
      "15623\n",
      "15624\n",
      "15625\n",
      "15626\n",
      "15627\n",
      "15628\n",
      "15629\n",
      "15630\n",
      "15631\n",
      "15632\n",
      "15633\n",
      "15634\n",
      "15635\n",
      "15636\n",
      "15637\n",
      "15638\n",
      "15639\n",
      "15640\n",
      "15641\n",
      "15642\n",
      "15643\n",
      "15644\n",
      "15645\n",
      "15646\n",
      "15647\n",
      "15648\n",
      "15649\n",
      "15650\n",
      "15651\n",
      "15652\n",
      "15653\n",
      "15654\n",
      "15655\n",
      "15656\n",
      "15657\n",
      "15658\n",
      "15659\n",
      "15660\n",
      "15661\n",
      "15662\n",
      "15663\n",
      "15664\n",
      "15665\n",
      "15666\n",
      "15667\n",
      "15668\n",
      "15669\n",
      "15670\n",
      "15671\n",
      "15672\n",
      "15673\n",
      "15674\n",
      "15675\n",
      "15676\n",
      "15677\n",
      "15678\n",
      "15679\n",
      "15680\n",
      "15681\n",
      "15682\n",
      "15683\n",
      "15684\n",
      "15685\n",
      "15686\n",
      "15687\n",
      "15688\n",
      "15689\n",
      "15690\n",
      "15691\n",
      "15692\n",
      "15693\n",
      "15694\n",
      "15695\n",
      "15696\n",
      "15697\n",
      "15698\n",
      "15699\n",
      "15700\n",
      "15701\n",
      "15702\n",
      "15703\n",
      "15704\n",
      "15705\n",
      "15706\n",
      "15707\n",
      "15708\n",
      "15709\n",
      "15710\n",
      "15711\n",
      "15712\n",
      "15713\n",
      "15714\n",
      "15715\n",
      "15716\n",
      "15717\n",
      "15718\n",
      "15719\n",
      "15720\n",
      "15721\n",
      "15722\n",
      "15723\n",
      "15724\n",
      "15725\n",
      "15726\n",
      "15727\n",
      "15728\n",
      "15729\n",
      "15730\n",
      "15731\n",
      "15732\n",
      "15733\n",
      "15734\n",
      "15735\n",
      "15736\n",
      "15737\n",
      "15738\n",
      "15739\n",
      "15740\n",
      "15741\n",
      "15742\n",
      "15743\n",
      "15744\n",
      "15745\n",
      "15746\n",
      "15747\n",
      "15748\n",
      "15749\n",
      "15750\n",
      "15751\n",
      "15752\n",
      "15753\n",
      "15754\n",
      "15755\n",
      "15756\n",
      "15757\n",
      "15758\n",
      "15759\n",
      "15760\n",
      "15761\n",
      "15762\n",
      "15763\n",
      "15764\n",
      "15765\n",
      "15766\n",
      "15767\n",
      "15768\n",
      "15769\n",
      "15770\n",
      "15771\n",
      "15772\n",
      "15773\n",
      "15774\n",
      "15775\n",
      "15776\n",
      "15777\n",
      "15778\n",
      "15779\n",
      "15780\n",
      "15781\n",
      "15782\n",
      "15783\n",
      "15784\n",
      "15785\n",
      "15786\n",
      "15787\n",
      "15788\n",
      "15789\n",
      "15790\n",
      "15791\n",
      "15792\n",
      "15793\n",
      "15794\n",
      "15795\n",
      "15796\n",
      "15797\n",
      "15798\n",
      "15799\n",
      "15800\n",
      "15801\n",
      "15802\n",
      "15803\n",
      "15804\n",
      "15805\n",
      "15806\n",
      "15807\n",
      "15808\n",
      "15809\n",
      "15810\n",
      "15811\n",
      "15812\n",
      "15813\n",
      "15814\n",
      "15815\n",
      "15816\n",
      "15817\n",
      "15818\n",
      "15819\n",
      "15820\n",
      "15821\n",
      "15822\n",
      "15823\n",
      "15824\n",
      "15825\n",
      "15826\n",
      "15827\n",
      "15828\n",
      "15829\n",
      "15830\n",
      "15831\n",
      "15832\n",
      "15833\n",
      "15834\n",
      "15835\n",
      "15836\n",
      "15837\n",
      "15838\n",
      "15839\n",
      "15840\n",
      "15841\n",
      "15842\n",
      "15843\n",
      "15844\n",
      "15845\n",
      "15846\n",
      "15847\n",
      "15848\n",
      "15849\n",
      "15850\n",
      "15851\n",
      "15852\n",
      "15853\n",
      "15854\n",
      "15855\n",
      "15856\n",
      "15857\n",
      "15858\n",
      "15859\n",
      "15860\n",
      "15861\n",
      "15862\n",
      "15863\n",
      "15864\n",
      "15865\n",
      "15866\n",
      "15867\n",
      "15868\n",
      "15869\n",
      "15870\n",
      "15871\n",
      "15872\n",
      "15873\n",
      "15874\n",
      "15875\n",
      "15876\n",
      "15877\n",
      "15878\n",
      "15879\n",
      "15880\n",
      "15881\n",
      "15882\n",
      "15883\n",
      "15884\n",
      "15885\n",
      "15886\n",
      "15887\n",
      "15888\n",
      "15889\n",
      "15890\n",
      "15891\n",
      "15892\n",
      "15893\n",
      "15894\n",
      "15895\n",
      "15896\n",
      "15897\n",
      "15898\n",
      "15899\n",
      "15900\n",
      "15901\n",
      "15902\n",
      "15903\n",
      "15904\n",
      "15905\n",
      "15906\n",
      "15907\n",
      "15908\n",
      "15909\n",
      "15910\n",
      "15911\n",
      "15912\n",
      "15913\n",
      "15914\n",
      "15915\n",
      "15916\n",
      "15917\n",
      "15918\n",
      "15919\n",
      "15920\n",
      "15921\n",
      "15922\n",
      "15923\n",
      "15924\n",
      "15925\n",
      "15926\n",
      "15927\n",
      "15928\n",
      "15929\n",
      "15930\n",
      "15931\n",
      "15932\n",
      "15933\n",
      "15934\n",
      "15935\n",
      "15936\n",
      "15937\n",
      "15938\n",
      "15939\n",
      "15940\n",
      "15941\n",
      "15942\n",
      "15943\n",
      "15944\n",
      "15945\n",
      "15946\n",
      "15947\n",
      "15948\n",
      "15949\n",
      "15950\n",
      "15951\n",
      "15952\n",
      "15953\n",
      "15954\n",
      "15955\n",
      "15956\n",
      "15957\n",
      "15958\n",
      "15959\n",
      "15960\n",
      "15961\n",
      "15962\n",
      "15963\n",
      "15964\n",
      "15965\n",
      "15966\n",
      "15967\n",
      "15968\n",
      "15969\n",
      "15970\n",
      "15971\n",
      "15972\n",
      "15973\n",
      "15974\n",
      "15975\n",
      "15976\n",
      "15977\n",
      "15978\n",
      "15979\n",
      "15980\n",
      "15981\n",
      "15982\n",
      "15983\n",
      "15984\n",
      "15985\n",
      "15986\n",
      "15987\n",
      "15988\n",
      "15989\n",
      "15990\n",
      "15991\n",
      "15992\n",
      "15993\n",
      "15994\n",
      "15995\n",
      "15996\n",
      "15997\n",
      "15998\n",
      "15999\n",
      "16000\n",
      "16001\n",
      "16002\n",
      "16003\n",
      "16004\n",
      "16005\n",
      "16006\n",
      "16007\n",
      "16008\n",
      "16009\n",
      "16010\n",
      "16011\n",
      "16012\n",
      "16013\n",
      "16014\n",
      "16015\n",
      "16016\n",
      "16017\n",
      "16018\n",
      "16019\n",
      "16020\n",
      "16021\n",
      "16022\n",
      "16023\n",
      "16024\n",
      "16025\n",
      "16026\n",
      "16027\n",
      "16028\n",
      "16029\n",
      "16030\n",
      "16031\n",
      "16032\n",
      "16033\n",
      "16034\n",
      "16035\n",
      "16036\n",
      "16037\n",
      "16038\n",
      "16039\n",
      "16040\n",
      "16041\n",
      "16042\n",
      "16043\n",
      "16044\n",
      "16045\n",
      "16046\n",
      "16047\n",
      "16048\n",
      "16049\n",
      "16050\n",
      "16051\n",
      "16052\n",
      "16053\n",
      "16054\n",
      "16055\n",
      "16056\n",
      "16057\n",
      "16058\n",
      "16059\n",
      "16060\n",
      "16061\n",
      "16062\n",
      "16063\n",
      "16064\n",
      "16065\n",
      "16066\n",
      "16067\n",
      "16068\n",
      "16069\n",
      "16070\n",
      "16071\n",
      "16072\n",
      "16073\n",
      "16074\n",
      "16075\n",
      "16076\n",
      "16077\n",
      "16078\n",
      "16079\n",
      "16080\n",
      "16081\n",
      "16082\n",
      "16083\n",
      "16084\n",
      "16085\n",
      "16086\n",
      "16087\n",
      "16088\n",
      "16089\n",
      "16090\n",
      "16091\n",
      "16092\n",
      "16093\n",
      "16094\n",
      "16095\n",
      "16096\n",
      "16097\n",
      "16098\n",
      "16099\n",
      "16100\n",
      "16101\n",
      "16102\n",
      "16103\n",
      "16104\n",
      "16105\n",
      "16106\n",
      "16107\n",
      "16108\n",
      "16109\n",
      "16110\n",
      "16111\n",
      "16112\n",
      "16113\n",
      "16114\n",
      "16115\n",
      "16116\n",
      "16117\n",
      "16118\n",
      "16119\n",
      "16120\n",
      "16121\n",
      "16122\n",
      "16123\n",
      "16124\n",
      "16125\n",
      "16126\n",
      "16127\n",
      "16128\n",
      "16129\n",
      "16130\n",
      "16131\n",
      "16132\n",
      "16133\n",
      "16134\n",
      "16135\n",
      "16136\n",
      "16137\n",
      "16138\n",
      "16139\n",
      "16140\n",
      "16141\n",
      "16142\n",
      "16143\n",
      "16144\n",
      "16145\n",
      "16146\n",
      "16147\n",
      "16148\n",
      "16149\n",
      "16150\n",
      "16151\n",
      "16152\n",
      "16153\n",
      "16154\n",
      "16155\n",
      "16156\n",
      "16157\n",
      "16158\n",
      "16159\n",
      "16160\n",
      "16161\n",
      "16162\n",
      "16163\n",
      "16164\n",
      "16165\n",
      "16166\n",
      "16167\n",
      "16168\n",
      "16169\n",
      "16170\n",
      "16171\n",
      "16172\n",
      "16173\n",
      "16174\n",
      "16175\n",
      "16176\n",
      "16177\n",
      "16178\n",
      "16179\n",
      "16180\n",
      "16181\n",
      "16182\n",
      "16183\n",
      "16184\n",
      "16185\n",
      "16186\n",
      "16187\n",
      "16188\n",
      "16189\n",
      "16190\n",
      "16191\n",
      "16192\n",
      "16193\n",
      "16194\n",
      "16195\n",
      "16196\n",
      "16197\n",
      "16198\n",
      "16199\n",
      "16200\n",
      "16201\n",
      "16202\n",
      "16203\n",
      "16204\n",
      "16205\n",
      "16206\n",
      "16207\n",
      "16208\n",
      "16209\n",
      "16210\n",
      "16211\n",
      "16212\n",
      "16213\n",
      "16214\n",
      "16215\n",
      "16216\n",
      "16217\n",
      "16218\n",
      "16219\n",
      "16220\n",
      "16221\n",
      "16222\n",
      "16223\n",
      "16224\n",
      "16225\n",
      "16226\n",
      "16227\n",
      "16228\n",
      "16229\n",
      "16230\n",
      "16231\n",
      "16232\n",
      "16233\n",
      "16234\n",
      "16235\n",
      "16236\n",
      "16237\n",
      "16238\n",
      "16239\n",
      "16240\n",
      "16241\n",
      "16242\n",
      "16243\n",
      "16244\n",
      "16245\n",
      "16246\n",
      "16247\n",
      "16248\n",
      "16249\n",
      "16250\n",
      "16251\n",
      "16252\n",
      "16253\n",
      "16254\n",
      "16255\n",
      "16256\n",
      "16257\n",
      "16258\n",
      "16259\n",
      "16260\n",
      "16261\n",
      "16262\n",
      "16263\n",
      "16264\n",
      "16265\n",
      "16266\n",
      "16267\n",
      "16268\n",
      "16269\n",
      "16270\n",
      "16271\n",
      "16272\n",
      "16273\n",
      "16274\n",
      "16275\n",
      "16276\n",
      "16277\n",
      "16278\n",
      "16279\n",
      "16280\n",
      "16281\n",
      "16282\n",
      "16283\n",
      "16284\n",
      "16285\n",
      "16286\n",
      "16287\n",
      "16288\n",
      "16289\n",
      "16290\n",
      "16291\n",
      "16292\n",
      "16293\n",
      "16294\n",
      "16295\n",
      "16296\n",
      "16297\n",
      "16298\n",
      "16299\n",
      "16300\n",
      "16301\n",
      "16302\n",
      "16303\n",
      "16304\n",
      "16305\n",
      "16306\n",
      "16307\n",
      "16308\n",
      "16309\n",
      "16310\n",
      "16311\n",
      "16312\n",
      "16313\n",
      "16314\n",
      "16315\n",
      "16316\n",
      "16317\n",
      "16318\n",
      "16319\n",
      "16320\n",
      "16321\n",
      "16322\n",
      "16323\n",
      "16324\n",
      "16325\n",
      "16326\n",
      "16327\n",
      "16328\n",
      "16329\n",
      "16330\n",
      "16331\n",
      "16332\n",
      "16333\n",
      "16334\n",
      "16335\n",
      "16336\n",
      "16337\n",
      "16338\n",
      "16339\n",
      "16340\n",
      "16341\n",
      "16342\n",
      "16343\n",
      "16344\n",
      "16345\n",
      "16346\n",
      "16347\n",
      "16348\n",
      "16349\n",
      "16350\n",
      "16351\n",
      "16352\n",
      "16353\n",
      "16354\n",
      "16355\n",
      "16356\n",
      "16357\n",
      "16358\n",
      "16359\n",
      "16360\n",
      "16361\n",
      "16362\n",
      "16363\n",
      "16364\n",
      "16365\n",
      "16366\n",
      "16367\n",
      "16368\n",
      "16369\n",
      "16370\n",
      "16371\n",
      "16372\n",
      "16373\n",
      "16374\n",
      "16375\n",
      "16376\n",
      "16377\n",
      "16378\n",
      "16379\n",
      "16380\n",
      "16381\n",
      "16382\n",
      "16383\n",
      "16384\n",
      "16385\n",
      "16386\n",
      "16387\n",
      "16388\n",
      "16389\n",
      "16390\n",
      "16391\n",
      "16392\n",
      "16393\n",
      "16394\n",
      "16395\n",
      "16396\n",
      "16397\n",
      "16398\n",
      "16399\n",
      "16400\n",
      "16401\n",
      "16402\n",
      "16403\n",
      "16404\n",
      "16405\n",
      "16406\n",
      "16407\n",
      "16408\n",
      "16409\n",
      "16410\n",
      "16411\n",
      "16412\n",
      "16413\n",
      "16414\n",
      "16415\n",
      "16416\n",
      "16417\n",
      "16418\n",
      "16419\n",
      "16420\n",
      "16421\n",
      "16422\n",
      "16423\n",
      "16424\n",
      "16425\n",
      "16426\n",
      "16427\n",
      "16428\n",
      "16429\n",
      "16430\n",
      "16431\n",
      "16432\n",
      "16433\n",
      "16434\n",
      "16435\n",
      "16436\n",
      "16437\n",
      "16438\n",
      "16439\n",
      "16440\n",
      "16441\n",
      "16442\n",
      "16443\n",
      "16444\n",
      "16445\n",
      "16446\n",
      "16447\n",
      "16448\n",
      "16449\n",
      "16450\n",
      "16451\n",
      "16452\n",
      "16453\n",
      "16454\n",
      "16455\n",
      "16456\n",
      "16457\n",
      "16458\n",
      "16459\n",
      "16460\n",
      "16461\n",
      "16462\n",
      "16463\n",
      "16464\n",
      "16465\n",
      "16466\n",
      "16467\n",
      "16468\n",
      "16469\n",
      "16470\n",
      "16471\n",
      "16472\n",
      "16473\n",
      "16474\n",
      "16475\n",
      "16476\n",
      "16477\n",
      "16478\n",
      "16479\n",
      "16480\n",
      "16481\n",
      "16482\n",
      "16483\n",
      "16484\n",
      "16485\n",
      "16486\n",
      "16487\n",
      "16488\n",
      "16489\n",
      "16490\n",
      "16491\n",
      "16492\n",
      "16493\n",
      "16494\n",
      "16495\n",
      "16496\n",
      "16497\n",
      "16498\n",
      "16499\n",
      "16500\n",
      "16501\n",
      "16502\n",
      "16503\n",
      "16504\n",
      "16505\n",
      "16506\n",
      "16507\n",
      "16508\n",
      "16509\n",
      "16510\n",
      "16511\n",
      "16512\n",
      "16513\n",
      "16514\n",
      "16515\n",
      "16516\n",
      "16517\n",
      "16518\n",
      "16519\n",
      "16520\n",
      "16521\n",
      "16522\n",
      "16523\n",
      "16524\n",
      "16525\n",
      "16526\n",
      "16527\n",
      "16528\n",
      "16529\n",
      "16530\n",
      "16531\n",
      "16532\n",
      "16533\n",
      "16534\n",
      "16535\n",
      "16536\n",
      "16537\n",
      "16538\n",
      "16539\n",
      "16540\n",
      "16541\n",
      "16542\n",
      "16543\n",
      "16544\n",
      "16545\n",
      "16546\n",
      "16547\n",
      "16548\n",
      "16549\n",
      "16550\n",
      "16551\n",
      "16552\n",
      "16553\n",
      "16554\n",
      "16555\n",
      "16556\n",
      "16557\n",
      "16558\n",
      "16559\n",
      "16560\n",
      "16561\n",
      "16562\n",
      "16563\n",
      "16564\n",
      "16565\n",
      "16566\n",
      "16567\n",
      "16568\n",
      "16569\n",
      "16570\n",
      "16571\n",
      "16572\n",
      "16573\n",
      "16574\n",
      "16575\n",
      "16576\n",
      "16577\n",
      "16578\n",
      "16579\n",
      "16580\n",
      "16581\n",
      "16582\n",
      "16583\n",
      "16584\n",
      "16585\n",
      "16586\n",
      "16587\n",
      "16588\n",
      "16589\n",
      "16590\n",
      "16591\n",
      "16592\n",
      "16593\n",
      "16594\n",
      "16595\n",
      "16596\n",
      "16597\n",
      "16598\n",
      "16599\n",
      "16600\n",
      "16601\n",
      "16602\n",
      "16603\n",
      "16604\n",
      "16605\n",
      "16606\n",
      "16607\n",
      "16608\n",
      "16609\n",
      "16610\n",
      "16611\n",
      "16612\n",
      "16613\n",
      "16614\n",
      "16615\n",
      "16616\n",
      "16617\n",
      "16618\n",
      "16619\n",
      "16620\n",
      "16621\n",
      "16622\n",
      "16623\n",
      "16624\n",
      "16625\n",
      "16626\n",
      "16627\n",
      "16628\n",
      "16629\n",
      "16630\n",
      "16631\n",
      "16632\n",
      "16633\n",
      "16634\n",
      "16635\n",
      "16636\n",
      "16637\n",
      "16638\n",
      "16639\n",
      "16640\n",
      "16641\n",
      "16642\n",
      "16643\n",
      "16644\n",
      "16645\n",
      "16646\n",
      "16647\n",
      "16648\n",
      "16649\n",
      "16650\n",
      "16651\n",
      "16652\n",
      "16653\n",
      "16654\n",
      "16655\n",
      "16656\n",
      "16657\n",
      "16658\n",
      "16659\n",
      "16660\n",
      "16661\n",
      "16662\n",
      "16663\n",
      "16664\n",
      "16665\n",
      "16666\n",
      "16667\n",
      "16668\n",
      "16669\n",
      "16670\n",
      "16671\n",
      "16672\n",
      "16673\n",
      "16674\n",
      "16675\n",
      "16676\n",
      "16677\n",
      "16678\n",
      "16679\n",
      "16680\n",
      "16681\n",
      "16682\n",
      "16683\n",
      "16684\n",
      "16685\n",
      "16686\n",
      "16687\n",
      "16688\n",
      "16689\n",
      "16690\n",
      "16691\n",
      "16692\n",
      "16693\n",
      "16694\n",
      "16695\n",
      "16696\n",
      "16697\n",
      "16698\n",
      "16699\n",
      "16700\n",
      "16701\n",
      "16702\n",
      "16703\n",
      "16704\n",
      "16705\n",
      "16706\n",
      "16707\n",
      "16708\n",
      "16709\n",
      "16710\n",
      "16711\n",
      "16712\n",
      "16713\n",
      "16714\n",
      "16715\n",
      "16716\n",
      "16717\n",
      "16718\n",
      "16719\n",
      "16720\n",
      "16721\n",
      "16722\n",
      "16723\n",
      "16724\n",
      "16725\n",
      "16726\n",
      "16727\n",
      "16728\n",
      "16729\n",
      "16730\n",
      "16731\n",
      "16732\n",
      "16733\n",
      "16734\n",
      "16735\n",
      "16736\n",
      "16737\n",
      "16738\n",
      "16739\n",
      "16740\n",
      "16741\n",
      "16742\n",
      "16743\n",
      "16744\n",
      "16745\n",
      "16746\n",
      "16747\n",
      "16748\n",
      "16749\n",
      "16750\n",
      "16751\n",
      "16752\n",
      "16753\n",
      "16754\n",
      "16755\n",
      "16756\n",
      "16757\n",
      "16758\n",
      "16759\n",
      "16760\n",
      "16761\n",
      "16762\n",
      "16763\n",
      "16764\n",
      "16765\n",
      "16766\n",
      "16767\n",
      "16768\n",
      "16769\n",
      "16770\n",
      "16771\n",
      "16772\n",
      "16773\n",
      "16774\n",
      "16775\n",
      "16776\n",
      "16777\n",
      "16778\n",
      "16779\n",
      "16780\n",
      "16781\n",
      "16782\n",
      "16783\n",
      "16784\n",
      "16785\n",
      "16786\n",
      "16787\n",
      "16788\n",
      "16789\n",
      "16790\n",
      "16791\n",
      "16792\n",
      "16793\n",
      "16794\n",
      "16795\n",
      "16796\n",
      "16797\n",
      "16798\n",
      "16799\n",
      "16800\n",
      "16801\n",
      "16802\n",
      "16803\n",
      "16804\n",
      "16805\n",
      "16806\n",
      "16807\n",
      "16808\n",
      "16809\n",
      "16810\n",
      "16811\n",
      "16812\n",
      "16813\n",
      "16814\n",
      "16815\n",
      "16816\n",
      "16817\n",
      "16818\n",
      "16819\n",
      "16820\n",
      "16821\n",
      "16822\n",
      "16823\n",
      "16824\n",
      "16825\n",
      "16826\n",
      "16827\n",
      "16828\n",
      "16829\n",
      "16830\n",
      "16831\n",
      "16832\n",
      "16833\n",
      "16834\n",
      "16835\n",
      "16836\n",
      "16837\n",
      "16838\n",
      "16839\n",
      "16840\n",
      "16841\n",
      "16842\n",
      "16843\n",
      "16844\n",
      "16845\n",
      "16846\n",
      "16847\n",
      "16848\n",
      "16849\n",
      "16850\n",
      "16851\n",
      "16852\n",
      "16853\n",
      "16854\n",
      "16855\n",
      "16856\n",
      "16857\n",
      "16858\n",
      "16859\n",
      "16860\n",
      "16861\n",
      "16862\n",
      "16863\n",
      "16864\n",
      "16865\n",
      "16866\n",
      "16867\n",
      "16868\n",
      "16869\n",
      "16870\n",
      "16871\n",
      "16872\n",
      "16873\n",
      "16874\n",
      "16875\n",
      "16876\n",
      "16877\n",
      "16878\n",
      "16879\n",
      "16880\n",
      "16881\n",
      "16882\n",
      "16883\n",
      "16884\n",
      "16885\n",
      "16886\n",
      "16887\n",
      "16888\n",
      "16889\n",
      "16890\n",
      "16891\n",
      "16892\n",
      "16893\n",
      "16894\n",
      "16895\n",
      "16896\n",
      "16897\n",
      "16898\n",
      "16899\n",
      "16900\n",
      "16901\n",
      "16902\n",
      "16903\n",
      "16904\n",
      "16905\n",
      "16906\n",
      "16907\n",
      "16908\n",
      "16909\n",
      "16910\n",
      "16911\n",
      "16912\n",
      "16913\n",
      "16914\n",
      "16915\n",
      "16916\n",
      "16917\n",
      "16918\n",
      "16919\n",
      "16920\n",
      "16921\n",
      "16922\n",
      "16923\n",
      "16924\n",
      "16925\n",
      "16926\n",
      "16927\n",
      "16928\n",
      "16929\n",
      "16930\n",
      "16931\n",
      "16932\n",
      "16933\n",
      "16934\n",
      "16935\n",
      "16936\n",
      "16937\n",
      "16938\n",
      "16939\n",
      "16940\n",
      "16941\n",
      "16942\n",
      "16943\n",
      "16944\n",
      "16945\n",
      "16946\n",
      "16947\n",
      "16948\n",
      "16949\n",
      "16950\n",
      "16951\n",
      "16952\n",
      "16953\n",
      "16954\n",
      "16955\n",
      "16956\n",
      "16957\n",
      "16958\n",
      "16959\n",
      "16960\n",
      "16961\n",
      "16962\n",
      "16963\n",
      "16964\n",
      "16965\n",
      "16966\n",
      "16967\n",
      "16968\n",
      "16969\n",
      "16970\n",
      "16971\n",
      "16972\n",
      "16973\n",
      "16974\n",
      "16975\n",
      "16976\n",
      "16977\n",
      "16978\n",
      "16979\n",
      "16980\n",
      "16981\n",
      "16982\n",
      "16983\n",
      "16984\n",
      "16985\n",
      "16986\n",
      "16987\n",
      "16988\n",
      "16989\n",
      "16990\n",
      "16991\n",
      "16992\n",
      "16993\n",
      "16994\n",
      "16995\n",
      "16996\n",
      "16997\n",
      "16998\n",
      "16999\n",
      "17000\n",
      "17001\n",
      "17002\n",
      "17003\n",
      "17004\n",
      "17005\n",
      "17006\n",
      "17007\n",
      "17008\n",
      "17009\n",
      "17010\n",
      "17011\n",
      "17012\n",
      "17013\n",
      "17014\n",
      "17015\n",
      "17016\n",
      "17017\n",
      "17018\n",
      "17019\n",
      "17020\n",
      "17021\n",
      "17022\n",
      "17023\n",
      "17024\n",
      "17025\n",
      "17026\n",
      "17027\n",
      "17028\n",
      "17029\n",
      "17030\n",
      "17031\n",
      "17032\n",
      "17033\n",
      "17034\n",
      "17035\n",
      "17036\n",
      "17037\n",
      "17038\n",
      "17039\n",
      "17040\n",
      "17041\n",
      "17042\n",
      "17043\n",
      "17044\n",
      "17045\n",
      "17046\n",
      "17047\n",
      "17048\n",
      "17049\n",
      "17050\n",
      "17051\n",
      "17052\n",
      "17053\n",
      "17054\n",
      "17055\n",
      "17056\n",
      "17057\n",
      "17058\n",
      "17059\n",
      "17060\n",
      "17061\n",
      "17062\n",
      "17063\n",
      "17064\n",
      "17065\n",
      "17066\n",
      "17067\n",
      "17068\n",
      "17069\n",
      "17070\n",
      "17071\n",
      "17072\n",
      "17073\n",
      "17074\n",
      "17075\n",
      "17076\n",
      "17077\n",
      "17078\n",
      "17079\n",
      "17080\n",
      "17081\n",
      "17082\n",
      "17083\n",
      "17084\n",
      "17085\n",
      "17086\n",
      "17087\n",
      "17088\n",
      "17089\n",
      "17090\n",
      "17091\n",
      "17092\n",
      "17093\n",
      "17094\n",
      "17095\n",
      "17096\n",
      "17097\n",
      "17098\n",
      "17099\n",
      "17100\n",
      "17101\n",
      "17102\n",
      "17103\n",
      "17104\n",
      "17105\n",
      "17106\n",
      "17107\n",
      "17108\n",
      "17109\n",
      "17110\n",
      "17111\n",
      "17112\n",
      "17113\n",
      "17114\n",
      "17115\n",
      "17116\n",
      "17117\n",
      "17118\n",
      "17119\n",
      "17120\n",
      "17121\n",
      "17122\n",
      "17123\n",
      "17124\n",
      "17125\n",
      "17126\n",
      "17127\n",
      "17128\n",
      "17129\n",
      "17130\n",
      "17131\n",
      "17132\n",
      "17133\n",
      "17134\n",
      "17135\n",
      "17136\n",
      "17137\n",
      "17138\n",
      "17139\n",
      "17140\n",
      "17141\n",
      "17142\n",
      "17143\n",
      "17144\n",
      "17145\n",
      "17146\n",
      "17147\n",
      "17148\n",
      "17149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17150\n",
      "17151\n",
      "17152\n",
      "17153\n",
      "17154\n",
      "17155\n",
      "17156\n",
      "17157\n",
      "17158\n",
      "17159\n",
      "17160\n",
      "17161\n",
      "17162\n",
      "17163\n",
      "17164\n",
      "17165\n",
      "17166\n",
      "17167\n",
      "17168\n",
      "17169\n",
      "17170\n",
      "17171\n",
      "17172\n",
      "17173\n",
      "17174\n",
      "17175\n",
      "17176\n",
      "17177\n",
      "17178\n",
      "17179\n",
      "17180\n",
      "17181\n",
      "17182\n",
      "17183\n",
      "17184\n",
      "17185\n",
      "17186\n",
      "17187\n",
      "17188\n",
      "17189\n",
      "17190\n",
      "17191\n",
      "17192\n",
      "17193\n",
      "17194\n",
      "17195\n",
      "17196\n",
      "17197\n",
      "17198\n",
      "17199\n",
      "17200\n",
      "17201\n",
      "17202\n",
      "17203\n",
      "17204\n",
      "17205\n",
      "17206\n",
      "17207\n",
      "17208\n",
      "17209\n",
      "17210\n",
      "17211\n",
      "17212\n",
      "17213\n",
      "17214\n",
      "17215\n",
      "17216\n",
      "17217\n",
      "17218\n",
      "17219\n",
      "17220\n",
      "17221\n",
      "17222\n",
      "17223\n",
      "17224\n",
      "17225\n",
      "17226\n",
      "17227\n",
      "17228\n",
      "17229\n",
      "17230\n",
      "17231\n",
      "17232\n",
      "17233\n",
      "17234\n",
      "17235\n",
      "17236\n",
      "17237\n",
      "17238\n",
      "17239\n",
      "17240\n",
      "17241\n",
      "17242\n",
      "17243\n",
      "17244\n",
      "17245\n",
      "17246\n",
      "17247\n",
      "17248\n",
      "17249\n",
      "17250\n",
      "17251\n",
      "17252\n",
      "17253\n",
      "17254\n",
      "17255\n",
      "17256\n",
      "17257\n",
      "17258\n",
      "17259\n",
      "17260\n",
      "17261\n",
      "17262\n",
      "17263\n",
      "17264\n",
      "17265\n",
      "17266\n",
      "17267\n",
      "17268\n",
      "17269\n",
      "17270\n",
      "17271\n",
      "17272\n",
      "17273\n",
      "17274\n",
      "17275\n",
      "17276\n",
      "17277\n",
      "17278\n",
      "17279\n",
      "17280\n",
      "17281\n",
      "17282\n",
      "17283\n",
      "17284\n",
      "17285\n",
      "17286\n",
      "17287\n",
      "17288\n",
      "17289\n",
      "17290\n",
      "17291\n",
      "17292\n",
      "17293\n",
      "17294\n",
      "17295\n",
      "17296\n",
      "17297\n",
      "17298\n",
      "17299\n",
      "17300\n",
      "17301\n",
      "17302\n",
      "17303\n",
      "17304\n",
      "17305\n",
      "17306\n",
      "17307\n",
      "17308\n",
      "17309\n",
      "17310\n",
      "17311\n",
      "17312\n",
      "17313\n",
      "17314\n",
      "17315\n",
      "17316\n",
      "17317\n",
      "17318\n",
      "17319\n",
      "17320\n",
      "17321\n",
      "17322\n",
      "17323\n",
      "17324\n",
      "17325\n",
      "17326\n",
      "17327\n",
      "17328\n",
      "17329\n",
      "17330\n",
      "17331\n",
      "17332\n",
      "17333\n",
      "17334\n",
      "17335\n",
      "17336\n",
      "17337\n",
      "17338\n",
      "17339\n",
      "17340\n",
      "17341\n",
      "17342\n",
      "17343\n",
      "17344\n",
      "17345\n",
      "17346\n",
      "17347\n",
      "17348\n",
      "17349\n",
      "17350\n",
      "17351\n",
      "17352\n",
      "17353\n",
      "17354\n",
      "17355\n",
      "17356\n",
      "17357\n",
      "17358\n",
      "17359\n",
      "17360\n",
      "17361\n",
      "17362\n",
      "17363\n",
      "17364\n",
      "17365\n",
      "17366\n",
      "17367\n",
      "17368\n",
      "17369\n",
      "17370\n",
      "17371\n",
      "17372\n",
      "17373\n",
      "17374\n",
      "17375\n",
      "17376\n",
      "17377\n",
      "17378\n",
      "17379\n",
      "17380\n",
      "17381\n",
      "17382\n",
      "17383\n",
      "17384\n",
      "17385\n",
      "17386\n",
      "17387\n",
      "17388\n",
      "17389\n",
      "17390\n",
      "17391\n",
      "17392\n",
      "17393\n",
      "17394\n",
      "17395\n",
      "17396\n",
      "17397\n",
      "17398\n",
      "17399\n",
      "17400\n",
      "17401\n",
      "17402\n",
      "17403\n",
      "17404\n",
      "17405\n",
      "17406\n",
      "17407\n",
      "17408\n",
      "17409\n",
      "17410\n",
      "17411\n",
      "17412\n",
      "17413\n",
      "17414\n",
      "17415\n",
      "17416\n",
      "17417\n",
      "17418\n",
      "17419\n",
      "17420\n",
      "17421\n",
      "17422\n",
      "17423\n",
      "17424\n",
      "17425\n",
      "17426\n",
      "17427\n",
      "17428\n",
      "17429\n",
      "17430\n",
      "17431\n",
      "17432\n",
      "17433\n",
      "17434\n",
      "17435\n",
      "17436\n",
      "17437\n",
      "17438\n",
      "17439\n",
      "17440\n",
      "17441\n",
      "17442\n",
      "17443\n",
      "17444\n",
      "17445\n",
      "17446\n",
      "17447\n",
      "17448\n",
      "17449\n",
      "17450\n",
      "17451\n",
      "17452\n",
      "17453\n",
      "17454\n",
      "17455\n",
      "17456\n",
      "17457\n",
      "17458\n",
      "17459\n",
      "17460\n",
      "17461\n",
      "17462\n",
      "17463\n",
      "17464\n",
      "17465\n",
      "17466\n",
      "17467\n",
      "17468\n",
      "17469\n",
      "17470\n",
      "17471\n",
      "17472\n",
      "17473\n",
      "17474\n",
      "17475\n",
      "17476\n",
      "17477\n",
      "17478\n",
      "17479\n",
      "17480\n",
      "17481\n",
      "17482\n",
      "17483\n",
      "17484\n",
      "17485\n",
      "17486\n",
      "17487\n",
      "17488\n",
      "17489\n",
      "17490\n",
      "17491\n",
      "17492\n",
      "17493\n",
      "17494\n",
      "17495\n",
      "17496\n",
      "17497\n",
      "17498\n",
      "17499\n",
      "17500\n",
      "17501\n",
      "17502\n",
      "17503\n",
      "17504\n",
      "17505\n",
      "17506\n",
      "17507\n",
      "17508\n",
      "17509\n",
      "17510\n",
      "17511\n",
      "17512\n",
      "17513\n",
      "17514\n",
      "17515\n",
      "17516\n",
      "17517\n",
      "17518\n",
      "17519\n",
      "17520\n",
      "17521\n",
      "17522\n",
      "17523\n",
      "17524\n",
      "17525\n",
      "17526\n",
      "17527\n",
      "17528\n",
      "17529\n",
      "17530\n",
      "17531\n",
      "17532\n",
      "17533\n",
      "17534\n",
      "17535\n",
      "17536\n",
      "17537\n",
      "17538\n",
      "17539\n",
      "17540\n",
      "17541\n",
      "17542\n",
      "17543\n",
      "17544\n",
      "17545\n",
      "17546\n",
      "17547\n",
      "17548\n",
      "17549\n",
      "17550\n",
      "17551\n",
      "17552\n",
      "17553\n",
      "17554\n",
      "17555\n",
      "17556\n",
      "17557\n",
      "17558\n",
      "17559\n",
      "17560\n",
      "17561\n",
      "17562\n",
      "17563\n",
      "17564\n",
      "17565\n",
      "17566\n",
      "17567\n",
      "17568\n",
      "17569\n",
      "17570\n",
      "17571\n",
      "17572\n",
      "17573\n",
      "17574\n",
      "17575\n",
      "17576\n",
      "17577\n",
      "17578\n",
      "17579\n",
      "17580\n",
      "17581\n",
      "17582\n",
      "17583\n",
      "17584\n",
      "17585\n",
      "17586\n",
      "17587\n",
      "17588\n",
      "17589\n",
      "17590\n",
      "17591\n",
      "17592\n",
      "17593\n",
      "17594\n",
      "17595\n",
      "17596\n",
      "17597\n",
      "17598\n",
      "17599\n",
      "17600\n",
      "17601\n",
      "17602\n",
      "17603\n",
      "17604\n",
      "17605\n",
      "17606\n",
      "17607\n",
      "17608\n",
      "17609\n",
      "17610\n",
      "17611\n",
      "17612\n",
      "17613\n",
      "17614\n",
      "17615\n",
      "17616\n",
      "17617\n",
      "17618\n",
      "17619\n",
      "17620\n",
      "17621\n",
      "17622\n",
      "17623\n",
      "17624\n",
      "17625\n",
      "17626\n",
      "17627\n",
      "17628\n",
      "17629\n",
      "17630\n",
      "17631\n",
      "17632\n",
      "17633\n",
      "17634\n",
      "17635\n",
      "17636\n",
      "17637\n",
      "17638\n",
      "17639\n",
      "17640\n",
      "17641\n",
      "17642\n",
      "17643\n",
      "17644\n",
      "17645\n",
      "17646\n",
      "17647\n",
      "17648\n",
      "17649\n",
      "17650\n",
      "17651\n",
      "17652\n",
      "17653\n",
      "17654\n",
      "17655\n",
      "17656\n",
      "17657\n",
      "17658\n",
      "17659\n",
      "17660\n",
      "17661\n",
      "17662\n",
      "17663\n",
      "17664\n",
      "17665\n",
      "17666\n",
      "17667\n",
      "17668\n",
      "17669\n",
      "17670\n",
      "17671\n",
      "17672\n",
      "17673\n",
      "17674\n",
      "17675\n",
      "17676\n",
      "17677\n",
      "17678\n",
      "17679\n",
      "17680\n",
      "17681\n",
      "17682\n",
      "17683\n",
      "17684\n",
      "17685\n",
      "17686\n",
      "17687\n",
      "17688\n",
      "17689\n",
      "17690\n",
      "17691\n",
      "17692\n",
      "17693\n",
      "17694\n",
      "17695\n",
      "17696\n",
      "17697\n",
      "17698\n",
      "17699\n",
      "17700\n",
      "17701\n",
      "17702\n",
      "17703\n",
      "17704\n",
      "17705\n",
      "17706\n",
      "17707\n",
      "17708\n",
      "17709\n",
      "17710\n",
      "17711\n",
      "17712\n",
      "17713\n",
      "17714\n",
      "17715\n",
      "17716\n",
      "17717\n",
      "17718\n",
      "17719\n",
      "17720\n",
      "17721\n",
      "17722\n",
      "17723\n",
      "17724\n",
      "17725\n",
      "17726\n",
      "17727\n",
      "17728\n",
      "17729\n",
      "17730\n",
      "17731\n",
      "17732\n",
      "17733\n",
      "17734\n",
      "17735\n",
      "17736\n",
      "17737\n",
      "17738\n",
      "17739\n",
      "17740\n",
      "17741\n",
      "17742\n",
      "17743\n",
      "17744\n",
      "17745\n",
      "17746\n",
      "17747\n",
      "17748\n",
      "17749\n",
      "17750\n",
      "17751\n",
      "17752\n",
      "17753\n",
      "17754\n",
      "17755\n",
      "17756\n",
      "17757\n",
      "17758\n",
      "17759\n",
      "17760\n",
      "17761\n",
      "17762\n",
      "17763\n",
      "17764\n",
      "17765\n",
      "17766\n",
      "17767\n",
      "17768\n",
      "17769\n",
      "17770\n",
      "17771\n",
      "17772\n",
      "17773\n",
      "17774\n",
      "17775\n",
      "17776\n",
      "17777\n",
      "17778\n",
      "17779\n",
      "17780\n",
      "17781\n",
      "17782\n",
      "17783\n",
      "17784\n",
      "17785\n",
      "17786\n",
      "17787\n",
      "17788\n",
      "17789\n",
      "17790\n",
      "17791\n",
      "17792\n",
      "17793\n",
      "17794\n",
      "17795\n",
      "17796\n",
      "17797\n",
      "17798\n",
      "17799\n",
      "17800\n",
      "17801\n",
      "17802\n",
      "17803\n",
      "17804\n",
      "17805\n",
      "17806\n",
      "17807\n",
      "17808\n",
      "17809\n",
      "17810\n",
      "17811\n",
      "17812\n",
      "17813\n",
      "17814\n",
      "17815\n",
      "17816\n",
      "17817\n",
      "17818\n",
      "17819\n",
      "17820\n",
      "17821\n",
      "17822\n",
      "17823\n",
      "17824\n",
      "17825\n",
      "17826\n",
      "17827\n",
      "17828\n",
      "17829\n",
      "17830\n",
      "17831\n",
      "17832\n",
      "17833\n",
      "17834\n",
      "17835\n",
      "17836\n",
      "17837\n",
      "17838\n",
      "17839\n",
      "17840\n",
      "17841\n",
      "17842\n",
      "17843\n",
      "17844\n",
      "17845\n",
      "17846\n",
      "17847\n",
      "17848\n",
      "17849\n",
      "17850\n",
      "17851\n",
      "17852\n",
      "17853\n",
      "17854\n",
      "17855\n",
      "17856\n",
      "17857\n",
      "17858\n",
      "17859\n",
      "17860\n",
      "17861\n",
      "17862\n",
      "17863\n",
      "17864\n",
      "17865\n",
      "17866\n",
      "17867\n",
      "17868\n",
      "17869\n",
      "17870\n",
      "17871\n",
      "17872\n",
      "17873\n",
      "17874\n",
      "17875\n",
      "17876\n",
      "17877\n",
      "17878\n",
      "17879\n",
      "17880\n",
      "17881\n",
      "17882\n",
      "17883\n",
      "17884\n",
      "17885\n",
      "17886\n",
      "17887\n",
      "17888\n",
      "17889\n",
      "17890\n",
      "17891\n",
      "17892\n",
      "17893\n",
      "17894\n",
      "17895\n",
      "17896\n",
      "17897\n",
      "17898\n",
      "17899\n",
      "17900\n",
      "17901\n",
      "17902\n",
      "17903\n",
      "17904\n",
      "17905\n",
      "17906\n",
      "17907\n",
      "17908\n",
      "17909\n",
      "17910\n",
      "17911\n",
      "17912\n",
      "17913\n",
      "17914\n",
      "17915\n",
      "17916\n",
      "17917\n",
      "17918\n",
      "17919\n",
      "17920\n",
      "17921\n",
      "17922\n",
      "17923\n",
      "17924\n",
      "17925\n",
      "17926\n",
      "17927\n",
      "17928\n",
      "17929\n",
      "17930\n",
      "17931\n",
      "17932\n",
      "17933\n",
      "17934\n",
      "17935\n",
      "17936\n",
      "17937\n",
      "17938\n",
      "17939\n",
      "17940\n",
      "17941\n",
      "17942\n",
      "17943\n",
      "17944\n",
      "17945\n",
      "17946\n",
      "17947\n",
      "17948\n",
      "17949\n",
      "17950\n",
      "17951\n",
      "17952\n",
      "17953\n",
      "17954\n",
      "17955\n",
      "17956\n",
      "17957\n",
      "17958\n",
      "17959\n",
      "17960\n",
      "17961\n",
      "17962\n",
      "17963\n",
      "17964\n",
      "17965\n",
      "17966\n",
      "17967\n",
      "17968\n",
      "17969\n",
      "17970\n",
      "17971\n",
      "17972\n",
      "17973\n",
      "17974\n",
      "17975\n",
      "17976\n",
      "17977\n",
      "17978\n",
      "17979\n",
      "17980\n",
      "17981\n",
      "17982\n",
      "17983\n",
      "17984\n",
      "17985\n",
      "17986\n",
      "17987\n",
      "17988\n",
      "17989\n",
      "17990\n",
      "17991\n",
      "17992\n",
      "17993\n",
      "17994\n",
      "17995\n",
      "17996\n",
      "17997\n",
      "17998\n",
      "17999\n",
      "18000\n",
      "18001\n",
      "18002\n",
      "18003\n",
      "18004\n",
      "18005\n",
      "18006\n",
      "18007\n",
      "18008\n",
      "18009\n",
      "18010\n",
      "18011\n",
      "18012\n",
      "18013\n",
      "18014\n",
      "18015\n",
      "18016\n",
      "18017\n",
      "18018\n",
      "18019\n",
      "18020\n",
      "18021\n",
      "18022\n",
      "18023\n",
      "18024\n",
      "18025\n",
      "18026\n",
      "18027\n",
      "18028\n",
      "18029\n",
      "18030\n",
      "18031\n",
      "18032\n",
      "18033\n",
      "18034\n",
      "18035\n",
      "18036\n",
      "18037\n",
      "18038\n",
      "18039\n",
      "18040\n",
      "18041\n",
      "18042\n",
      "18043\n",
      "18044\n",
      "18045\n",
      "18046\n",
      "18047\n",
      "18048\n",
      "18049\n",
      "18050\n",
      "18051\n",
      "18052\n",
      "18053\n",
      "18054\n",
      "18055\n",
      "18056\n",
      "18057\n",
      "18058\n",
      "18059\n",
      "18060\n",
      "18061\n",
      "18062\n",
      "18063\n",
      "18064\n",
      "18065\n",
      "18066\n",
      "18067\n",
      "18068\n",
      "18069\n",
      "18070\n",
      "18071\n",
      "18072\n",
      "18073\n",
      "18074\n",
      "18075\n",
      "18076\n",
      "18077\n",
      "18078\n",
      "18079\n",
      "18080\n",
      "18081\n",
      "18082\n",
      "18083\n",
      "18084\n",
      "18085\n",
      "18086\n",
      "18087\n",
      "18088\n",
      "18089\n",
      "18090\n",
      "18091\n",
      "18092\n",
      "18093\n",
      "18094\n",
      "18095\n",
      "18096\n",
      "18097\n",
      "18098\n",
      "18099\n",
      "18100\n",
      "18101\n",
      "18102\n",
      "18103\n",
      "18104\n",
      "18105\n",
      "18106\n",
      "18107\n",
      "18108\n",
      "18109\n",
      "18110\n",
      "18111\n",
      "18112\n",
      "18113\n",
      "18114\n",
      "18115\n",
      "18116\n",
      "18117\n",
      "18118\n",
      "18119\n",
      "18120\n",
      "18121\n",
      "18122\n",
      "18123\n",
      "18124\n",
      "18125\n",
      "18126\n",
      "18127\n",
      "18128\n",
      "18129\n",
      "18130\n",
      "18131\n",
      "18132\n",
      "18133\n",
      "18134\n",
      "18135\n",
      "18136\n",
      "18137\n",
      "18138\n",
      "18139\n",
      "18140\n",
      "18141\n",
      "18142\n",
      "18143\n",
      "18144\n",
      "18145\n",
      "18146\n",
      "18147\n",
      "18148\n",
      "18149\n",
      "18150\n",
      "18151\n",
      "18152\n",
      "18153\n",
      "18154\n",
      "18155\n",
      "18156\n",
      "18157\n",
      "18158\n",
      "18159\n",
      "18160\n",
      "18161\n",
      "18162\n",
      "18163\n",
      "18164\n",
      "18165\n",
      "18166\n",
      "18167\n",
      "18168\n",
      "18169\n",
      "18170\n",
      "18171\n",
      "18172\n",
      "18173\n",
      "18174\n",
      "18175\n",
      "18176\n",
      "18177\n",
      "18178\n",
      "18179\n",
      "18180\n",
      "18181\n",
      "18182\n",
      "18183\n",
      "18184\n",
      "18185\n",
      "18186\n",
      "18187\n",
      "18188\n",
      "18189\n",
      "18190\n",
      "18191\n",
      "18192\n",
      "18193\n",
      "18194\n",
      "18195\n",
      "18196\n",
      "18197\n",
      "18198\n",
      "18199\n",
      "18200\n",
      "18201\n",
      "18202\n",
      "18203\n",
      "18204\n",
      "18205\n",
      "18206\n",
      "18207\n",
      "18208\n",
      "18209\n",
      "18210\n",
      "18211\n",
      "18212\n",
      "18213\n",
      "18214\n",
      "18215\n",
      "18216\n",
      "18217\n",
      "18218\n",
      "18219\n",
      "18220\n",
      "18221\n",
      "18222\n",
      "18223\n",
      "18224\n",
      "18225\n",
      "18226\n",
      "18227\n",
      "18228\n",
      "18229\n",
      "18230\n",
      "18231\n",
      "18232\n",
      "18233\n",
      "18234\n",
      "18235\n",
      "18236\n",
      "18237\n",
      "18238\n",
      "18239\n",
      "18240\n",
      "18241\n",
      "18242\n",
      "18243\n",
      "18244\n",
      "18245\n",
      "18246\n",
      "18247\n",
      "18248\n",
      "18249\n",
      "18250\n",
      "18251\n",
      "18252\n",
      "18253\n",
      "18254\n",
      "18255\n",
      "18256\n",
      "18257\n",
      "18258\n",
      "18259\n",
      "18260\n",
      "18261\n",
      "18262\n",
      "18263\n",
      "18264\n",
      "18265\n",
      "18266\n",
      "18267\n",
      "18268\n",
      "18269\n",
      "18270\n",
      "18271\n",
      "18272\n",
      "18273\n",
      "18274\n",
      "18275\n",
      "18276\n",
      "18277\n",
      "18278\n",
      "18279\n",
      "18280\n",
      "18281\n",
      "18282\n",
      "18283\n",
      "18284\n",
      "18285\n",
      "18286\n",
      "18287\n",
      "18288\n",
      "18289\n",
      "18290\n",
      "18291\n",
      "18292\n",
      "18293\n",
      "18294\n",
      "18295\n",
      "18296\n",
      "18297\n",
      "18298\n",
      "18299\n",
      "18300\n",
      "18301\n",
      "18302\n",
      "18303\n",
      "18304\n",
      "18305\n",
      "18306\n",
      "18307\n",
      "18308\n",
      "18309\n",
      "18310\n",
      "18311\n",
      "18312\n",
      "18313\n",
      "18314\n",
      "18315\n",
      "18316\n",
      "18317\n",
      "18318\n",
      "18319\n",
      "18320\n",
      "18321\n",
      "18322\n",
      "18323\n",
      "18324\n",
      "18325\n",
      "18326\n",
      "18327\n",
      "18328\n",
      "18329\n",
      "18330\n",
      "18331\n",
      "18332\n",
      "18333\n",
      "18334\n",
      "18335\n",
      "18336\n",
      "18337\n",
      "18338\n",
      "18339\n",
      "18340\n",
      "18341\n",
      "18342\n",
      "18343\n",
      "18344\n",
      "18345\n",
      "18346\n",
      "18347\n",
      "18348\n",
      "18349\n",
      "18350\n",
      "18351\n",
      "18352\n",
      "18353\n",
      "18354\n",
      "18355\n",
      "18356\n",
      "18357\n",
      "18358\n",
      "18359\n",
      "18360\n",
      "18361\n",
      "18362\n",
      "18363\n",
      "18364\n",
      "18365\n",
      "18366\n",
      "18367\n",
      "18368\n",
      "18369\n",
      "18370\n",
      "18371\n",
      "18372\n",
      "18373\n",
      "18374\n",
      "18375\n",
      "18376\n",
      "18377\n",
      "18378\n",
      "18379\n",
      "18380\n",
      "18381\n",
      "18382\n",
      "18383\n",
      "18384\n",
      "18385\n",
      "18386\n",
      "18387\n",
      "18388\n",
      "18389\n",
      "18390\n",
      "18391\n",
      "18392\n",
      "18393\n",
      "18394\n",
      "18395\n",
      "18396\n",
      "18397\n",
      "18398\n",
      "18399\n",
      "18400\n",
      "18401\n",
      "18402\n",
      "18403\n",
      "18404\n",
      "18405\n",
      "18406\n",
      "18407\n",
      "18408\n",
      "18409\n",
      "18410\n",
      "18411\n",
      "18412\n",
      "18413\n",
      "18414\n",
      "18415\n",
      "18416\n",
      "18417\n",
      "18418\n",
      "18419\n",
      "18420\n",
      "18421\n",
      "18422\n",
      "18423\n",
      "18424\n",
      "18425\n",
      "18426\n",
      "18427\n",
      "18428\n",
      "18429\n",
      "18430\n",
      "18431\n",
      "18432\n",
      "18433\n",
      "18434\n",
      "18435\n",
      "18436\n",
      "18437\n",
      "18438\n",
      "18439\n",
      "18440\n",
      "18441\n",
      "18442\n",
      "18443\n",
      "18444\n",
      "18445\n",
      "18446\n",
      "18447\n",
      "18448\n",
      "18449\n",
      "18450\n",
      "18451\n",
      "18452\n",
      "18453\n",
      "18454\n",
      "18455\n",
      "18456\n",
      "18457\n",
      "18458\n",
      "18459\n",
      "18460\n",
      "18461\n",
      "18462\n",
      "18463\n",
      "18464\n",
      "18465\n",
      "18466\n",
      "18467\n",
      "18468\n",
      "18469\n",
      "18470\n",
      "18471\n",
      "18472\n",
      "18473\n",
      "18474\n",
      "18475\n",
      "18476\n",
      "18477\n",
      "18478\n",
      "18479\n",
      "18480\n",
      "18481\n",
      "18482\n",
      "18483\n",
      "18484\n",
      "18485\n",
      "18486\n",
      "18487\n",
      "18488\n",
      "18489\n",
      "18490\n",
      "18491\n",
      "18492\n",
      "18493\n",
      "18494\n",
      "18495\n",
      "18496\n",
      "18497\n",
      "18498\n",
      "18499\n",
      "18500\n",
      "18501\n",
      "18502\n",
      "18503\n",
      "18504\n",
      "18505\n",
      "18506\n",
      "18507\n",
      "18508\n",
      "18509\n",
      "18510\n",
      "18511\n",
      "18512\n",
      "18513\n",
      "18514\n",
      "18515\n",
      "18516\n",
      "18517\n",
      "18518\n",
      "18519\n",
      "18520\n",
      "18521\n",
      "18522\n",
      "18523\n",
      "18524\n",
      "18525\n",
      "18526\n",
      "18527\n",
      "18528\n",
      "18529\n",
      "18530\n",
      "18531\n",
      "18532\n",
      "18533\n",
      "18534\n",
      "18535\n",
      "18536\n",
      "18537\n",
      "18538\n",
      "18539\n",
      "18540\n",
      "18541\n",
      "18542\n",
      "18543\n",
      "18544\n",
      "18545\n",
      "18546\n",
      "18547\n",
      "18548\n",
      "18549\n",
      "18550\n",
      "18551\n",
      "18552\n",
      "18553\n",
      "18554\n",
      "18555\n",
      "18556\n",
      "18557\n",
      "18558\n",
      "18559\n",
      "18560\n",
      "18561\n",
      "18562\n",
      "18563\n",
      "18564\n",
      "18565\n",
      "18566\n",
      "18567\n",
      "18568\n",
      "18569\n",
      "18570\n",
      "18571\n",
      "18572\n",
      "18573\n",
      "18574\n",
      "18575\n",
      "18576\n",
      "18577\n",
      "18578\n",
      "18579\n",
      "18580\n",
      "18581\n",
      "18582\n",
      "18583\n",
      "18584\n",
      "18585\n",
      "18586\n",
      "18587\n",
      "18588\n",
      "18589\n",
      "18590\n",
      "18591\n",
      "18592\n",
      "18593\n",
      "18594\n",
      "18595\n",
      "18596\n",
      "18597\n",
      "18598\n",
      "18599\n",
      "18600\n",
      "18601\n",
      "18602\n",
      "18603\n",
      "18604\n",
      "18605\n",
      "18606\n",
      "18607\n",
      "18608\n",
      "18609\n",
      "18610\n",
      "18611\n",
      "18612\n",
      "18613\n",
      "18614\n",
      "18615\n",
      "18616\n",
      "18617\n",
      "18618\n",
      "18619\n",
      "18620\n",
      "18621\n",
      "18622\n",
      "18623\n",
      "18624\n",
      "18625\n",
      "18626\n",
      "18627\n",
      "18628\n",
      "18629\n",
      "18630\n",
      "18631\n",
      "18632\n",
      "18633\n",
      "18634\n",
      "18635\n",
      "18636\n",
      "18637\n",
      "18638\n",
      "18639\n",
      "18640\n",
      "18641\n",
      "18642\n",
      "18643\n",
      "18644\n",
      "18645\n",
      "18646\n",
      "18647\n",
      "18648\n",
      "18649\n",
      "18650\n",
      "18651\n",
      "18652\n",
      "18653\n",
      "18654\n",
      "18655\n",
      "18656\n",
      "18657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18658\n",
      "18659\n",
      "18660\n",
      "18661\n",
      "18662\n",
      "18663\n",
      "18664\n",
      "18665\n",
      "18666\n",
      "18667\n",
      "18668\n",
      "18669\n",
      "18670\n",
      "18671\n",
      "18672\n",
      "18673\n",
      "18674\n",
      "18675\n",
      "18676\n",
      "18677\n",
      "18678\n",
      "18679\n",
      "18680\n",
      "18681\n",
      "18682\n",
      "18683\n",
      "18684\n",
      "18685\n",
      "18686\n",
      "18687\n",
      "18688\n",
      "18689\n",
      "18690\n",
      "18691\n",
      "18692\n",
      "18693\n",
      "18694\n",
      "18695\n",
      "18696\n",
      "18697\n",
      "18698\n",
      "18699\n",
      "18700\n",
      "18701\n",
      "18702\n",
      "18703\n",
      "18704\n",
      "18705\n",
      "18706\n",
      "18707\n",
      "18708\n",
      "18709\n",
      "18710\n",
      "18711\n",
      "18712\n",
      "18713\n",
      "18714\n",
      "18715\n",
      "18716\n",
      "18717\n",
      "18718\n",
      "18719\n",
      "18720\n",
      "18721\n",
      "18722\n",
      "18723\n",
      "18724\n",
      "18725\n",
      "18726\n",
      "18727\n",
      "18728\n",
      "18729\n",
      "18730\n",
      "18731\n",
      "18732\n",
      "18733\n",
      "18734\n",
      "18735\n",
      "18736\n",
      "18737\n",
      "18738\n",
      "18739\n",
      "18740\n",
      "18741\n",
      "18742\n",
      "18743\n",
      "18744\n",
      "18745\n",
      "18746\n",
      "18747\n",
      "18748\n",
      "18749\n",
      "18750\n",
      "18751\n",
      "18752\n",
      "18753\n",
      "18754\n",
      "18755\n",
      "18756\n",
      "18757\n",
      "18758\n",
      "18759\n",
      "18760\n",
      "18761\n",
      "18762\n",
      "18763\n",
      "18764\n",
      "18765\n",
      "18766\n",
      "18767\n",
      "18768\n",
      "18769\n",
      "18770\n",
      "18771\n",
      "18772\n",
      "18773\n",
      "18774\n",
      "18775\n",
      "18776\n",
      "18777\n",
      "18778\n",
      "18779\n",
      "18780\n",
      "18781\n",
      "18782\n",
      "18783\n",
      "18784\n",
      "18785\n",
      "18786\n",
      "18787\n",
      "18788\n",
      "18789\n",
      "18790\n",
      "18791\n",
      "18792\n",
      "18793\n",
      "18794\n",
      "18795\n",
      "18796\n",
      "18797\n",
      "18798\n",
      "18799\n",
      "18800\n",
      "18801\n",
      "18802\n",
      "18803\n",
      "18804\n",
      "18805\n",
      "18806\n",
      "18807\n",
      "18808\n",
      "18809\n",
      "18810\n",
      "18811\n",
      "18812\n",
      "18813\n",
      "18814\n",
      "18815\n",
      "18816\n",
      "18817\n",
      "18818\n",
      "18819\n",
      "18820\n",
      "18821\n",
      "18822\n",
      "18823\n",
      "18824\n",
      "18825\n",
      "18826\n",
      "18827\n",
      "18828\n",
      "18829\n",
      "18830\n",
      "18831\n",
      "18832\n",
      "18833\n",
      "18834\n",
      "18835\n",
      "18836\n",
      "18837\n",
      "18838\n",
      "18839\n",
      "18840\n",
      "18841\n",
      "18842\n",
      "18843\n",
      "18844\n",
      "18845\n",
      "18846\n",
      "18847\n",
      "18848\n",
      "18849\n",
      "18850\n",
      "18851\n",
      "18852\n",
      "18853\n",
      "18854\n",
      "18855\n",
      "18856\n",
      "18857\n",
      "18858\n",
      "18859\n",
      "18860\n",
      "18861\n",
      "18862\n",
      "18863\n",
      "18864\n",
      "18865\n",
      "18866\n",
      "18867\n",
      "18868\n",
      "18869\n",
      "18870\n",
      "18871\n",
      "18872\n",
      "18873\n",
      "18874\n",
      "18875\n",
      "18876\n",
      "18877\n",
      "18878\n",
      "18879\n",
      "18880\n",
      "18881\n",
      "18882\n",
      "18883\n",
      "18884\n",
      "18885\n",
      "18886\n",
      "18887\n",
      "18888\n",
      "18889\n",
      "18890\n",
      "18891\n",
      "18892\n",
      "18893\n",
      "18894\n",
      "18895\n",
      "18896\n",
      "18897\n",
      "18898\n",
      "18899\n",
      "18900\n",
      "18901\n",
      "18902\n",
      "18903\n",
      "18904\n",
      "18905\n",
      "18906\n",
      "18907\n",
      "18908\n",
      "18909\n",
      "18910\n",
      "18911\n",
      "18912\n",
      "18913\n",
      "18914\n",
      "18915\n",
      "18916\n",
      "18917\n",
      "18918\n",
      "18919\n",
      "18920\n",
      "18921\n",
      "18922\n",
      "18923\n",
      "18924\n",
      "18925\n",
      "18926\n",
      "18927\n",
      "18928\n",
      "18929\n",
      "18930\n",
      "18931\n",
      "18932\n",
      "18933\n",
      "18934\n",
      "18935\n",
      "18936\n",
      "18937\n",
      "18938\n",
      "18939\n",
      "18940\n",
      "18941\n",
      "18942\n",
      "18943\n",
      "18944\n",
      "18945\n",
      "18946\n",
      "18947\n",
      "18948\n",
      "18949\n",
      "18950\n",
      "18951\n",
      "18952\n",
      "18953\n",
      "18954\n",
      "18955\n",
      "18956\n",
      "18957\n",
      "18958\n",
      "18959\n",
      "18960\n",
      "18961\n",
      "18962\n",
      "18963\n",
      "18964\n",
      "18965\n",
      "18966\n",
      "18967\n",
      "18968\n",
      "18969\n",
      "18970\n",
      "18971\n",
      "18972\n",
      "18973\n",
      "18974\n",
      "18975\n",
      "18976\n",
      "18977\n",
      "18978\n",
      "18979\n",
      "18980\n",
      "18981\n",
      "18982\n",
      "18983\n",
      "18984\n",
      "18985\n",
      "18986\n",
      "18987\n",
      "18988\n",
      "18989\n",
      "18990\n",
      "18991\n",
      "18992\n",
      "18993\n",
      "18994\n",
      "18995\n",
      "18996\n",
      "18997\n",
      "18998\n",
      "18999\n",
      "19000\n",
      "19001\n",
      "19002\n",
      "19003\n",
      "19004\n",
      "19005\n",
      "19006\n",
      "19007\n",
      "19008\n",
      "19009\n",
      "19010\n",
      "19011\n",
      "19012\n",
      "19013\n",
      "19014\n",
      "19015\n",
      "19016\n",
      "19017\n",
      "19018\n",
      "19019\n",
      "19020\n",
      "19021\n",
      "19022\n",
      "19023\n",
      "19024\n",
      "19025\n",
      "19026\n",
      "19027\n",
      "19028\n",
      "19029\n",
      "19030\n",
      "19031\n",
      "19032\n",
      "19033\n",
      "19034\n",
      "19035\n",
      "19036\n",
      "19037\n",
      "19038\n",
      "19039\n",
      "19040\n",
      "19041\n",
      "19042\n",
      "19043\n",
      "19044\n",
      "19045\n",
      "19046\n",
      "19047\n",
      "19048\n",
      "19049\n",
      "19050\n",
      "19051\n",
      "19052\n",
      "19053\n",
      "19054\n",
      "19055\n",
      "19056\n",
      "19057\n",
      "19058\n",
      "19059\n",
      "19060\n",
      "19061\n",
      "19062\n",
      "19063\n",
      "19064\n",
      "19065\n",
      "19066\n",
      "19067\n",
      "19068\n",
      "19069\n",
      "19070\n",
      "19071\n",
      "19072\n",
      "19073\n",
      "19074\n",
      "19075\n",
      "19076\n",
      "19077\n",
      "19078\n",
      "19079\n",
      "19080\n",
      "19081\n",
      "19082\n",
      "19083\n",
      "19084\n",
      "19085\n",
      "19086\n",
      "19087\n",
      "19088\n",
      "19089\n",
      "19090\n",
      "19091\n",
      "19092\n",
      "19093\n",
      "19094\n",
      "19095\n",
      "19096\n",
      "19097\n",
      "19098\n",
      "19099\n",
      "19100\n",
      "19101\n",
      "19102\n",
      "19103\n",
      "19104\n",
      "19105\n",
      "19106\n",
      "19107\n",
      "19108\n",
      "19109\n",
      "19110\n",
      "19111\n",
      "19112\n",
      "19113\n",
      "19114\n",
      "19115\n",
      "19116\n",
      "19117\n",
      "19118\n",
      "19119\n",
      "19120\n",
      "19121\n",
      "19122\n",
      "19123\n",
      "19124\n",
      "19125\n",
      "19126\n",
      "19127\n",
      "19128\n",
      "19129\n",
      "19130\n",
      "19131\n",
      "19132\n",
      "19133\n",
      "19134\n",
      "19135\n",
      "19136\n",
      "19137\n",
      "19138\n",
      "19139\n",
      "19140\n",
      "19141\n",
      "19142\n",
      "19143\n",
      "19144\n",
      "19145\n",
      "19146\n",
      "19147\n",
      "19148\n",
      "19149\n",
      "19150\n",
      "19151\n",
      "19152\n",
      "19153\n",
      "19154\n",
      "19155\n",
      "19156\n",
      "19157\n",
      "19158\n",
      "19159\n",
      "19160\n",
      "19161\n",
      "19162\n",
      "19163\n",
      "19164\n",
      "19165\n",
      "19166\n",
      "19167\n",
      "19168\n",
      "19169\n",
      "19170\n",
      "19171\n",
      "19172\n",
      "19173\n",
      "19174\n",
      "19175\n",
      "19176\n",
      "19177\n",
      "19178\n",
      "19179\n",
      "19180\n",
      "19181\n",
      "19182\n",
      "19183\n",
      "19184\n",
      "19185\n",
      "19186\n",
      "19187\n",
      "19188\n",
      "19189\n",
      "19190\n",
      "19191\n",
      "19192\n",
      "19193\n",
      "19194\n",
      "19195\n",
      "19196\n",
      "19197\n",
      "19198\n",
      "19199\n",
      "19200\n",
      "19201\n",
      "19202\n",
      "19203\n",
      "19204\n",
      "19205\n",
      "19206\n",
      "19207\n",
      "19208\n",
      "19209\n",
      "19210\n",
      "19211\n",
      "19212\n",
      "19213\n",
      "19214\n",
      "19215\n",
      "19216\n",
      "19217\n",
      "19218\n",
      "19219\n",
      "19220\n",
      "19221\n",
      "19222\n",
      "19223\n",
      "19224\n",
      "19225\n",
      "19226\n",
      "19227\n",
      "19228\n",
      "19229\n",
      "19230\n",
      "19231\n",
      "19232\n",
      "19233\n",
      "19234\n",
      "19235\n",
      "19236\n",
      "19237\n",
      "19238\n",
      "19239\n",
      "19240\n",
      "19241\n",
      "19242\n",
      "19243\n",
      "19244\n",
      "19245\n",
      "19246\n",
      "19247\n",
      "19248\n",
      "19249\n",
      "19250\n",
      "19251\n",
      "19252\n",
      "19253\n",
      "19254\n",
      "19255\n",
      "19256\n",
      "19257\n",
      "19258\n",
      "19259\n",
      "19260\n",
      "19261\n",
      "19262\n",
      "19263\n",
      "19264\n",
      "19265\n",
      "19266\n",
      "19267\n",
      "19268\n",
      "19269\n",
      "19270\n",
      "19271\n",
      "19272\n",
      "19273\n",
      "19274\n",
      "19275\n",
      "19276\n",
      "19277\n",
      "19278\n",
      "19279\n",
      "19280\n",
      "19281\n",
      "19282\n",
      "19283\n",
      "19284\n",
      "19285\n",
      "19286\n",
      "19287\n",
      "19288\n",
      "19289\n",
      "19290\n",
      "19291\n",
      "19292\n",
      "19293\n",
      "19294\n",
      "19295\n",
      "19296\n",
      "19297\n",
      "19298\n",
      "19299\n",
      "19300\n",
      "19301\n",
      "19302\n",
      "19303\n",
      "19304\n",
      "19305\n",
      "19306\n",
      "19307\n",
      "19308\n",
      "19309\n",
      "19310\n",
      "19311\n",
      "19312\n",
      "19313\n",
      "19314\n",
      "19315\n",
      "19316\n",
      "19317\n",
      "19318\n",
      "19319\n",
      "19320\n",
      "19321\n",
      "19322\n",
      "19323\n",
      "19324\n",
      "19325\n",
      "19326\n",
      "19327\n",
      "19328\n",
      "19329\n",
      "19330\n",
      "19331\n",
      "19332\n",
      "19333\n",
      "19334\n",
      "19335\n",
      "19336\n",
      "19337\n",
      "19338\n",
      "19339\n",
      "19340\n",
      "19341\n",
      "19342\n",
      "19343\n",
      "19344\n",
      "19345\n",
      "19346\n",
      "19347\n",
      "19348\n",
      "19349\n",
      "19350\n",
      "19351\n",
      "19352\n",
      "19353\n",
      "19354\n",
      "19355\n",
      "19356\n",
      "19357\n",
      "19358\n",
      "19359\n",
      "19360\n",
      "19361\n",
      "19362\n",
      "19363\n",
      "19364\n",
      "19365\n",
      "19366\n",
      "19367\n",
      "19368\n",
      "19369\n",
      "19370\n",
      "19371\n",
      "19372\n",
      "19373\n",
      "19374\n",
      "19375\n",
      "19376\n",
      "19377\n",
      "19378\n",
      "19379\n",
      "19380\n",
      "19381\n",
      "19382\n",
      "19383\n",
      "19384\n",
      "19385\n",
      "19386\n",
      "19387\n",
      "19388\n",
      "19389\n",
      "19390\n",
      "19391\n",
      "19392\n",
      "19393\n",
      "19394\n",
      "19395\n",
      "19396\n",
      "19397\n",
      "19398\n",
      "19399\n",
      "19400\n",
      "19401\n",
      "19402\n",
      "19403\n",
      "19404\n",
      "19405\n",
      "19406\n",
      "19407\n",
      "19408\n",
      "19409\n",
      "19410\n",
      "19411\n",
      "19412\n",
      "19413\n",
      "19414\n",
      "19415\n",
      "19416\n",
      "19417\n",
      "19418\n",
      "19419\n",
      "19420\n",
      "19421\n",
      "19422\n",
      "19423\n",
      "19424\n",
      "19425\n",
      "19426\n",
      "19427\n",
      "19428\n",
      "19429\n",
      "19430\n",
      "19431\n",
      "19432\n",
      "19433\n",
      "19434\n",
      "19435\n",
      "19436\n",
      "19437\n",
      "19438\n",
      "19439\n",
      "19440\n",
      "19441\n",
      "19442\n",
      "19443\n",
      "19444\n",
      "19445\n",
      "19446\n",
      "19447\n",
      "19448\n",
      "19449\n",
      "19450\n",
      "19451\n",
      "19452\n",
      "19453\n",
      "19454\n",
      "19455\n",
      "19456\n",
      "19457\n",
      "19458\n",
      "19459\n",
      "19460\n",
      "19461\n",
      "19462\n",
      "19463\n",
      "19464\n",
      "19465\n",
      "19466\n",
      "19467\n",
      "19468\n",
      "19469\n",
      "19470\n",
      "19471\n",
      "19472\n",
      "19473\n",
      "19474\n",
      "19475\n",
      "19476\n",
      "19477\n",
      "19478\n",
      "19479\n",
      "19480\n",
      "19481\n",
      "19482\n",
      "19483\n",
      "19484\n",
      "19485\n",
      "19486\n",
      "19487\n",
      "19488\n",
      "19489\n",
      "19490\n",
      "19491\n",
      "19492\n",
      "19493\n",
      "19494\n",
      "19495\n",
      "19496\n",
      "19497\n",
      "19498\n",
      "19499\n",
      "19500\n",
      "19501\n",
      "19502\n",
      "19503\n",
      "19504\n",
      "19505\n",
      "19506\n",
      "19507\n",
      "19508\n",
      "19509\n",
      "19510\n",
      "19511\n",
      "19512\n",
      "19513\n",
      "19514\n",
      "19515\n",
      "19516\n",
      "19517\n",
      "19518\n",
      "19519\n",
      "19520\n",
      "19521\n",
      "19522\n",
      "19523\n",
      "19524\n",
      "19525\n",
      "19526\n",
      "19527\n",
      "19528\n",
      "19529\n",
      "19530\n",
      "19531\n",
      "19532\n",
      "19533\n",
      "19534\n",
      "19535\n",
      "19536\n",
      "19537\n",
      "19538\n",
      "19539\n",
      "19540\n",
      "19541\n",
      "19542\n",
      "19543\n",
      "19544\n",
      "19545\n",
      "19546\n",
      "19547\n",
      "19548\n",
      "19549\n",
      "19550\n",
      "19551\n",
      "19552\n",
      "19553\n",
      "19554\n",
      "19555\n",
      "19556\n",
      "19557\n",
      "19558\n",
      "19559\n",
      "19560\n",
      "19561\n",
      "19562\n",
      "19563\n",
      "19564\n",
      "19565\n",
      "19566\n",
      "19567\n",
      "19568\n",
      "19569\n",
      "19570\n",
      "19571\n",
      "19572\n",
      "19573\n",
      "19574\n",
      "19575\n",
      "19576\n",
      "19577\n",
      "19578\n",
      "19579\n",
      "19580\n",
      "19581\n",
      "19582\n",
      "19583\n",
      "19584\n",
      "19585\n",
      "19586\n",
      "19587\n",
      "19588\n",
      "19589\n",
      "19590\n",
      "19591\n",
      "19592\n",
      "19593\n",
      "19594\n",
      "19595\n",
      "19596\n",
      "19597\n",
      "19598\n",
      "19599\n",
      "19600\n",
      "19601\n",
      "19602\n",
      "19603\n",
      "19604\n",
      "19605\n",
      "19606\n",
      "19607\n",
      "19608\n",
      "19609\n",
      "19610\n",
      "19611\n",
      "19612\n",
      "19613\n",
      "19614\n",
      "19615\n",
      "19616\n",
      "19617\n",
      "19618\n",
      "19619\n",
      "19620\n",
      "19621\n",
      "19622\n",
      "19623\n",
      "19624\n",
      "19625\n",
      "19626\n",
      "19627\n",
      "19628\n",
      "19629\n",
      "19630\n",
      "19631\n",
      "19632\n",
      "19633\n",
      "19634\n",
      "19635\n",
      "19636\n",
      "19637\n",
      "19638\n",
      "19639\n",
      "19640\n",
      "19641\n",
      "19642\n",
      "19643\n",
      "19644\n",
      "19645\n",
      "19646\n",
      "19647\n",
      "19648\n",
      "19649\n",
      "19650\n",
      "19651\n",
      "19652\n",
      "19653\n",
      "19654\n",
      "19655\n",
      "19656\n",
      "19657\n",
      "19658\n",
      "19659\n",
      "19660\n",
      "19661\n",
      "19662\n",
      "19663\n",
      "19664\n",
      "19665\n",
      "19666\n",
      "19667\n",
      "19668\n",
      "19669\n",
      "19670\n",
      "19671\n",
      "19672\n",
      "19673\n",
      "19674\n",
      "19675\n",
      "19676\n",
      "19677\n",
      "19678\n",
      "19679\n",
      "19680\n",
      "19681\n",
      "19682\n",
      "19683\n",
      "19684\n",
      "19685\n",
      "19686\n",
      "19687\n",
      "19688\n",
      "19689\n",
      "19690\n",
      "19691\n",
      "19692\n",
      "19693\n",
      "19694\n",
      "19695\n",
      "19696\n",
      "19697\n",
      "19698\n",
      "19699\n",
      "19700\n",
      "19701\n",
      "19702\n",
      "19703\n",
      "19704\n",
      "19705\n",
      "19706\n",
      "19707\n",
      "19708\n",
      "19709\n",
      "19710\n",
      "19711\n",
      "19712\n",
      "19713\n",
      "19714\n",
      "19715\n",
      "19716\n",
      "19717\n",
      "19718\n",
      "19719\n",
      "19720\n",
      "19721\n",
      "19722\n",
      "19723\n",
      "19724\n",
      "19725\n",
      "19726\n",
      "19727\n",
      "19728\n",
      "19729\n",
      "19730\n",
      "19731\n",
      "19732\n",
      "19733\n",
      "19734\n",
      "19735\n",
      "19736\n",
      "19737\n",
      "19738\n",
      "19739\n",
      "19740\n",
      "19741\n",
      "19742\n",
      "19743\n",
      "19744\n",
      "19745\n",
      "19746\n",
      "19747\n",
      "19748\n",
      "19749\n",
      "19750\n",
      "19751\n",
      "19752\n",
      "19753\n",
      "19754\n",
      "19755\n",
      "19756\n",
      "19757\n",
      "19758\n",
      "19759\n",
      "19760\n",
      "19761\n",
      "19762\n",
      "19763\n",
      "19764\n",
      "19765\n",
      "19766\n",
      "19767\n",
      "19768\n",
      "19769\n",
      "19770\n",
      "19771\n",
      "19772\n",
      "19773\n",
      "19774\n",
      "19775\n",
      "19776\n",
      "19777\n",
      "19778\n",
      "19779\n",
      "19780\n",
      "19781\n",
      "19782\n",
      "19783\n",
      "19784\n",
      "19785\n",
      "19786\n",
      "19787\n",
      "19788\n",
      "19789\n",
      "19790\n",
      "19791\n",
      "19792\n",
      "19793\n",
      "19794\n",
      "19795\n",
      "19796\n",
      "19797\n",
      "19798\n",
      "19799\n",
      "19800\n",
      "19801\n",
      "19802\n",
      "19803\n",
      "19804\n",
      "19805\n",
      "19806\n",
      "19807\n",
      "19808\n",
      "19809\n",
      "19810\n",
      "19811\n",
      "19812\n",
      "19813\n",
      "19814\n",
      "19815\n",
      "19816\n",
      "19817\n",
      "19818\n",
      "19819\n",
      "19820\n",
      "19821\n",
      "19822\n",
      "19823\n",
      "19824\n",
      "19825\n",
      "19826\n",
      "19827\n",
      "19828\n",
      "19829\n",
      "19830\n",
      "19831\n",
      "19832\n",
      "19833\n",
      "19834\n",
      "19835\n",
      "19836\n",
      "19837\n",
      "19838\n",
      "19839\n",
      "19840\n",
      "19841\n",
      "19842\n",
      "19843\n",
      "19844\n",
      "19845\n",
      "19846\n",
      "19847\n",
      "19848\n",
      "19849\n",
      "19850\n",
      "19851\n",
      "19852\n",
      "19853\n",
      "19854\n",
      "19855\n",
      "19856\n",
      "19857\n",
      "19858\n",
      "19859\n",
      "19860\n",
      "19861\n",
      "19862\n",
      "19863\n",
      "19864\n",
      "19865\n",
      "19866\n",
      "19867\n",
      "19868\n",
      "19869\n",
      "19870\n",
      "19871\n",
      "19872\n",
      "19873\n",
      "19874\n",
      "19875\n",
      "19876\n",
      "19877\n",
      "19878\n",
      "19879\n",
      "19880\n",
      "19881\n",
      "19882\n",
      "19883\n",
      "19884\n",
      "19885\n",
      "19886\n",
      "19887\n",
      "19888\n",
      "19889\n",
      "19890\n",
      "19891\n",
      "19892\n",
      "19893\n",
      "19894\n",
      "19895\n",
      "19896\n",
      "19897\n",
      "19898\n",
      "19899\n",
      "19900\n",
      "19901\n",
      "19902\n",
      "19903\n",
      "19904\n",
      "19905\n",
      "19906\n",
      "19907\n",
      "19908\n",
      "19909\n",
      "19910\n",
      "19911\n",
      "19912\n",
      "19913\n",
      "19914\n",
      "19915\n",
      "19916\n",
      "19917\n",
      "19918\n",
      "19919\n",
      "19920\n",
      "19921\n",
      "19922\n",
      "19923\n",
      "19924\n",
      "19925\n",
      "19926\n",
      "19927\n",
      "19928\n",
      "19929\n",
      "19930\n",
      "19931\n",
      "19932\n",
      "19933\n",
      "19934\n",
      "19935\n",
      "19936\n",
      "19937\n",
      "19938\n",
      "19939\n",
      "19940\n",
      "19941\n",
      "19942\n",
      "19943\n",
      "19944\n",
      "19945\n",
      "19946\n",
      "19947\n",
      "19948\n",
      "19949\n",
      "19950\n",
      "19951\n",
      "19952\n",
      "19953\n",
      "19954\n",
      "19955\n",
      "19956\n",
      "19957\n",
      "19958\n",
      "19959\n",
      "19960\n",
      "19961\n",
      "19962\n",
      "19963\n",
      "19964\n",
      "19965\n",
      "19966\n",
      "19967\n",
      "19968\n",
      "19969\n",
      "19970\n",
      "19971\n",
      "19972\n",
      "19973\n",
      "19974\n",
      "19975\n",
      "19976\n",
      "19977\n",
      "19978\n",
      "19979\n",
      "19980\n",
      "19981\n",
      "19982\n",
      "19983\n",
      "19984\n",
      "19985\n",
      "19986\n",
      "19987\n",
      "19988\n",
      "19989\n",
      "19990\n",
      "19991\n",
      "19992\n",
      "19993\n",
      "19994\n",
      "19995\n",
      "19996\n",
      "19997\n",
      "19998\n",
      "19999\n",
      "20000\n",
      "20001\n",
      "20002\n",
      "20003\n",
      "20004\n",
      "20005\n",
      "20006\n",
      "20007\n",
      "20008\n",
      "20009\n",
      "20010\n",
      "20011\n",
      "20012\n",
      "20013\n",
      "20014\n",
      "20015\n",
      "20016\n",
      "20017\n",
      "20018\n",
      "20019\n",
      "20020\n",
      "20021\n",
      "20022\n",
      "20023\n",
      "20024\n",
      "20025\n",
      "20026\n",
      "20027\n",
      "20028\n",
      "20029\n",
      "20030\n",
      "20031\n",
      "20032\n",
      "20033\n",
      "20034\n",
      "20035\n",
      "20036\n",
      "20037\n",
      "20038\n",
      "20039\n",
      "20040\n",
      "20041\n",
      "20042\n",
      "20043\n",
      "20044\n",
      "20045\n",
      "20046\n",
      "20047\n",
      "20048\n",
      "20049\n",
      "20050\n",
      "20051\n",
      "20052\n",
      "20053\n",
      "20054\n",
      "20055\n",
      "20056\n",
      "20057\n",
      "20058\n",
      "20059\n",
      "20060\n",
      "20061\n",
      "20062\n",
      "20063\n",
      "20064\n",
      "20065\n",
      "20066\n",
      "20067\n",
      "20068\n",
      "20069\n",
      "20070\n",
      "20071\n",
      "20072\n",
      "20073\n",
      "20074\n",
      "20075\n",
      "20076\n",
      "20077\n",
      "20078\n",
      "20079\n",
      "20080\n",
      "20081\n",
      "20082\n",
      "20083\n",
      "20084\n",
      "20085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20086\n",
      "20087\n",
      "20088\n",
      "20089\n",
      "20090\n",
      "20091\n",
      "20092\n",
      "20093\n",
      "20094\n",
      "20095\n",
      "20096\n",
      "20097\n",
      "20098\n",
      "20099\n",
      "20100\n",
      "20101\n",
      "20102\n",
      "20103\n",
      "20104\n",
      "20105\n",
      "20106\n",
      "20107\n",
      "20108\n",
      "20109\n",
      "20110\n",
      "20111\n",
      "20112\n",
      "20113\n",
      "20114\n",
      "20115\n",
      "20116\n",
      "20117\n",
      "20118\n",
      "20119\n",
      "20120\n",
      "20121\n",
      "20122\n",
      "20123\n",
      "20124\n",
      "20125\n",
      "20126\n",
      "20127\n",
      "20128\n",
      "20129\n",
      "20130\n",
      "20131\n",
      "20132\n",
      "20133\n",
      "20134\n",
      "20135\n",
      "20136\n",
      "20137\n",
      "20138\n",
      "20139\n",
      "20140\n",
      "20141\n",
      "20142\n",
      "20143\n",
      "20144\n",
      "20145\n",
      "20146\n",
      "20147\n",
      "20148\n",
      "20149\n",
      "20150\n",
      "20151\n",
      "20152\n",
      "20153\n",
      "20154\n",
      "20155\n",
      "20156\n",
      "20157\n",
      "20158\n",
      "20159\n",
      "20160\n",
      "20161\n",
      "20162\n",
      "20163\n",
      "20164\n",
      "20165\n",
      "20166\n",
      "20167\n",
      "20168\n",
      "20169\n",
      "20170\n",
      "20171\n",
      "20172\n",
      "20173\n",
      "20174\n",
      "20175\n",
      "20176\n",
      "20177\n",
      "20178\n",
      "20179\n",
      "20180\n",
      "20181\n",
      "20182\n",
      "20183\n",
      "20184\n",
      "20185\n",
      "20186\n",
      "20187\n",
      "20188\n",
      "20189\n",
      "20190\n",
      "20191\n",
      "20192\n",
      "20193\n",
      "20194\n",
      "20195\n",
      "20196\n",
      "20197\n",
      "20198\n",
      "20199\n",
      "20200\n",
      "20201\n",
      "20202\n",
      "20203\n",
      "20204\n",
      "20205\n",
      "20206\n",
      "20207\n",
      "20208\n",
      "20209\n",
      "20210\n",
      "20211\n",
      "20212\n",
      "20213\n",
      "20214\n",
      "20215\n",
      "20216\n",
      "20217\n",
      "20218\n",
      "20219\n",
      "20220\n",
      "20221\n",
      "20222\n",
      "20223\n",
      "20224\n",
      "20225\n",
      "20226\n",
      "20227\n",
      "20228\n",
      "20229\n",
      "20230\n",
      "20231\n",
      "20232\n",
      "20233\n",
      "20234\n",
      "20235\n",
      "20236\n",
      "20237\n",
      "20238\n",
      "20239\n",
      "20240\n",
      "20241\n",
      "20242\n",
      "20243\n",
      "20244\n",
      "20245\n",
      "20246\n",
      "20247\n",
      "20248\n",
      "20249\n",
      "20250\n",
      "20251\n",
      "20252\n",
      "20253\n",
      "20254\n",
      "20255\n",
      "20256\n",
      "20257\n",
      "20258\n",
      "20259\n",
      "20260\n",
      "20261\n",
      "20262\n",
      "20263\n",
      "20264\n",
      "20265\n",
      "20266\n",
      "20267\n",
      "20268\n",
      "20269\n",
      "20270\n",
      "20271\n",
      "20272\n",
      "20273\n",
      "20274\n",
      "20275\n",
      "20276\n",
      "20277\n",
      "20278\n",
      "20279\n",
      "20280\n",
      "20281\n",
      "20282\n",
      "20283\n",
      "20284\n",
      "20285\n",
      "20286\n",
      "20287\n",
      "20288\n",
      "20289\n",
      "20290\n",
      "20291\n",
      "20292\n",
      "20293\n",
      "20294\n",
      "20295\n",
      "20296\n",
      "20297\n",
      "20298\n",
      "20299\n",
      "20300\n",
      "20301\n",
      "20302\n",
      "20303\n",
      "20304\n",
      "20305\n",
      "20306\n",
      "20307\n",
      "20308\n",
      "20309\n",
      "20310\n",
      "20311\n",
      "20312\n",
      "20313\n",
      "20314\n",
      "20315\n",
      "20316\n",
      "20317\n",
      "20318\n",
      "20319\n",
      "20320\n",
      "20321\n",
      "20322\n",
      "20323\n",
      "20324\n",
      "20325\n",
      "20326\n",
      "20327\n",
      "20328\n",
      "20329\n",
      "20330\n",
      "20331\n",
      "20332\n",
      "20333\n",
      "20334\n",
      "20335\n",
      "20336\n",
      "20337\n",
      "20338\n",
      "20339\n",
      "20340\n",
      "20341\n",
      "20342\n",
      "20343\n",
      "20344\n",
      "20345\n",
      "20346\n",
      "20347\n",
      "20348\n",
      "20349\n",
      "20350\n",
      "20351\n",
      "20352\n",
      "20353\n",
      "20354\n",
      "20355\n",
      "20356\n",
      "20357\n",
      "20358\n",
      "20359\n",
      "20360\n",
      "20361\n",
      "20362\n",
      "20363\n",
      "20364\n",
      "20365\n",
      "20366\n",
      "20367\n",
      "20368\n",
      "20369\n",
      "20370\n",
      "20371\n",
      "20372\n",
      "20373\n",
      "20374\n",
      "20375\n",
      "20376\n",
      "20377\n",
      "20378\n",
      "20379\n",
      "20380\n",
      "20381\n",
      "20382\n",
      "20383\n",
      "20384\n",
      "20385\n",
      "20386\n",
      "20387\n",
      "20388\n",
      "20389\n",
      "20390\n",
      "20391\n",
      "20392\n",
      "20393\n",
      "20394\n",
      "20395\n",
      "20396\n",
      "20397\n",
      "20398\n",
      "20399\n",
      "20400\n",
      "20401\n",
      "20402\n",
      "20403\n",
      "20404\n",
      "20405\n",
      "20406\n",
      "20407\n",
      "20408\n",
      "20409\n",
      "20410\n",
      "20411\n",
      "20412\n",
      "20413\n",
      "20414\n",
      "20415\n",
      "20416\n",
      "20417\n",
      "20418\n",
      "20419\n",
      "20420\n",
      "20421\n",
      "20422\n",
      "20423\n",
      "20424\n",
      "20425\n",
      "20426\n",
      "20427\n",
      "20428\n",
      "20429\n",
      "20430\n",
      "20431\n",
      "20432\n",
      "20433\n",
      "20434\n",
      "20435\n",
      "20436\n",
      "20437\n",
      "20438\n",
      "20439\n",
      "20440\n",
      "20441\n",
      "20442\n",
      "20443\n",
      "20444\n",
      "20445\n",
      "20446\n",
      "20447\n",
      "20448\n",
      "20449\n",
      "20450\n",
      "20451\n",
      "20452\n",
      "20453\n",
      "20454\n",
      "20455\n",
      "20456\n",
      "20457\n",
      "20458\n",
      "20459\n",
      "20460\n",
      "20461\n",
      "20462\n",
      "20463\n",
      "20464\n",
      "20465\n",
      "20466\n",
      "20467\n",
      "20468\n",
      "20469\n",
      "20470\n",
      "20471\n",
      "20472\n",
      "20473\n",
      "20474\n",
      "20475\n",
      "20476\n",
      "20477\n",
      "20478\n",
      "20479\n",
      "20480\n",
      "20481\n",
      "20482\n",
      "20483\n",
      "20484\n",
      "20485\n",
      "20486\n",
      "20487\n",
      "20488\n",
      "20489\n",
      "20490\n",
      "20491\n",
      "20492\n",
      "20493\n",
      "20494\n",
      "20495\n",
      "20496\n",
      "20497\n",
      "20498\n",
      "20499\n",
      "20500\n",
      "20501\n",
      "20502\n",
      "20503\n",
      "20504\n",
      "20505\n",
      "20506\n",
      "20507\n",
      "20508\n",
      "20509\n",
      "20510\n",
      "20511\n",
      "20512\n",
      "20513\n",
      "20514\n",
      "20515\n",
      "20516\n",
      "20517\n",
      "20518\n",
      "20519\n",
      "20520\n",
      "20521\n",
      "20522\n",
      "20523\n",
      "20524\n",
      "20525\n",
      "20526\n",
      "20527\n",
      "20528\n",
      "20529\n",
      "20530\n",
      "20531\n",
      "20532\n",
      "20533\n",
      "20534\n",
      "20535\n",
      "20536\n",
      "20537\n",
      "20538\n",
      "20539\n",
      "20540\n",
      "20541\n",
      "20542\n",
      "20543\n",
      "20544\n",
      "20545\n",
      "20546\n",
      "20547\n",
      "20548\n",
      "20549\n",
      "20550\n",
      "20551\n",
      "20552\n",
      "20553\n",
      "20554\n",
      "20555\n",
      "20556\n",
      "20557\n",
      "20558\n",
      "20559\n",
      "20560\n",
      "20561\n",
      "20562\n",
      "20563\n",
      "20564\n",
      "20565\n",
      "20566\n",
      "20567\n",
      "20568\n",
      "20569\n",
      "20570\n",
      "20571\n",
      "20572\n",
      "20573\n",
      "20574\n",
      "20575\n",
      "20576\n",
      "20577\n",
      "20578\n",
      "20579\n",
      "20580\n",
      "20581\n",
      "20582\n",
      "20583\n",
      "20584\n",
      "20585\n",
      "20586\n",
      "20587\n",
      "20588\n",
      "20589\n",
      "20590\n",
      "20591\n",
      "20592\n",
      "20593\n",
      "20594\n",
      "20595\n",
      "20596\n",
      "20597\n",
      "20598\n",
      "20599\n",
      "20600\n",
      "20601\n",
      "20602\n",
      "20603\n",
      "20604\n",
      "20605\n",
      "20606\n",
      "20607\n",
      "20608\n",
      "20609\n",
      "20610\n",
      "20611\n",
      "20612\n",
      "20613\n",
      "20614\n",
      "20615\n",
      "20616\n",
      "20617\n",
      "20618\n",
      "20619\n",
      "20620\n",
      "20621\n",
      "20622\n",
      "20623\n",
      "20624\n",
      "20625\n",
      "20626\n",
      "20627\n",
      "20628\n",
      "20629\n",
      "20630\n",
      "20631\n",
      "20632\n",
      "20633\n",
      "20634\n",
      "20635\n",
      "20636\n",
      "20637\n",
      "20638\n",
      "20639\n",
      "20640\n",
      "20641\n",
      "20642\n",
      "20643\n",
      "20644\n",
      "20645\n",
      "20646\n",
      "20647\n",
      "20648\n",
      "20649\n",
      "20650\n",
      "20651\n",
      "20652\n",
      "20653\n",
      "20654\n",
      "20655\n",
      "20656\n",
      "20657\n",
      "20658\n",
      "20659\n",
      "20660\n",
      "20661\n",
      "20662\n",
      "20663\n",
      "20664\n",
      "20665\n",
      "20666\n",
      "20667\n",
      "20668\n",
      "20669\n",
      "20670\n",
      "20671\n",
      "20672\n",
      "20673\n",
      "20674\n",
      "20675\n",
      "20676\n",
      "20677\n",
      "20678\n",
      "20679\n",
      "20680\n",
      "20681\n",
      "20682\n",
      "20683\n",
      "20684\n",
      "20685\n",
      "20686\n",
      "20687\n",
      "20688\n",
      "20689\n",
      "20690\n",
      "20691\n",
      "20692\n",
      "20693\n",
      "20694\n",
      "20695\n",
      "20696\n",
      "20697\n",
      "20698\n",
      "20699\n",
      "20700\n",
      "20701\n",
      "20702\n",
      "20703\n",
      "20704\n",
      "20705\n",
      "20706\n",
      "20707\n",
      "20708\n",
      "20709\n",
      "20710\n",
      "20711\n",
      "20712\n",
      "20713\n",
      "20714\n",
      "20715\n",
      "20716\n",
      "20717\n",
      "20718\n",
      "20719\n",
      "20720\n",
      "20721\n",
      "20722\n",
      "20723\n",
      "20724\n",
      "20725\n",
      "20726\n",
      "20727\n",
      "20728\n",
      "20729\n",
      "20730\n",
      "20731\n",
      "20732\n",
      "20733\n",
      "20734\n",
      "20735\n",
      "20736\n",
      "20737\n",
      "20738\n",
      "20739\n",
      "20740\n",
      "20741\n",
      "20742\n",
      "20743\n",
      "20744\n",
      "20745\n",
      "20746\n",
      "20747\n",
      "20748\n",
      "20749\n",
      "20750\n",
      "20751\n",
      "20752\n",
      "20753\n",
      "20754\n",
      "20755\n",
      "20756\n",
      "20757\n",
      "20758\n",
      "20759\n",
      "20760\n",
      "20761\n",
      "20762\n",
      "20763\n",
      "20764\n",
      "20765\n",
      "20766\n",
      "20767\n",
      "20768\n",
      "20769\n",
      "20770\n",
      "20771\n",
      "20772\n",
      "20773\n",
      "20774\n",
      "20775\n",
      "20776\n",
      "20777\n",
      "20778\n",
      "20779\n",
      "20780\n",
      "20781\n",
      "20782\n",
      "20783\n",
      "20784\n",
      "20785\n",
      "20786\n",
      "20787\n",
      "20788\n",
      "20789\n",
      "20790\n",
      "20791\n",
      "20792\n",
      "20793\n",
      "20794\n",
      "20795\n",
      "20796\n",
      "20797\n",
      "20798\n",
      "20799\n",
      "20800\n",
      "20801\n",
      "20802\n",
      "20803\n",
      "20804\n",
      "20805\n",
      "20806\n",
      "20807\n",
      "20808\n",
      "20809\n",
      "20810\n",
      "20811\n",
      "20812\n",
      "20813\n",
      "20814\n",
      "20815\n",
      "20816\n",
      "20817\n",
      "20818\n",
      "20819\n",
      "20820\n",
      "20821\n",
      "20822\n",
      "20823\n",
      "20824\n",
      "20825\n",
      "20826\n",
      "20827\n",
      "20828\n",
      "20829\n",
      "20830\n",
      "20831\n",
      "20832\n",
      "20833\n",
      "20834\n",
      "20835\n",
      "20836\n",
      "20837\n",
      "20838\n",
      "20839\n",
      "20840\n",
      "20841\n",
      "20842\n",
      "20843\n",
      "20844\n",
      "20845\n",
      "20846\n",
      "20847\n",
      "20848\n",
      "20849\n",
      "20850\n",
      "20851\n",
      "20852\n",
      "20853\n",
      "20854\n",
      "20855\n",
      "20856\n",
      "20857\n",
      "20858\n",
      "20859\n",
      "20860\n",
      "20861\n",
      "20862\n",
      "20863\n",
      "20864\n",
      "20865\n",
      "20866\n",
      "20867\n",
      "20868\n",
      "20869\n",
      "20870\n",
      "20871\n",
      "20872\n",
      "20873\n",
      "20874\n",
      "20875\n",
      "20876\n",
      "20877\n",
      "20878\n",
      "20879\n",
      "20880\n",
      "20881\n",
      "20882\n",
      "20883\n",
      "20884\n",
      "20885\n",
      "20886\n",
      "20887\n",
      "20888\n",
      "20889\n",
      "20890\n",
      "20891\n",
      "20892\n",
      "20893\n",
      "20894\n",
      "20895\n",
      "20896\n",
      "20897\n",
      "20898\n",
      "20899\n",
      "20900\n",
      "20901\n",
      "20902\n",
      "20903\n",
      "20904\n",
      "20905\n",
      "20906\n",
      "20907\n",
      "20908\n",
      "20909\n",
      "20910\n",
      "20911\n",
      "20912\n",
      "20913\n",
      "20914\n",
      "20915\n",
      "20916\n",
      "20917\n",
      "20918\n",
      "20919\n",
      "20920\n",
      "20921\n",
      "20922\n",
      "20923\n",
      "20924\n",
      "20925\n",
      "20926\n",
      "20927\n",
      "20928\n",
      "20929\n",
      "20930\n",
      "20931\n",
      "20932\n",
      "20933\n",
      "20934\n",
      "20935\n",
      "20936\n",
      "20937\n",
      "20938\n",
      "20939\n",
      "20940\n",
      "20941\n",
      "20942\n",
      "20943\n",
      "20944\n",
      "20945\n",
      "20946\n",
      "20947\n",
      "20948\n",
      "20949\n",
      "20950\n",
      "20951\n",
      "20952\n",
      "20953\n",
      "20954\n",
      "20955\n",
      "20956\n",
      "20957\n",
      "20958\n",
      "20959\n",
      "20960\n",
      "20961\n",
      "20962\n",
      "20963\n",
      "20964\n",
      "20965\n",
      "20966\n",
      "20967\n",
      "20968\n",
      "20969\n",
      "20970\n",
      "20971\n",
      "20972\n",
      "20973\n",
      "20974\n",
      "20975\n",
      "20976\n",
      "20977\n",
      "20978\n",
      "20979\n",
      "20980\n",
      "20981\n",
      "20982\n",
      "20983\n",
      "20984\n",
      "20985\n",
      "20986\n",
      "20987\n",
      "20988\n",
      "20989\n",
      "20990\n",
      "20991\n",
      "20992\n",
      "20993\n",
      "20994\n",
      "20995\n",
      "20996\n",
      "20997\n",
      "20998\n",
      "20999\n",
      "21000\n",
      "21001\n",
      "21002\n",
      "21003\n",
      "21004\n",
      "21005\n",
      "21006\n",
      "21007\n",
      "21008\n",
      "21009\n",
      "21010\n",
      "21011\n",
      "21012\n",
      "21013\n",
      "21014\n",
      "21015\n",
      "21016\n",
      "21017\n",
      "21018\n",
      "21019\n",
      "21020\n",
      "21021\n",
      "21022\n",
      "21023\n",
      "21024\n",
      "21025\n",
      "21026\n",
      "21027\n",
      "21028\n",
      "21029\n",
      "21030\n",
      "21031\n",
      "21032\n",
      "21033\n",
      "21034\n",
      "21035\n",
      "21036\n",
      "21037\n",
      "21038\n",
      "21039\n",
      "21040\n",
      "21041\n",
      "21042\n",
      "21043\n",
      "21044\n",
      "21045\n",
      "21046\n",
      "21047\n",
      "21048\n",
      "21049\n",
      "21050\n",
      "21051\n",
      "21052\n",
      "21053\n",
      "21054\n",
      "21055\n",
      "21056\n",
      "21057\n",
      "21058\n",
      "21059\n",
      "21060\n",
      "21061\n",
      "21062\n",
      "21063\n",
      "21064\n",
      "21065\n",
      "21066\n",
      "21067\n",
      "21068\n",
      "21069\n",
      "21070\n",
      "21071\n",
      "21072\n",
      "21073\n",
      "21074\n",
      "21075\n",
      "21076\n",
      "21077\n",
      "21078\n",
      "21079\n",
      "21080\n",
      "21081\n",
      "21082\n",
      "21083\n",
      "21084\n",
      "21085\n",
      "21086\n",
      "21087\n",
      "21088\n",
      "21089\n",
      "21090\n",
      "21091\n",
      "21092\n",
      "21093\n",
      "21094\n",
      "21095\n",
      "21096\n",
      "21097\n",
      "21098\n",
      "21099\n",
      "21100\n",
      "21101\n",
      "21102\n",
      "21103\n",
      "21104\n",
      "21105\n",
      "21106\n",
      "21107\n",
      "21108\n",
      "21109\n",
      "21110\n",
      "21111\n",
      "21112\n",
      "21113\n",
      "21114\n",
      "21115\n",
      "21116\n",
      "21117\n",
      "21118\n",
      "21119\n",
      "21120\n",
      "21121\n",
      "21122\n",
      "21123\n",
      "21124\n",
      "21125\n",
      "21126\n",
      "21127\n",
      "21128\n",
      "21129\n",
      "21130\n",
      "21131\n",
      "21132\n",
      "21133\n",
      "21134\n",
      "21135\n",
      "21136\n",
      "21137\n",
      "21138\n",
      "21139\n",
      "21140\n",
      "21141\n",
      "21142\n",
      "21143\n",
      "21144\n",
      "21145\n",
      "21146\n",
      "21147\n",
      "21148\n",
      "21149\n",
      "21150\n",
      "21151\n",
      "21152\n",
      "21153\n",
      "21154\n",
      "21155\n",
      "21156\n",
      "21157\n",
      "21158\n",
      "21159\n",
      "21160\n",
      "21161\n",
      "21162\n",
      "21163\n",
      "21164\n",
      "21165\n",
      "21166\n",
      "21167\n",
      "21168\n",
      "21169\n",
      "21170\n",
      "21171\n",
      "21172\n",
      "21173\n",
      "21174\n",
      "21175\n",
      "21176\n",
      "21177\n",
      "21178\n",
      "21179\n",
      "21180\n",
      "21181\n",
      "21182\n",
      "21183\n",
      "21184\n",
      "21185\n",
      "21186\n",
      "21187\n",
      "21188\n",
      "21189\n",
      "21190\n",
      "21191\n",
      "21192\n",
      "21193\n",
      "21194\n",
      "21195\n",
      "21196\n",
      "21197\n",
      "21198\n",
      "21199\n",
      "21200\n",
      "21201\n",
      "21202\n",
      "21203\n",
      "21204\n",
      "21205\n",
      "21206\n",
      "21207\n",
      "21208\n",
      "21209\n",
      "21210\n",
      "21211\n",
      "21212\n",
      "21213\n",
      "21214\n",
      "21215\n",
      "21216\n",
      "21217\n",
      "21218\n",
      "21219\n",
      "21220\n",
      "21221\n",
      "21222\n",
      "21223\n",
      "21224\n",
      "21225\n",
      "21226\n",
      "21227\n",
      "21228\n",
      "21229\n",
      "21230\n",
      "21231\n",
      "21232\n",
      "21233\n",
      "21234\n",
      "21235\n",
      "21236\n",
      "21237\n",
      "21238\n",
      "21239\n",
      "21240\n",
      "21241\n",
      "21242\n",
      "21243\n",
      "21244\n",
      "21245\n",
      "21246\n",
      "21247\n",
      "21248\n",
      "21249\n",
      "21250\n",
      "21251\n",
      "21252\n",
      "21253\n",
      "21254\n",
      "21255\n",
      "21256\n",
      "21257\n",
      "21258\n",
      "21259\n",
      "21260\n",
      "21261\n",
      "21262\n",
      "21263\n",
      "21264\n",
      "21265\n",
      "21266\n",
      "21267\n",
      "21268\n",
      "21269\n",
      "21270\n",
      "21271\n",
      "21272\n",
      "21273\n",
      "21274\n",
      "21275\n",
      "21276\n",
      "21277\n",
      "21278\n",
      "21279\n",
      "21280\n",
      "21281\n",
      "21282\n",
      "21283\n",
      "21284\n",
      "21285\n",
      "21286\n",
      "21287\n",
      "21288\n",
      "21289\n",
      "21290\n",
      "21291\n",
      "21292\n",
      "21293\n",
      "21294\n",
      "21295\n",
      "21296\n",
      "21297\n",
      "21298\n",
      "21299\n",
      "21300\n",
      "21301\n",
      "21302\n",
      "21303\n",
      "21304\n",
      "21305\n",
      "21306\n",
      "21307\n",
      "21308\n",
      "21309\n",
      "21310\n",
      "21311\n",
      "21312\n",
      "21313\n",
      "21314\n",
      "21315\n",
      "21316\n",
      "21317\n",
      "21318\n",
      "21319\n",
      "21320\n",
      "21321\n",
      "21322\n",
      "21323\n",
      "21324\n",
      "21325\n",
      "21326\n",
      "21327\n",
      "21328\n",
      "21329\n",
      "21330\n",
      "21331\n",
      "21332\n",
      "21333\n",
      "21334\n",
      "21335\n",
      "21336\n",
      "21337\n",
      "21338\n",
      "21339\n",
      "21340\n",
      "21341\n",
      "21342\n",
      "21343\n",
      "21344\n",
      "21345\n",
      "21346\n",
      "21347\n",
      "21348\n",
      "21349\n",
      "21350\n",
      "21351\n",
      "21352\n",
      "21353\n",
      "21354\n",
      "21355\n",
      "21356\n",
      "21357\n",
      "21358\n",
      "21359\n",
      "21360\n",
      "21361\n",
      "21362\n",
      "21363\n",
      "21364\n",
      "21365\n",
      "21366\n",
      "21367\n",
      "21368\n",
      "21369\n",
      "21370\n",
      "21371\n",
      "21372\n",
      "21373\n",
      "21374\n",
      "21375\n",
      "21376\n",
      "21377\n",
      "21378\n",
      "21379\n",
      "21380\n",
      "21381\n",
      "21382\n",
      "21383\n",
      "21384\n",
      "21385\n",
      "21386\n",
      "21387\n",
      "21388\n",
      "21389\n",
      "21390\n",
      "21391\n",
      "21392\n",
      "21393\n",
      "21394\n",
      "21395\n",
      "21396\n",
      "21397\n",
      "21398\n",
      "21399\n",
      "21400\n",
      "21401\n",
      "21402\n",
      "21403\n",
      "21404\n",
      "21405\n",
      "21406\n",
      "21407\n",
      "21408\n",
      "21409\n",
      "21410\n",
      "21411\n",
      "21412\n",
      "21413\n",
      "21414\n",
      "21415\n",
      "21416\n",
      "21417\n",
      "21418\n",
      "21419\n",
      "21420\n",
      "21421\n",
      "21422\n",
      "21423\n",
      "21424\n",
      "21425\n",
      "21426\n",
      "21427\n",
      "21428\n",
      "21429\n",
      "21430\n",
      "21431\n",
      "21432\n",
      "21433\n",
      "21434\n",
      "21435\n",
      "21436\n",
      "21437\n",
      "21438\n",
      "21439\n",
      "21440\n",
      "21441\n",
      "21442\n",
      "21443\n",
      "21444\n",
      "21445\n",
      "21446\n",
      "21447\n",
      "21448\n",
      "21449\n",
      "21450\n",
      "21451\n",
      "21452\n",
      "21453\n",
      "21454\n",
      "21455\n",
      "21456\n",
      "21457\n",
      "21458\n",
      "21459\n",
      "21460\n",
      "21461\n",
      "21462\n",
      "21463\n",
      "21464\n",
      "21465\n",
      "21466\n",
      "21467\n",
      "21468\n",
      "21469\n",
      "21470\n",
      "21471\n",
      "21472\n",
      "21473\n",
      "21474\n",
      "21475\n",
      "21476\n",
      "21477\n",
      "21478\n",
      "21479\n",
      "21480\n",
      "21481\n",
      "21482\n",
      "21483\n",
      "21484\n",
      "21485\n",
      "21486\n",
      "21487\n",
      "21488\n",
      "21489\n",
      "21490\n",
      "21491\n",
      "21492\n",
      "21493\n",
      "21494\n",
      "21495\n",
      "21496\n",
      "21497\n",
      "21498\n",
      "21499\n",
      "21500\n",
      "21501\n",
      "21502\n",
      "21503\n",
      "21504\n",
      "21505\n",
      "21506\n",
      "21507\n",
      "21508\n",
      "21509\n",
      "21510\n",
      "21511\n",
      "21512\n",
      "21513\n",
      "21514\n",
      "21515\n",
      "21516\n",
      "21517\n",
      "21518\n",
      "21519\n",
      "21520\n",
      "21521\n",
      "21522\n",
      "21523\n",
      "21524\n",
      "21525\n",
      "21526\n",
      "21527\n",
      "21528\n",
      "21529\n",
      "21530\n",
      "21531\n",
      "21532\n",
      "21533\n",
      "21534\n",
      "21535\n",
      "21536\n",
      "21537\n",
      "21538\n",
      "21539\n",
      "21540\n",
      "21541\n",
      "21542\n",
      "21543\n",
      "21544\n",
      "21545\n",
      "21546\n",
      "21547\n",
      "21548\n",
      "21549\n",
      "21550\n",
      "21551\n",
      "21552\n",
      "21553\n",
      "21554\n",
      "21555\n",
      "21556\n",
      "21557\n",
      "21558\n",
      "21559\n",
      "21560\n",
      "21561\n",
      "21562\n",
      "21563\n",
      "21564\n",
      "21565\n",
      "21566\n",
      "21567\n",
      "21568\n",
      "21569\n",
      "21570\n",
      "21571\n",
      "21572\n",
      "21573\n",
      "21574\n",
      "21575\n",
      "21576\n",
      "21577\n",
      "21578\n",
      "21579\n",
      "21580\n",
      "21581\n",
      "21582\n",
      "21583\n",
      "21584\n",
      "21585\n",
      "21586\n",
      "21587\n",
      "21588\n",
      "21589\n",
      "21590\n",
      "21591\n",
      "21592\n",
      "21593\n",
      "21594\n",
      "21595\n",
      "21596\n",
      "21597\n",
      "21598\n",
      "21599\n",
      "21600\n",
      "21601\n",
      "21602\n",
      "21603\n",
      "21604\n",
      "21605\n",
      "21606\n",
      "21607\n",
      "21608\n",
      "21609\n",
      "21610\n",
      "21611\n",
      "21612\n",
      "21613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21614\n",
      "21615\n",
      "21616\n",
      "21617\n",
      "21618\n",
      "21619\n",
      "21620\n",
      "21621\n",
      "21622\n",
      "21623\n",
      "21624\n",
      "21625\n",
      "21626\n",
      "21627\n",
      "21628\n",
      "21629\n",
      "21630\n",
      "21631\n",
      "21632\n",
      "21633\n",
      "21634\n",
      "21635\n",
      "21636\n",
      "21637\n",
      "21638\n",
      "21639\n",
      "21640\n",
      "21641\n",
      "21642\n",
      "21643\n",
      "21644\n",
      "21645\n",
      "21646\n",
      "21647\n",
      "21648\n",
      "21649\n",
      "21650\n",
      "21651\n",
      "21652\n",
      "21653\n",
      "21654\n",
      "21655\n",
      "21656\n",
      "21657\n",
      "21658\n",
      "21659\n",
      "21660\n",
      "21661\n",
      "21662\n",
      "21663\n",
      "21664\n",
      "21665\n",
      "21666\n",
      "21667\n",
      "21668\n",
      "21669\n",
      "21670\n",
      "21671\n",
      "21672\n",
      "21673\n",
      "21674\n",
      "21675\n",
      "21676\n",
      "21677\n",
      "21678\n",
      "21679\n",
      "21680\n",
      "21681\n",
      "21682\n",
      "21683\n",
      "21684\n",
      "21685\n",
      "21686\n",
      "21687\n",
      "21688\n",
      "21689\n",
      "21690\n",
      "21691\n",
      "21692\n",
      "21693\n",
      "21694\n",
      "21695\n",
      "21696\n",
      "21697\n",
      "21698\n",
      "21699\n",
      "21700\n",
      "21701\n",
      "21702\n",
      "21703\n",
      "21704\n",
      "21705\n",
      "21706\n",
      "21707\n",
      "21708\n",
      "21709\n",
      "21710\n",
      "21711\n",
      "21712\n",
      "21713\n",
      "21714\n",
      "21715\n",
      "21716\n",
      "21717\n",
      "21718\n",
      "21719\n",
      "21720\n",
      "21721\n",
      "21722\n",
      "21723\n",
      "21724\n",
      "21725\n",
      "21726\n",
      "21727\n",
      "21728\n",
      "21729\n",
      "21730\n",
      "21731\n",
      "21732\n",
      "21733\n",
      "21734\n",
      "21735\n",
      "21736\n",
      "21737\n",
      "21738\n",
      "21739\n",
      "21740\n",
      "21741\n",
      "21742\n",
      "21743\n",
      "21744\n",
      "21745\n",
      "21746\n",
      "21747\n",
      "21748\n",
      "21749\n",
      "21750\n",
      "21751\n",
      "21752\n",
      "21753\n",
      "21754\n",
      "21755\n",
      "21756\n",
      "21757\n",
      "21758\n",
      "21759\n",
      "21760\n",
      "21761\n",
      "21762\n",
      "21763\n",
      "21764\n",
      "21765\n",
      "21766\n",
      "21767\n",
      "21768\n",
      "21769\n",
      "21770\n",
      "21771\n",
      "21772\n",
      "21773\n",
      "21774\n",
      "21775\n",
      "21776\n",
      "21777\n",
      "21778\n",
      "21779\n",
      "21780\n",
      "21781\n",
      "21782\n",
      "21783\n",
      "21784\n",
      "21785\n",
      "21786\n",
      "21787\n",
      "21788\n",
      "21789\n",
      "21790\n",
      "21791\n",
      "21792\n",
      "21793\n",
      "21794\n",
      "21795\n",
      "21796\n",
      "21797\n",
      "21798\n",
      "21799\n",
      "21800\n",
      "21801\n",
      "21802\n",
      "21803\n",
      "21804\n",
      "21805\n",
      "21806\n",
      "21807\n",
      "21808\n",
      "21809\n",
      "21810\n",
      "21811\n",
      "21812\n",
      "21813\n",
      "21814\n",
      "21815\n",
      "21816\n",
      "21817\n",
      "21818\n",
      "21819\n",
      "21820\n",
      "21821\n",
      "21822\n",
      "21823\n",
      "21824\n",
      "21825\n",
      "21826\n",
      "21827\n",
      "21828\n",
      "21829\n",
      "21830\n",
      "21831\n",
      "21832\n",
      "21833\n",
      "21834\n",
      "21835\n",
      "21836\n",
      "21837\n",
      "21838\n",
      "21839\n",
      "21840\n",
      "21841\n",
      "21842\n",
      "21843\n",
      "21844\n",
      "21845\n",
      "21846\n",
      "21847\n",
      "21848\n",
      "21849\n",
      "21850\n",
      "21851\n",
      "21852\n",
      "21853\n",
      "21854\n",
      "21855\n",
      "21856\n",
      "21857\n",
      "21858\n",
      "21859\n",
      "21860\n",
      "21861\n",
      "21862\n",
      "21863\n",
      "21864\n",
      "21865\n",
      "21866\n",
      "21867\n",
      "21868\n",
      "21869\n",
      "21870\n",
      "21871\n",
      "21872\n",
      "21873\n",
      "21874\n",
      "21875\n",
      "21876\n",
      "21877\n",
      "21878\n",
      "21879\n",
      "21880\n",
      "21881\n",
      "21882\n",
      "21883\n",
      "21884\n",
      "21885\n",
      "21886\n",
      "21887\n",
      "21888\n",
      "21889\n",
      "21890\n",
      "21891\n",
      "21892\n",
      "21893\n",
      "21894\n",
      "21895\n",
      "21896\n",
      "21897\n",
      "21898\n",
      "21899\n",
      "21900\n",
      "21901\n",
      "21902\n",
      "21903\n",
      "21904\n",
      "21905\n",
      "21906\n",
      "21907\n",
      "21908\n",
      "21909\n",
      "21910\n",
      "21911\n",
      "21912\n",
      "21913\n",
      "21914\n",
      "21915\n",
      "21916\n",
      "21917\n",
      "21918\n",
      "21919\n",
      "21920\n",
      "21921\n",
      "21922\n",
      "21923\n",
      "21924\n",
      "21925\n",
      "21926\n",
      "21927\n",
      "21928\n",
      "21929\n",
      "21930\n",
      "21931\n",
      "21932\n",
      "21933\n",
      "21934\n",
      "21935\n",
      "21936\n",
      "21937\n",
      "21938\n",
      "21939\n",
      "21940\n",
      "21941\n",
      "21942\n",
      "21943\n",
      "21944\n",
      "21945\n",
      "21946\n",
      "21947\n",
      "21948\n",
      "21949\n",
      "21950\n",
      "21951\n",
      "21952\n",
      "21953\n",
      "21954\n",
      "21955\n",
      "21956\n",
      "21957\n",
      "21958\n",
      "21959\n",
      "21960\n",
      "21961\n",
      "21962\n",
      "21963\n",
      "21964\n",
      "21965\n",
      "21966\n",
      "21967\n",
      "21968\n",
      "21969\n",
      "21970\n",
      "21971\n",
      "21972\n",
      "21973\n",
      "21974\n",
      "21975\n",
      "21976\n",
      "21977\n",
      "21978\n",
      "21979\n",
      "21980\n",
      "21981\n",
      "21982\n",
      "21983\n",
      "21984\n",
      "21985\n",
      "21986\n",
      "21987\n",
      "21988\n",
      "21989\n",
      "21990\n",
      "21991\n",
      "21992\n",
      "21993\n",
      "21994\n",
      "21995\n",
      "21996\n",
      "21997\n",
      "21998\n",
      "21999\n",
      "22000\n",
      "22001\n",
      "22002\n",
      "22003\n",
      "22004\n",
      "22005\n",
      "22006\n",
      "22007\n",
      "22008\n",
      "22009\n",
      "22010\n",
      "22011\n",
      "22012\n",
      "22013\n",
      "22014\n",
      "22015\n",
      "22016\n",
      "22017\n",
      "22018\n",
      "22019\n",
      "22020\n",
      "22021\n",
      "22022\n",
      "22023\n",
      "22024\n",
      "22025\n",
      "22026\n",
      "22027\n",
      "22028\n",
      "22029\n",
      "22030\n",
      "22031\n",
      "22032\n",
      "22033\n",
      "22034\n",
      "22035\n",
      "22036\n",
      "22037\n",
      "22038\n",
      "22039\n",
      "22040\n",
      "22041\n",
      "22042\n",
      "22043\n",
      "22044\n",
      "22045\n",
      "22046\n",
      "22047\n",
      "22048\n",
      "22049\n",
      "22050\n",
      "22051\n",
      "22052\n",
      "22053\n",
      "22054\n",
      "22055\n",
      "22056\n",
      "22057\n",
      "Done getting embedding weights\n"
     ]
    }
   ],
   "source": [
    "token2id,id2token,emb_weights = build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22059"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(emb_weights,open('emb_weights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(token2id,open('token2id.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(id2token,open('id2token.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20434"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.arange(0,2*3*4).reshape(2,3,4))\n",
    "b = np.zeros((2,3,1),dtype=int)\n",
    "b[0,2,:] = 1\n",
    "b[1,1,:] = 1\n",
    "b = torch.from_numpy(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id = pkl.load(open('token2id.pkl','rb'))\n",
    "id2token = pkl.load(open('id2token.pkl','rb'))\n",
    "emb_weights = pkl.load(open('emb_weights.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, token2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.sentence1,self.sentence2,self.target_list = df['sentence1'].values,df['sentence2'].values,df['label'].values\n",
    "        assert (len(self.sentence1) == len(self.target_list))\n",
    "        self.token2id = token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        x1_mask,x2_mask = [],[]\n",
    "        x1_word_idx,x2_word_idx = [],[]\n",
    "        for word in self.sentence1[i][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x1_word_idx.append(self.token2id[word])\n",
    "                x1_mask.append(0)\n",
    "            else:\n",
    "                x1_word_idx.append(UNK_IDX)\n",
    "                x1_mask.append(1)\n",
    "        \n",
    "        for word in self.sentence2[i][:MAX_SENTENCE_LENGTH]:\n",
    "            if word in self.token2id.keys():\n",
    "                x2_word_idx.append(self.token2id[word])\n",
    "                x2_mask.append(0)\n",
    "            else:\n",
    "                x2_word_idx.append(UNK_IDX)\n",
    "                x2_mask.append(1)\n",
    "                \n",
    "        label = self.target_list[i]\n",
    "        return [x1_word_idx,x2_word_idx, len(x1_word_idx),len(x2_word_idx),x1_mask,x2_mask, label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snli_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    x1_list = []\n",
    "    x2_list = []\n",
    "    length_x1_list = []\n",
    "    length_x2_list = []\n",
    "    label_list = []\n",
    "    x1_mask_list,x2_mask_list = [],[]\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[6])\n",
    "        length_x1_list.append(datum[2])\n",
    "        length_x2_list.append(datum[3])\n",
    "        x1_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x2_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_mask_padded_vec = np.pad(np.array(datum[4]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x2_mask_padded_vec = np.pad(np.array(datum[5]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_list.append(x1_padded_vec)\n",
    "        x2_list.append(x2_padded_vec)\n",
    "        x1_mask_list.append(x1_mask_padded_vec)\n",
    "        x2_mask_list.append(x2_mask_padded_vec)\n",
    "    # padding\n",
    "    \n",
    "    ind_dec_order = np.argsort(length_x1_list)[::-1]\n",
    "    x1_list = np.array(x1_list)[ind_dec_order]\n",
    "    x2_list = np.array(x2_list)[ind_dec_order]\n",
    "    length_x1_list = np.array(length_x1_list)[ind_dec_order]\n",
    "    length_x2_list = np.array(length_x2_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    x1_mask_list = np.array(x1_mask_list)[ind_dec_order].reshape(len(batch),-1,1)\n",
    "    x2_mask_list = np.array(x2_mask_list)[ind_dec_order].reshape(len(batch),-1,1)\n",
    "    #print(x1_mask_list)\n",
    "    #print(x1_mask_list.shape)\n",
    "    return [torch.from_numpy(x1_list), torch.from_numpy(x2_list), length_x1_list, length_x2_list, torch.from_numpy(x1_mask_list).float(), torch.from_numpy(x2_mask_list).float(), torch.from_numpy(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset(train_data, token2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_data, token2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# test_dataset = SNLIDataset(test_data, token2id)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=snli_collate_func,\n",
    "#                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_mat = np.zeros((len(token2id),300))\n",
    "for i in range(2,len(emb_weights)):\n",
    "    weights_mat[i] = emb_weights[id2token[i]]\n",
    "\n",
    "weights_mat[1] = np.random.randn(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vocab_size,weights, bidirectional = True):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        # emb_weights = pretrained embedding weights\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(300,hidden_size, num_layers, batch_first=True,bidirectional=bidirectional)\n",
    "        self.gru = nn.GRU(300,hidden_size, num_layers, batch_first=True,bidirectional=bidirectional)\n",
    "        self.num_directions = 1 if not bidirectional else 2\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "\n",
    "    def init_hidden_gru(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        return hidden\n",
    "    \n",
    "    def init_hidden_lstm(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_directions*num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        c_0 = torch.randn(self.num_directions*self.num_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        return hidden, c_0\n",
    "\n",
    "    def forward(self, x, lengths,masks):\n",
    "        # reset hidden state\n",
    "        \n",
    "        true2sorted = sorted(range(len(lengths)), key=lambda i: -lengths[i])\n",
    "        sorted2true = sorted(range(len(lengths)), key=lambda i: true2sorted[i])\n",
    "        #enc_input = torch.stack([enc_input[i, :] for i in true2sorted], dim=1)\n",
    "        x = x[true2sorted]\n",
    "        lengths = lengths[true2sorted]\n",
    "\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        #self.hidden, self.c = self.init_hidden_lstm(batch_size)\n",
    "        self.hidden = self.init_hidden_gru(batch_size)\n",
    "    \n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(x)\n",
    "        #mask out all others except <UNK> token to freeze their weights\n",
    "        #print(embed)\n",
    "        #print(embed.size())\n",
    "        #print(masks)\n",
    "        #print(masks.size())\n",
    "        embed = masks*embed + (1-masks)*embed.clone().detach()\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        # fprop though RNN\n",
    "        #rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "        \n",
    "        #rnn_out, (self.hidden, self.c) = self.lstm(embed, (self.hidden,self.c))\n",
    "        rnn_out,self.hidden = self.gru(embed,self.hidden)\n",
    "        # undo packing\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        rnn_out = rnn_out.view(batch_size,-1,self.num_directions,self.hidden_size)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out = torch.sum(rnn_out, dim=1)\n",
    "        #concat both directions\n",
    "        if(self.num_directions == 2):\n",
    "            out_concat = torch.cat((rnn_out[:,-1,:],rnn_out[:,-2,:]),dim=1)\n",
    "        else:\n",
    "            out_concat = torch.cat((rnn_out[:,-1,:]),dim=1)\n",
    "        #print(out_concat.size())\n",
    "        out_concat = out_concat[sorted2true]\n",
    "        return out_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size, num_outputs,num_directions=2,interact_type = 'concat'):\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "        # Fully connected and ReLU layers\n",
    "        if(interact_type == 'concat'):\n",
    "            self.fc1 = nn.Linear(num_directions*num_inputs*2, hidden_size)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(num_directions*num_inputs, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        self.type = interact_type\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, embedding_output1, embedding_output2):\n",
    "        if(self.type == 'concat'):\n",
    "            input = torch.cat((embedding_output1,embedding_output2),dim=1)\n",
    "        elif(self.type == 'mul'):\n",
    "            input = embedding_output1 * embedding_output2\n",
    "        \n",
    "        input = input.view(input.size(0), -1) # Reshape input to batch_size x num_inputs\n",
    "        output = self.fc1(input)\n",
    "        output = self.relu(output)\n",
    "        #output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.uniform_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:3' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 30.5\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 35.2\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 38.2\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 34.9\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 36.0\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 35.5\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 39.6\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 34.2\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 37.6\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 35.8\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 39.1\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 40.7\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 38.3\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 39.8\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 39.2\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 39.6\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 40.8\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 40.9\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 42.2\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 41.9\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 43.3\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 41.7\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 43.2\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 44.5\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 41.7\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 43.7\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 43.7\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 45.2\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 44.0\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 46.5\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 46.6\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 47.1\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 43.7\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 47.0\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 46.2\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 47.5\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 45.2\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 48.9\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 44.8\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 47.2\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 45.0\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 48.0\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 48.2\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 49.2\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 49.2\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 48.2\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 49.3\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 49.4\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 48.7\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 48.7\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 51.0\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 49.4\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 50.0\n",
      "Epoch: [2/5], Step: [2301/3125], Validation Acc: 50.5\n",
      "Epoch: [2/5], Step: [2401/3125], Validation Acc: 52.1\n",
      "Epoch: [2/5], Step: [2501/3125], Validation Acc: 49.2\n",
      "Epoch: [2/5], Step: [2601/3125], Validation Acc: 51.9\n",
      "Epoch: [2/5], Step: [2701/3125], Validation Acc: 49.3\n",
      "Epoch: [2/5], Step: [2801/3125], Validation Acc: 49.9\n",
      "Epoch: [2/5], Step: [2901/3125], Validation Acc: 50.5\n",
      "Epoch: [2/5], Step: [3001/3125], Validation Acc: 50.0\n",
      "Epoch: [2/5], Step: [3101/3125], Validation Acc: 54.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8cf5fbdd9654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutputs_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moutputs_x2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_x2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_x1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs_x2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-69fbe4ace3fb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths, masks)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# pack padded sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#rnn_out, self.hidden = self.rnn(embed, self.hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# fast pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmight_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_tensors_permissive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackPadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/nn/_functions/packing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, lengths, batch_first)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mc_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprev_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model(loader, model,classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x1,x2,length_x1,length_x2,x1_mask,x2_mask,label in loader:\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = F.softmax(classification_network(outputs_x1,outputs_x2),dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(hidden_size=RNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat, bidirectional = True).to(DEVICE)\n",
    "classification_network = ClassificationNetwork(num_inputs=RNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=2,interact_type='mul').to(DEVICE)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = classification_network(outputs_x1,outputs_x2)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_model(test_loader, model,classification_network=classification_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vocab_size,weights,kernel_size = 3):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv1d(300, hidden_size, kernel_size=self.kernel_size, padding=1, bias = True)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=self.kernel_size, padding=1, bias = True)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        #self._init_weights()\n",
    "\n",
    "    def forward(self, x, lengths,masks):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        embed = masks*embed + (1-masks)*embed.clone().detach()\n",
    "        embed = embed.transpose(1,2)\n",
    "        #hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        #hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))      \n",
    "#         hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "#         hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "        #changed to work like a normal conv layer where the final hidden size is:(batch_size,num_filters,output_size)\n",
    "        #and finally maxpooling over the final output size\n",
    "        hidden = self.conv1(embed)\n",
    "        #hidden = self.bn1(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.conv2(hidden)\n",
    "        #hidden = self.bn2(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        #print(hidden.size())\n",
    "        hidden = torch.max(hidden, dim=2)\n",
    "        #print(hidden[0].size())\n",
    "        #logits = self.linear(hidden)\n",
    "        return hidden[0]\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.uniform_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.uniform_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 42.6\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 55.2\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d8de9b9ce795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_loss_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone_project/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model(loader, model,classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x1,x2,length_x1,length_x2,x1_mask,x2_mask,label in loader:\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = F.softmax(classification_network(outputs_x1,outputs_x2),dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "CNN_HIDDEN_SIZE = 512\n",
    "\n",
    "model = CNN(hidden_size=CNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat).to(DEVICE)\n",
    "classification_network = ClassificationNetwork(num_inputs=CNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=1,interact_type='concat').to(DEVICE)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        print(outputs_x1.size())\n",
    "        outputs = classification_network(outputs_x1,outputs_x2)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_hist.append(loss.item())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "            val_acc_hist.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            # validate\n",
    "    \n",
    "            \n",
    "\n",
    "val_acc_hist = np.array(val_acc_hist)\n",
    "max_val_acc = np.max(val_acc_hist)\n",
    "max_val_acc_epoch = np.argmax(val_acc_hist)\n",
    "print(max_val_acc)\n",
    "print(max_val_acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_HIDDEN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the original CNN models presented in CNN for text classification paper\n",
    "#try to do multiple kernels with different kernel sizes and concat their output(results in using just one CNN layer)\n",
    "#inspired from allennlp repo for seq2vecencoder\n",
    "class MultipleCNN(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vocab_size,weights):\n",
    "\n",
    "        super(MultipleCNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "        self.filter_sizes = [2,3,4,5]\n",
    "#         self.conv_filter_1 = nn.Conv1d(300, hidden_size, kernel_size=2)\n",
    "#         self.conv_filter_2 = nn.Conv1d(300, hidden_size, kernel_size=3)\n",
    "#         self.conv_filter_3 = nn.Conv1d(300, hidden_size, kernel_size=2)\n",
    "#         self.conv_filter_4 = nn.Conv1d(300, hidden_size, kernel_size=3)\n",
    "\n",
    "        self.conv_layers = [nn.Conv1d(300, hidden_size, kernel_size=i, padding=1) for i in self.filter_sizes]\n",
    "        \n",
    "        #add modules for later using it as list rather than adding each one individually \n",
    "        \n",
    "        for i,conv_layer in enumerate(self.conv_layers):\n",
    "            self.add_module('conv_layer_{}'.format(i),conv_layer)\n",
    "        #self.conv1 = nn.Conv1d(300, hidden_size, kernel_size=3, padding=1)\n",
    "        #self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths,masks):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        embed = masks*embed + (1-masks)*embed.clone().detach()\n",
    "        embed = embed.transpose(1,2)\n",
    "        #hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        #hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))      \n",
    "#         hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "#         hidden = self.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "        #changed to work like a normal conv layer where the final hidden size is:(batch_size,num_filters,output_size)\n",
    "        #and finally maxpooling over the final output size\n",
    "        outputs_conv = []\n",
    "        for i in range(len(self.filter_sizes)):\n",
    "            conv_layer = getattr(self,'conv_layer_{}'.format(i))\n",
    "            outputs_conv.append(self.relu(conv_layer(embed)).max(dim=2)[0])\n",
    "        \n",
    "        final_out = torch.cat(outputs_conv,dim=1)\n",
    "        #print(final_out.size())\n",
    "        #print(hidden[0].size())\n",
    "        #logits = self.linear(hidden)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.1229647397994995\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 46.2\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0760072469711304\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 55.0\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 0.8719912767410278\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 56.2\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 0.8041960000991821\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 55.8\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.7246642112731934\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.7938550114631653\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 0.9209326505661011\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 56.7\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.7432420253753662\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.6338348388671875\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 59.6\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.8503522872924805\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.5809110403060913\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 59.6\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.6593281626701355\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 61.4\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.7747929096221924\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.9032117128372192\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 62.1\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.8886633515357971\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.9588658809661865\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 62.2\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.6613653898239136\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.945167601108551\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.7552067637443542\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.8723558187484741\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 63.3\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.7620833516120911\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7993824481964111\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.8502479791641235\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 63.6\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.7893892526626587\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.7425559163093567\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.7316412925720215\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 63.7\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.9644593596458435\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.9237822890281677\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 63.1\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.7815278172492981\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 64.3\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.9578350782394409\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 64.0\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8179216980934143\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.613319993019104\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.6950972676277161\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.5948003530502319\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.8792072534561157\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 65.5\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.8229225873947144\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.7753905653953552\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.6697649955749512\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 66.0\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.6461910009384155\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.7723643183708191\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.7280547022819519\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.6133120059967041\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.7480353713035583\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.8030463457107544\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.48144933581352234\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 66.3\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.6062608361244202\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 64.9\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.983666181564331\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.7934601902961731\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.7339872717857361\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.7060982584953308\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.5912999510765076\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.9056807160377502\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 66.6\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.5691096782684326\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.8938186764717102\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.7507861852645874\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 66.7\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.5471530556678772\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.5051157474517822\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 66.7\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.6604372262954712\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 66.0\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.4796200394630432\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.8380677103996277\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.7581383585929871\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 67.3\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.7519215941429138\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.6902477741241455\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 67.3\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.6093656420707703\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.6013224124908447\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 66.7\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.6716948747634888\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.49711668491363525\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 65.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.463113397359848\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.5352632999420166\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6947352886199951\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.4957398772239685\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.8345785737037659\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.5729821920394897\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.5527711510658264\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.4693891108036041\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.7225181460380554\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.7577152848243713\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 65.8\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.6247058510780334\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.7911638617515564\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 66.3\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.7398590445518494\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.7500359416007996\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6375779509544373\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.4075009524822235\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.6924338936805725\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 68.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.7206146121025085\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 68.7\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.5726945400238037\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.5677512288093567\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.7780652046203613\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 68.4\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.6348868608474731\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.5871150493621826\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 68.4\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.6204308867454529\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.6395386457443237\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 69.0\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.6517505645751953\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 68.1\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.29273539781570435\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 68.1\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.4048984944820404\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 68.7\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.28332796692848206\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.3942584693431854\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.3294445872306824\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.4355887770652771\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.3671616315841675\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.5488413572311401\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 67.1\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.5117095112800598\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.5201221108436584\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.5028718113899231\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 67.2\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.23388642072677612\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.6828387379646301\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 67.1\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.5751770734786987\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 67.9\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.5685839056968689\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.4554142355918884\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.4788503050804138\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.5618876218795776\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.37529173493385315\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.50251704454422\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 69.5\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.663273274898529\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 67.2\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.584254264831543\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 68.0\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.5023801326751709\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.4105278551578522\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.5743024945259094\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 69.6\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.4225016236305237\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 69.1\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.38812536001205444\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6179146766662598\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.3627351224422455\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.3025520145893097\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 67.9\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.4242924451828003\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.37543514370918274\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.25536447763442993\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.5719862580299377\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.5100071430206299\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.17730984091758728\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.28312018513679504\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.32049161195755005\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.918645977973938\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 66.4\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.41576793789863586\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.23349550366401672\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 66.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.4246789813041687\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.09909610450267792\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 67.5\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.30465900897979736\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 67.9\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.246515691280365\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.31940966844558716\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.2033720165491104\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.5279890298843384\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 66.4\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.5281568765640259\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 67.7\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.2871999740600586\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.39706915616989136\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.49342644214630127\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 65.7\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.45930200815200806\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.4769119620323181\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.6068318486213684\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.2311851680278778\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.38038188219070435\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 66.3\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.5080414414405823\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 66.9\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.5370166301727295\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.3774678111076355\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.4233461320400238\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.45387592911720276\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.23718595504760742\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.28774988651275635\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.3752644658088684\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.29145172238349915\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.18953591585159302\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.09892476350069046\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.22589603066444397\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 67.1\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.18539893627166748\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 68.7\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.18955686688423157\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.20255649089813232\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.15467219054698944\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.26317664980888367\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 67.0\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.09106645733118057\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.23344646394252777\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.27397388219833374\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.24583405256271362\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.2882561683654785\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.3059486448764801\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.34107905626296997\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.36863094568252563\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 66.4\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.2462102770805359\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.42211297154426575\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.2770702540874481\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 68.4\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.16079865396022797\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.3606470227241516\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.20246398448944092\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 67.1\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.2988263964653015\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.3584638237953186\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.45338964462280273\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.19433343410491943\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.17384298145771027\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.1260475367307663\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.1258479356765747\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.09295777976512909\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.11155080795288086\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.19027748703956604\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.3704431653022766\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.23293286561965942\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.1396249532699585\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 65.4\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.18819621205329895\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 66.9\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.03340165689587593\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.23583444952964783\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.10586156696081161\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.06437419354915619\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.17465531826019287\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.14582329988479614\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 66.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.08262232691049576\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.2034592628479004\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.22055676579475403\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.2956238090991974\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.33237606287002563\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.21481171250343323\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.3193310499191284\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.17313474416732788\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.2074277400970459\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.34341156482696533\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.21720342338085175\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.14993727207183838\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 65.7\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.24992017447948456\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.15273109078407288\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 66.5\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.364164799451828\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 68.8\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.24960315227508545\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.15046879649162292\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.056417010724544525\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 65.8\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.18418154120445251\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.03940697759389877\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.10616188496351242\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.16340431571006775\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 66.1\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.18145626783370972\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 66.1\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.019998379051685333\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.20283783972263336\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.0505950003862381\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.05269494652748108\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.17675386369228363\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.15779981017112732\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.09688467532396317\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.13828147947788239\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.26872938871383667\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.2374666929244995\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.22698350250720978\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.07005001604557037\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 69.1\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.06367358565330505\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.3860307037830353\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.13229046761989594\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.17344292998313904\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.14572422206401825\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.2862861156463623\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 68.6\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.272918701171875\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.12085475027561188\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.2140900045633316\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.1398342400789261\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 67.2\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.20297512412071228\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 67.2\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.10650545358657837\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.12681461870670319\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.13267575204372406\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.07935861498117447\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.05713697522878647\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 66.0\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.09151817858219147\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.11422084271907806\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 69.2\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.11385399848222733\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 66.0\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.05201934278011322\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.07870005816221237\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.05884561687707901\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.2862871289253235\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.1975686252117157\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.07532136887311935\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.18243154883384705\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 69.0\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.0842389464378357\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 65.5\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.0674373060464859\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.13565979897975922\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 67.8\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.2231810986995697\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.18777799606323242\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.09847696870565414\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 67.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.1275109350681305\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.07499410212039948\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.21343280375003815\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.08270110934972763\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 67.5\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.03841625154018402\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.061952102929353714\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.20915617048740387\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.08882326632738113\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 66.1\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.23034372925758362\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.35986727476119995\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.09243535250425339\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.06188366189599037\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.10878275334835052\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.03883139789104462\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.0995892733335495\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.05436449870467186\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.10401222854852676\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.2601293921470642\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.15806648135185242\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 68.6\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.11794421076774597\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.07130952179431915\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.14231449365615845\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.07183286547660828\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 67.9\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.04828879237174988\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.10896027088165283\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.17397601902484894\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.04402175545692444\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.06226540356874466\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 67.9\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.10733053833246231\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.41782844066619873\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 68.7\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.2292594164609909\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.18210241198539734\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 69.0\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.5773420333862305\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.07300286740064621\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.1769295334815979\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 69.1\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.13276134431362152\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.10921923071146011\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.07981418073177338\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.013175003230571747\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.12659317255020142\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.12302246689796448\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.05684376507997513\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 67.7\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model,classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x1,x2,length_x1,length_x2,x1_mask,x2_mask,label in loader:\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = F.softmax(classification_network(outputs_x1,outputs_x2),dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "CNN_HIDDEN_SIZE = 512\n",
    "model = MultipleCNN(hidden_size=CNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat).to(DEVICE)\n",
    "classification_network = ClassificationNetwork(num_inputs=4*CNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=1).to(DEVICE)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters())+list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss_hist = []\n",
    "val_acc_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = classification_network(outputs_x1,outputs_x2)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_hist.append(loss.item())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(epoch+1,num_epochs,i+1,len(train_loader),loss.item()))\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            val_acc_hist.append(val_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.6\n"
     ]
    }
   ],
   "source": [
    "max_val_acc = np.max(val_acc_hist)\n",
    "max_val_acc_epoch = np.argmax(val_acc_hist)\n",
    "print(max_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(train_loss_hist,open('train_hist_mult_cnn.pkl','wb'))\n",
    "pkl.dump(val_acc_hist,open('val_hist_mult_cnn.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_HIDDEN_SIZES = [256,512]\n",
    "INTERACT_TYPES = ['concat','mul']\n",
    "KERNEL_SIZES = [3,5]\n",
    "somelists = [CNN_HIDDEN_SIZES,KERNEL_SIZES,INTERACT_TYPES]\n",
    "\n",
    "result = list(itertools.product(*somelists))\n",
    "df_param = pd.DataFrame(result,columns=['hidden_size','kernel_size','interaction_type'])\n",
    "df_param['train_loss_hist'] = None\n",
    "df_param['val_acc_hist'] = None\n",
    "df_param['max_val_acc'] = None\n",
    "df_param['max_val_acc_epoch'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>train_loss_hist</th>\n",
       "      <th>val_acc_hist</th>\n",
       "      <th>max_val_acc</th>\n",
       "      <th>max_val_acc_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>concat</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>mul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>mul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>concat</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>mul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>concat</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>mul</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_size  kernel_size interaction_type train_loss_hist val_acc_hist  \\\n",
       "0          256            3           concat            None         None   \n",
       "1          256            3              mul            None         None   \n",
       "2          256            5           concat            None         None   \n",
       "3          256            5              mul            None         None   \n",
       "4          512            3           concat            None         None   \n",
       "5          512            3              mul            None         None   \n",
       "6          512            5           concat            None         None   \n",
       "7          512            5              mul            None         None   \n",
       "\n",
       "  max_val_acc max_val_acc_epoch  \n",
       "0        None              None  \n",
       "1        None              None  \n",
       "2        None              None  \n",
       "3        None              None  \n",
       "4        None              None  \n",
       "5        None              None  \n",
       "6        None              None  \n",
       "7        None              None  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size             200\n",
      "kernel_size               3\n",
      "interaction_type     concat\n",
      "train_loss_hist        None\n",
      "val_acc_hist           None\n",
      "max_val_acc            None\n",
      "max_val_acc_epoch      None\n",
      "Name: 0, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.0734792947769165\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 44.5\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0744177103042603\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 55.0\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.028780221939087\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 54.5\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 0.6995874643325806\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 56.7\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.8842617869377136\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 58.5\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 1.0650224685668945\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 0.7182438969612122\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.8766521215438843\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 59.2\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.8552703261375427\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.8866839408874512\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 60.3\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.9978044033050537\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.9057486653327942\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.9241368174552917\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 1.0236586332321167\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.6885021924972534\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 62.5\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.7950941324234009\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 63.8\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.8604922890663147\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.8459550142288208\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 61.3\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.6901282072067261\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 63.3\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 1.0312082767486572\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 63.7\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.8473195433616638\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 63.7\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7830649614334106\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.8002837896347046\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.6714153289794922\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.8326343297958374\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.669847846031189\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 63.1\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.5258114337921143\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.937425434589386\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.7435988783836365\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 63.0\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.6928182244300842\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8225973844528198\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 64.5\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.7547205686569214\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.7487876415252686\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.7990966439247131\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.5908620357513428\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 65.9\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.7420467138290405\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.6117141842842102\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 65.6\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.6101034879684448\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.7909766435623169\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.5032567977905273\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 64.8\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.7654930949211121\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.6327580809593201\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.7554627656936646\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 66.0\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.8094354867935181\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.7710216045379639\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 65.7\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.6450251936912537\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.8546708226203918\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.6823869347572327\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.9014859199523926\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.5971546173095703\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 66.3\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.6936812996864319\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 65.5\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.884172797203064\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.686617374420166\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 66.9\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.7752177119255066\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.4893547594547272\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 67.5\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.9924617409706116\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 66.9\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.9358333349227905\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.86592036485672\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 66.7\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.7929452061653137\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 67.2\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.6529417037963867\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.6281377673149109\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 66.8\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.6257331371307373\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 68.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.7298415899276733\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.7901319265365601\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 68.0\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.658893883228302\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 65.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [401/3125], Training Loss: 1.0571174621582031\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.6541440486907959\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.6137204766273499\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 68.2\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.59590744972229\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6771609783172607\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.9394620060920715\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 67.6\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.6120421290397644\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.8885984420776367\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.46833255887031555\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.6515992879867554\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.7749658823013306\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.6969444155693054\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.6302129626274109\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.5807496309280396\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.8905945420265198\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.700658917427063\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.5660606622695923\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.7691878080368042\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.6644910573959351\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.5420739650726318\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 68.5\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.6086549758911133\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 68.3\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.6022636890411377\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.6524213552474976\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 69.1\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.7829639911651611\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 68.7\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.6282401084899902\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.6930240988731384\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.8056950569152832\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.7606805562973022\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.7360429167747498\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.6222528219223022\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 67.9\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.6965970993041992\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 67.2\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.5006725788116455\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.597050130367279\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.5985608100891113\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 68.1\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.7012418508529663\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.6296736001968384\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.6729625463485718\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.606194019317627\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.4723691940307617\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 68.0\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.6865793466567993\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.47356635332107544\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.4899154305458069\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.5952599048614502\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 68.1\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.6110783815383911\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.5728296637535095\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.5613308548927307\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 68.6\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.47049611806869507\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.6044203042984009\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 68.0\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.6897091865539551\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.705531120300293\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.6835379600524902\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.6060961484909058\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 68.0\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.7529932856559753\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.5533140897750854\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.4868021607398987\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.696300208568573\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.7938909530639648\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 68.9\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.49910587072372437\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.7695510983467102\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 69.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 69.5\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.5059671401977539\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.49618011713027954\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.6091200113296509\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 66.8\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.5039467811584473\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 67.3\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.7884677648544312\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.4210461974143982\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 66.3\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.8708559274673462\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.5279185771942139\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 67.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.6541707515716553\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 68.9\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.46971943974494934\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.795462429523468\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 68.5\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.5756881833076477\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.37578344345092773\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.4424251914024353\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 69.5\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.44197434186935425\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.45086583495140076\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.565214991569519\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.4734792113304138\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.4602852463722229\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 66.6\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.7763929963111877\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.7756862640380859\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 70.0\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.9756897687911987\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 70.6\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.5131137371063232\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 69.9\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.5423461198806763\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 69.4\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.5137547254562378\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.7334650158882141\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 69.3\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.48206913471221924\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.385952353477478\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 70.0\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.5304664373397827\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 70.1\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.5120402574539185\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 68.6\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.63228839635849\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 69.3\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 69.7\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.3975491523742676\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 69.1\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.3726138770580292\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.41768476366996765\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.4367743134498596\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.42494475841522217\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.5373077392578125\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 68.0\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.4328283369541168\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.43760937452316284\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.41529378294944763\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 69.3\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.5065181255340576\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.34134209156036377\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 68.4\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.2675873041152954\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.416759729385376\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 70.1\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.3797779381275177\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 69.8\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.5998102426528931\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 69.9\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.48228055238723755\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.6283195614814758\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 69.7\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.6695449352264404\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 69.2\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.4947190284729004\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.3486327528953552\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.47979360818862915\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 70.1\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.5228525400161743\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.5622131824493408\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 68.3\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.32441389560699463\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.41403019428253174\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 69.2\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.6064326763153076\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.4788106083869934\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 68.3\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.5109502077102661\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 69.1\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.6662142276763916\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 69.2\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.6253162026405334\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 69.3\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5195208191871643\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 69.8\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.36509203910827637\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 69.6\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.5888316631317139\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 69.8\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.4930683374404907\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.17917805910110474\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.397596538066864\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.46119335293769836\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 69.9\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.39058917760849\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.239759624004364\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.2772442102432251\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.2559622824192047\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.5734505653381348\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.29927343130111694\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 69.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.47891440987586975\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.19726520776748657\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.24004001915454865\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.42772766947746277\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.3925114870071411\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.44506099820137024\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 65.7\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.5493829846382141\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.5653658509254456\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.35142970085144043\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.4087851643562317\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.45073458552360535\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.5759027004241943\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.3332900106906891\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.48887068033218384\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.5724969506263733\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.5162667036056519\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 65.9\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.5786461234092712\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.5568773150444031\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 69.5\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.5543637871742249\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.29795703291893005\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.1972198784351349\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 68.3\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.17328530550003052\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.37858378887176514\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.38056713342666626\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 69.5\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.13991431891918182\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 68.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.2996784448623657\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.3637646436691284\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.21578669548034668\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.22963060438632965\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 65.5\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.7006509304046631\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.44454750418663025\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.4673956632614136\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.5841926336288452\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.23178620636463165\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.36369678378105164\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.2964642345905304\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.2721511125564575\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 68.6\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.4050940275192261\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.2770702838897705\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.3611468970775604\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 69.0\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.3657439947128296\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 68.3\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.4586772620677948\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.27255144715309143\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.20444348454475403\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.24212586879730225\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.35261547565460205\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.29242876172065735\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.3637385964393616\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.4292909502983093\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.22795668244361877\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.15847782790660858\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.3542504608631134\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 67.8\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.3273780047893524\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.27905261516571045\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 68.7\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.40959620475769043\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.27510854601860046\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.4906349182128906\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.11419931799173355\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.3224940299987793\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.24972139298915863\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.45363569259643555\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 66.1\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.12517783045768738\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.48770907521247864\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.42450714111328125\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.34380292892456055\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.2920873463153839\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 67.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.23722928762435913\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.31583282351493835\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.2600606679916382\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 67.8\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.3725855350494385\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.2850981056690216\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.270473450422287\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.2543026804924011\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.22653305530548096\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.367206335067749\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.2707855701446533\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.2555030584335327\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 66.5\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.2760698199272156\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.38299068808555603\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.29634106159210205\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 68.7\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.48348480463027954\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 68.6\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.10946819186210632\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.18318098783493042\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.15686917304992676\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.5751510858535767\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.2111859917640686\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.3948158025741577\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.2855687141418457\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.13700643181800842\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.32112357020378113\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.2542515695095062\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.4161565899848938\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 68.4\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.31846681237220764\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.1383925974369049\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.5330126881599426\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 68.7\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.25326642394065857\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 66.2\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.27094602584838867\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 68.3\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.31221088767051697\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.416013240814209\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 68.5\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.2687358856201172\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 68.5\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.35531336069107056\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 68.1\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.2890339493751526\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 68.5\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.3290454149246216\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.43270212411880493\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 68.3\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.2059749811887741\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 68.5\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.28206804394721985\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 67.3\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.27697721123695374\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.292079359292984\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.45237046480178833\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 66.2\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.26462212204933167\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 65.8\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.2890547215938568\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 67.5\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.19191698729991913\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 68.7\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 68.4\n",
      "70.6\n",
      "149\n",
      "hidden_size           200\n",
      "kernel_size             3\n",
      "interaction_type      mul\n",
      "train_loss_hist      None\n",
      "val_acc_hist         None\n",
      "max_val_acc          None\n",
      "max_val_acc_epoch    None\n",
      "Name: 1, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.1076890230178833\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 34.7\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.113966703414917\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 41.4\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.056931734085083\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 41.1\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.0577938556671143\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 44.5\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 1.1102081537246704\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 46.9\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.9277237057685852\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 51.1\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 0.9805027842521667\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 1.0077968835830688\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 53.1\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 1.006125569343567\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 52.3\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 1.0674843788146973\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 54.1\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.8856599926948547\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 54.4\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.9373573064804077\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.9893832206726074\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 53.5\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.8707185983657837\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 54.8\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.9502301216125488\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 56.5\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.9790518283843994\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.9014009833335876\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 54.6\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.6731010675430298\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 56.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.8402913212776184\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 56.4\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.898898184299469\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 55.8\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.8270833492279053\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 57.9\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.8428290486335754\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.9624852538108826\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.8046665191650391\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 57.5\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.935598611831665\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 59.6\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.8730244636535645\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 58.4\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.8750064373016357\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 58.9\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.975892186164856\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 58.8\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.8198681473731995\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.7428202033042908\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 60.1\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.798401951789856\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 58.4\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 60.0\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.7414340376853943\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 61.1\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.885176956653595\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 59.9\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 1.0625022649765015\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 61.2\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.8575698137283325\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.7207006216049194\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.7903347015380859\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 61.1\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 1.0403189659118652\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.6782324314117432\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 59.7\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.7345514893531799\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 60.7\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.7357726693153381\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 59.3\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 1.1146249771118164\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 60.4\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.9045465588569641\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 60.0\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.8024817109107971\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 60.8\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.594238817691803\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.8660901784896851\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.7345672249794006\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 60.6\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.9449561834335327\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 61.0\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.8230888247489929\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.9128596186637878\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.8387337923049927\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 61.8\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.8774983882904053\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 60.7\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.6907299160957336\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.8246634602546692\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.8167603015899658\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.906741738319397\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 60.3\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.88249272108078\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.8137888312339783\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.6151109337806702\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.7571715712547302\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.7661532163619995\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.6867253184318542\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.0\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.6989555358886719\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 63.7\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.5877013802528381\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 62.2\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.9225051403045654\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 60.8\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.5718435049057007\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.7418518662452698\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 63.1\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.7899484038352966\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.6994684934616089\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.7515138983726501\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 62.0\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.655333936214447\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.9167326092720032\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.7801848649978638\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 63.0\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.6699467301368713\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.6739827394485474\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.7893490791320801\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 65.8\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.8730303645133972\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.7145315408706665\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.9554557800292969\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 63.7\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 1.0107346773147583\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 63.7\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.5215035080909729\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6433280110359192\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.7159889936447144\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.6972528100013733\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 60.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.8443467020988464\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 65.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.7729149460792542\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.6847625970840454\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.681370735168457\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.5371304750442505\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 64.2\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.8388192057609558\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.6973751783370972\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.8275224566459656\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.7040328979492188\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.7751612663269043\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.7152523994445801\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.5670182108879089\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.7513096928596497\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.7947994470596313\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.6007837057113647\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.7467520236968994\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.8423468470573425\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.5306534767150879\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.7734718322753906\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.8259930610656738\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.6750558614730835\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.4827093780040741\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.7391824722290039\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.6052567958831787\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.6761540174484253\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.6508703231811523\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.44962278008461\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.9235900640487671\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.5372216701507568\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.6722390651702881\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.7157707214355469\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.8958961963653564\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.641782283782959\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.7458934783935547\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.7736843824386597\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.8544684648513794\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 65.2\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6310379505157471\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.6999893188476562\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.5986812710762024\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.6355949640274048\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.3055399954319\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.5085312128067017\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.37574124336242676\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.6261427402496338\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.3575911223888397\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.7858907580375671\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.6009442806243896\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.5407711863517761\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.5824187994003296\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.55415278673172\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.6048034429550171\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 67.4\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.5760558843612671\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.4849649667739868\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.612012505531311\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.599758505821228\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 67.4\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.9142408967018127\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.672791063785553\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.7693760395050049\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 66.9\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.7454068660736084\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.4901188611984253\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.7711705565452576\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 66.6\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.446249783039093\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 65.7\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.5144689083099365\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.5735867023468018\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.6126412749290466\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.8256091475486755\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.6300501823425293\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 67.3\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.4242568612098694\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 67.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.5706024765968323\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.8212899565696716\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.7624106407165527\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.7318341135978699\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.513002872467041\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.9127899408340454\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 65.0\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.3818405866622925\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 62.8\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.7695192694664001\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.6427240967750549\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.5874530673027039\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.5636467337608337\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.6727175116539001\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.531101405620575\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.8266956210136414\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 67.1\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.49051615595817566\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.5130114555358887\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 66.4\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.47286444902420044\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.6152897477149963\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.46154332160949707\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.5070805549621582\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.6326177716255188\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.38657423853874207\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.8868991732597351\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.6557856798171997\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.4258057475090027\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.6635221242904663\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 65.6\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.500385046005249\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.4382740557193756\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.7321888208389282\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.7044426798820496\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.5679603219032288\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.6274446249008179\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.4825657606124878\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5421142578125\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.4916810989379883\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 66.9\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.41074785590171814\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.42627400159835815\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.4317883849143982\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.30029651522636414\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.5947935581207275\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.5035300254821777\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.48692041635513306\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 65.7\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.6300729513168335\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.85594242811203\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.4123387932777405\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.40515103936195374\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.26597440242767334\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.37906938791275024\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.5216114521026611\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 65.6\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.6222959756851196\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.4475427269935608\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.5379961133003235\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.3526075780391693\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 64.8\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.37513238191604614\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.3449397683143616\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 64.9\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.4449484348297119\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.5364947319030762\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.6977434158325195\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.41501694917678833\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.4252074360847473\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.8115708827972412\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 68.4\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.6381222605705261\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.31772804260253906\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.6366685628890991\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.5797854065895081\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.365633100271225\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 68.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.44973284006118774\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.5507070422172546\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.25497984886169434\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 65.7\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.4188198447227478\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 65.4\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.34905344247817993\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.4267171025276184\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.36455899477005005\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.3178556263446808\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 65.8\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.356930136680603\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 65.5\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.6840323209762573\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.40656501054763794\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.5677184462547302\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.30662834644317627\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.4930934011936188\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.5461193323135376\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.3205743432044983\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.5316357612609863\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.4468246102333069\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.5006620287895203\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.536626935005188\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.5191470980644226\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.3080487847328186\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 66.1\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.35067734122276306\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.32239213585853577\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 66.1\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.3937085270881653\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.6257083415985107\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 66.4\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.5666863918304443\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 64.9\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.5080603957176208\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.5249450206756592\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.49341973662376404\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 67.1\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.41294246912002563\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 65.7\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.38147109746932983\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 63.9\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.27731242775917053\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.25592875480651855\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 66.2\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.48878926038742065\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 64.0\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.3356024920940399\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.33648258447647095\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.289202481508255\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.48069795966148376\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 65.6\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.3650714159011841\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.6825342178344727\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 64.4\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.17323236167430878\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.1855783462524414\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.4392778277397156\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 65.7\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.27182480692863464\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 65.6\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.1960887908935547\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.4962695837020874\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 67.3\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.3960321843624115\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.4941693842411041\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.5549433827400208\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 65.4\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.5500873327255249\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.33795011043548584\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.2515682578086853\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 67.3\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.40004032850265503\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 67.3\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.39066213369369507\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.4000290036201477\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.3049283027648926\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.4303518533706665\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 65.6\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.8192158341407776\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.3585807681083679\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.40537068247795105\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 68.3\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.2824113667011261\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.19449567794799805\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.21661414206027985\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.10480552166700363\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 66.5\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.3029020130634308\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 65.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.4906935393810272\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 67.9\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.3007795214653015\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.2787054777145386\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 66.1\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.21237266063690186\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.36229151487350464\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.30740463733673096\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 65.2\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.22371764481067657\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.4700711965560913\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.4062880575656891\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 67.9\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.545780599117279\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.3994371294975281\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.23995986580848694\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 63.7\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.2524116039276123\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 65.6\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.3848162293434143\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.3377681374549866\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 67.5\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.46903517842292786\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.2791827321052551\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.1982782781124115\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 68.4\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.4884558618068695\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 65.5\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.24330317974090576\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 68.6\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.3862268924713135\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 65.8\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.41676080226898193\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 65.9\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.412090003490448\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 68.1\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.18074478209018707\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.16263091564178467\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 66.2\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.24683712422847748\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "68.9\n",
      "184\n",
      "hidden_size             200\n",
      "kernel_size               5\n",
      "interaction_type     concat\n",
      "train_loss_hist        None\n",
      "val_acc_hist           None\n",
      "max_val_acc            None\n",
      "max_val_acc_epoch      None\n",
      "Name: 2, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.0376193523406982\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 46.8\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 0.9739963412284851\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 0.9785653352737427\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 53.2\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.146263837814331\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.9976943731307983\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 54.7\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.9125290513038635\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 59.5\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.129968285560608\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 56.5\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.8752574920654297\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 58.7\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.8459742069244385\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 60.3\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.9242058396339417\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.8414199352264404\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 59.1\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.8809474110603333\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.8291817307472229\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 58.4\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.9171596765518188\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.7757261991500854\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 60.4\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.825153112411499\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.6882092952728271\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.6376309990882874\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.8748800158500671\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.7130526304244995\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 60.3\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.8553985953330994\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7999570965766907\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.7085791826248169\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 60.6\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.8480621576309204\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.8711105585098267\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.9060335159301758\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.8853687644004822\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 60.2\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.851557731628418\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 62.2\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.771193265914917\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 1.0528030395507812\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.7128985524177551\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 59.7\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.6792991161346436\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.7538102269172668\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 60.7\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.6816200017929077\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 61.4\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.9524203538894653\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.6957329511642456\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 63.8\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.8127371072769165\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.7694125771522522\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 64.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.8322593569755554\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 1.12204909324646\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.6552528738975525\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.8722684383392334\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 61.8\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.7358697652816772\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 63.8\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.8436897993087769\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.8538573384284973\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 63.3\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.6955330967903137\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.8839347958564758\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.7328237295150757\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.9644020795822144\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.6798154711723328\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.6951740980148315\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.6455072164535522\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.47431913018226624\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.7945264577865601\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.738500714302063\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.6718817353248596\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.8158125281333923\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.6517937779426575\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.9887887835502625\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 64.8\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.5842329859733582\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.8587219715118408\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.8383270502090454\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.7996252775192261\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.7172913551330566\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.7247968912124634\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.5178582072257996\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.6115421056747437\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.7344796657562256\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.8357240557670593\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6226076483726501\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.6981586813926697\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.6201858520507812\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.6393309831619263\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.5779129266738892\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.6000869274139404\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.6697067618370056\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.8265398740768433\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.6777855753898621\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.7492826581001282\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.6489977240562439\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.5831906199455261\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.8446476459503174\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.3\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.7560274600982666\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.6201765537261963\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.6353940367698669\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.741860032081604\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.6915559768676758\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.6766266822814941\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 64.6\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.4237608313560486\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.8541917204856873\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.651946485042572\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 66.1\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.7138282656669617\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.7973455786705017\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.6210947036743164\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.49223655462265015\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.46668344736099243\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.7008658647537231\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.5\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.5763295888900757\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.8260848522186279\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.8053191304206848\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.5396060347557068\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.7225396633148193\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.7347680926322937\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.528168797492981\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.605778694152832\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 66.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.696312665939331\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.43636414408683777\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.7810540795326233\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 67.9\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.48136189579963684\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.549876868724823\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.5363568067550659\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.5599669814109802\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.4807821810245514\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.5159656405448914\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.5711320042610168\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.8723225593566895\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.7539372444152832\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.6517698168754578\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.616370677947998\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.5086480975151062\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6885483264923096\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.4963933825492859\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 65.2\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.48840489983558655\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.7148976922035217\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.6823198795318604\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.6328101754188538\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.3601424992084503\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.5562779307365417\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.88591468334198\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.3853520154953003\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.641952633857727\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.526915967464447\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.5523059368133545\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.44263961911201477\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.6987891793251038\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.5413222312927246\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.46596798300743103\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.4351462721824646\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 66.4\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.48499682545661926\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.46296384930610657\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.5205132365226746\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.351862370967865\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.3417675495147705\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.699205219745636\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.689337432384491\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.6370377540588379\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.4960690438747406\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.49846571683883667\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.46334147453308105\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.5781049132347107\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.6014398336410522\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.5189499855041504\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.4472358524799347\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.5236133337020874\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 67.5\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.46095001697540283\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.3662448823451996\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 65.0\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.5279085636138916\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.5099251866340637\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.3479137420654297\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.38027822971343994\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 65.4\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.29911327362060547\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.34384316205978394\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.3060477077960968\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.2864379584789276\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 65.4\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.5008231401443481\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.32296472787857056\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.689117968082428\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.41946250200271606\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.39488375186920166\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.3721672296524048\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.594494104385376\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 63.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.4456534683704376\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.6322200894355774\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.8216878771781921\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.5433279871940613\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 65.0\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.49276304244995117\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.572292149066925\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.8296078443527222\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.5993982553482056\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.5423389077186584\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 61.5\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.5853443741798401\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.4235455393791199\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.4859844446182251\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.4188762903213501\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.34205782413482666\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5097110271453857\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.5\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.16369053721427917\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.32581084966659546\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.41434958577156067\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 64.8\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.3107796609401703\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 64.9\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.23464973270893097\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.26101452112197876\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.34360429644584656\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 62.3\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.29870080947875977\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.2340068370103836\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.14890959858894348\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.27855628728866577\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.4413411021232605\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.3270472586154938\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 62.9\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.46134576201438904\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.22797134518623352\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.44119885563850403\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.20059846341609955\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 63.8\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.4021068811416626\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.2110190987586975\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 62.5\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.37709006667137146\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.3689397871494293\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 62.6\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.34503114223480225\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.38029491901397705\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.313712477684021\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.5145118832588196\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.44365304708480835\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.4990858733654022\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.24697397649288177\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.3672388195991516\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 62.7\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.290761798620224\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 62.1\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.6629635095596313\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 62.3\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.5561180710792542\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 63.9\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.3110605776309967\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.10783689469099045\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.3055189251899719\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.3167962431907654\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.3524986803531647\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.09861747175455093\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.21940360963344574\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.3051634728908539\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.34990227222442627\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.24486765265464783\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.12744532525539398\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.22178304195404053\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 62.4\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.2779136598110199\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.3682584762573242\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 64.1\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.22844664752483368\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 63.4\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.23985376954078674\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 62.4\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.2757853865623474\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.2769695520401001\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.18637914955615997\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 64.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.2570982575416565\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 62.2\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.3564380407333374\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 61.6\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.6120457649230957\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.24654044210910797\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.11985526233911514\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.4723634123802185\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.28689414262771606\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.21431097388267517\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.4482901990413666\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 63.9\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.3422126770019531\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.4326450824737549\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.1314617395401001\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 62.1\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.20692184567451477\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.31584298610687256\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.17560294270515442\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.13298629224300385\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.2700403332710266\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.19009555876255035\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 64.1\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.16760361194610596\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.18792280554771423\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.05567445233464241\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 65.0\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.18773336708545685\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.2309512495994568\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 61.8\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.14735785126686096\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.36026692390441895\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.31185176968574524\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.18716830015182495\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 65.1\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.28108978271484375\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.0948210135102272\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 61.3\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.13092882931232452\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.21561723947525024\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 61.2\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.25010186433792114\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 61.3\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.1698666214942932\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 61.2\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.24825194478034973\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 61.6\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.16900989413261414\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 60.8\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.22761672735214233\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 62.5\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.38062864542007446\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.10341717302799225\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.36452239751815796\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 61.6\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.44762134552001953\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 61.0\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.3434291481971741\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.05287636071443558\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.0\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.18332040309906006\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 61.9\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.10744834691286087\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 62.2\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.06951506435871124\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.2323431372642517\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 62.8\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.03378918766975403\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.0534542053937912\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 60.4\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.26583296060562134\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 64.5\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.09451670199632645\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.07841981947422028\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 63.0\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.207502081990242\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.19204026460647583\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 60.8\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.1424180567264557\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.10874804854393005\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 63.6\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.1815348118543625\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 61.2\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.3344038426876068\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.12779711186885834\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 61.7\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.2304409146308899\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 61.3\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.18154165148735046\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 61.2\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.18852263689041138\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.5055000185966492\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.43871742486953735\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 62.2\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.4207571744918823\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.2372315376996994\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.131727933883667\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 62.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.04979032650589943\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.13754446804523468\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.315653920173645\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.282657265663147\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 61.6\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.0688466727733612\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 62.2\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.08564211428165436\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.2568601667881012\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "67.9\n",
      "110\n",
      "hidden_size           200\n",
      "kernel_size             5\n",
      "interaction_type      mul\n",
      "train_loss_hist      None\n",
      "val_acc_hist         None\n",
      "max_val_acc          None\n",
      "max_val_acc_epoch    None\n",
      "Name: 3, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.1192126274108887\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 34.3\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0659078359603882\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 38.3\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.0323406457901\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 48.0\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.1084492206573486\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 49.0\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.9109975695610046\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 48.6\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.8355240821838379\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.0931042432785034\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 51.6\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 1.0497465133666992\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 49.2\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.9654601812362671\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 50.0\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 1.028200387954712\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 1.0459959506988525\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 49.0\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.8525189161300659\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 56.0\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 1.3093527555465698\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 52.5\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.8877571225166321\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 55.5\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.8538956642150879\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 55.9\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 1.092879295349121\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 56.5\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.8909944891929626\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 56.9\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.9524365663528442\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 57.4\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 1.1142749786376953\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 55.5\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.8643614649772644\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 56.1\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.8537213206291199\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 57.8\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.8565248847007751\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 57.8\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 1.033055305480957\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.7913949489593506\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 56.2\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.8232779502868652\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.9381844997406006\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 57.4\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.7423192262649536\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 57.7\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.857506275177002\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 58.4\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.8033841848373413\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 59.3\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.9521501660346985\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 57.5\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8375968337059021\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.8737592101097107\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 60.4\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.7059544920921326\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 59.1\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.8678773045539856\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 62.2\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.7525883316993713\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 62.1\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.8073791861534119\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 61.1\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.7746105790138245\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 59.9\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.9010398983955383\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.7174782156944275\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 60.4\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.7554360032081604\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.5914249420166016\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.8167973756790161\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 59.6\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.7066069841384888\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 60.6\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.9269843697547913\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.927162230014801\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.6744094491004944\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 60.8\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.8414230346679688\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 61.4\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.6676539182662964\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 61.9\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.8655235171318054\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.8462741374969482\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.8337045311927795\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 62.2\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.865693986415863\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 58.0\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.8788062334060669\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.9174599647521973\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 60.6\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.8347165584564209\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 61.1\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.6858584880828857\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.7474287748336792\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.6901392340660095\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 61.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.7381757497787476\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 63.7\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.5293483734130859\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.7318052053451538\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 1.0729809999465942\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.6764594912528992\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.6391422152519226\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.8141990303993225\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.5270096063613892\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.722561240196228\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.6984875202178955\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.8363592624664307\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 64.2\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6608180999755859\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.5607514977455139\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.9935190677642822\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 62.3\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.7740424871444702\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.7809279561042786\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 63.4\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.8004704117774963\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.6706259846687317\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.5821805000305176\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.5750520825386047\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 61.7\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.7808099389076233\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 61.7\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.7975335717201233\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.8187257051467896\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6017225384712219\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.8217779994010925\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.615732729434967\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.9207326769828796\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.6021971702575684\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.8799057006835938\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 60.5\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.844574511051178\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.8088088631629944\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.6893600821495056\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.6886149644851685\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.7039005756378174\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 63.6\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.7443886995315552\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.7459975481033325\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.47794944047927856\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.920066773891449\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.6827110052108765\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.8219387531280518\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.7646386623382568\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.6446068286895752\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 63.5\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.5595369338989258\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.5557057857513428\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.6183087825775146\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.7934170961380005\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.5950199365615845\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.5402732491493225\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.6800528764724731\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.6303022503852844\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.5497618913650513\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.7755073308944702\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.6140071153640747\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.822149932384491\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.5788094997406006\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.6119913458824158\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.6707996726036072\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 63.3\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.6672887206077576\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 61.3\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.5274593234062195\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.7937910556793213\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.6509895324707031\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.5934125185012817\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 62.4\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.7119235396385193\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.5277005434036255\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.7544876337051392\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.48622724413871765\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 64.3\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.6356619000434875\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 62.9\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.436411589384079\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.5238315463066101\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.6443713903427124\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 62.7\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.6425521969795227\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.6116721034049988\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.5135117769241333\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.42945393919944763\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.5964950919151306\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 61.2\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.6337623000144958\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.35243284702301025\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.6296511888504028\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.6580332517623901\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.6605759859085083\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.616742730140686\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.5449144244194031\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.9105456471443176\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.5613527894020081\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.6705430150032043\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.5400479435920715\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.4260074496269226\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.46206405758857727\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.38332653045654297\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 62.1\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.7455216646194458\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 62.1\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.670155942440033\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.4770919680595398\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.7473382949829102\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.6720777153968811\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.6676822304725647\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.9089237451553345\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.6621682047843933\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.45306631922721863\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.3634908199310303\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.6286377310752869\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 65.4\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.4265441298484802\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.4508189558982849\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.5514089465141296\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.3983272612094879\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.7258633971214294\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.48741763830184937\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.34864214062690735\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.5010080337524414\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 62.7\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.5461491942405701\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 63.2\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.40988874435424805\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 63.4\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.4061175584793091\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 63.5\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.5231295824050903\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.3468458950519562\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 63.5\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.476504385471344\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.48238250613212585\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.30457907915115356\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 62.9\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.5546745657920837\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.49807190895080566\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.4666038453578949\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.6563090085983276\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.6211350560188293\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.3464377224445343\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.7107194662094116\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.6185996532440186\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.5110635161399841\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 63.0\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.4137272834777832\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 63.4\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.4979911148548126\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.7001358866691589\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 61.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.23519256711006165\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 62.0\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.481136292219162\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 62.7\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.46241116523742676\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.37263643741607666\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 61.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.4469790756702423\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 65.1\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.29684916138648987\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 65.1\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.5238204598426819\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 64.6\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.3624670207500458\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.38687601685523987\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 61.8\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.4987877309322357\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 63.1\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.566204845905304\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.31164389848709106\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.33305656909942627\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.29549017548561096\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 62.0\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.42544007301330566\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 62.5\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.22074252367019653\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 61.1\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.25020381808280945\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 63.5\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.19330571591854095\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.6737098097801208\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.7628519535064697\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.6817014813423157\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 60.9\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.4940865635871887\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.533187210559845\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 62.4\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.6237103343009949\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 64.8\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.46378064155578613\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.2832954525947571\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 59.8\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.31713226437568665\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.6607025861740112\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 64.5\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.3552214503288269\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.45348334312438965\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 64.6\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.4138072431087494\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 62.2\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.37814515829086304\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.5209837555885315\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.5070581436157227\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 63.4\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.34655505418777466\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 61.7\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.3217250108718872\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 61.3\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.30139219760894775\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.3077075481414795\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 62.0\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.36251160502433777\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 62.9\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.5564494729042053\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.25895825028419495\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.19397157430648804\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.4386778473854065\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.3598386347293854\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.16475343704223633\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 62.6\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.42003071308135986\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 62.0\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.3464806079864502\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 61.5\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.34464818239212036\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 61.8\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.4276355803012848\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.15589389204978943\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 62.5\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.23615160584449768\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 62.8\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.39117932319641113\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.12036116421222687\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.4536830186843872\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.28730371594429016\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.22241653501987457\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 61.6\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.33996638655662537\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 65.1\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.4243600368499756\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.3056173026561737\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 63.9\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.24282556772232056\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.316111296415329\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.43338337540626526\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.2142658829689026\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 63.4\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.25832486152648926\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.26860618591308594\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 62.1\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.42244264483451843\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.3018907308578491\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 59.9\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.24423153698444366\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.11522707343101501\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.12203294783830643\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 62.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.21244001388549805\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 61.5\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.1692037582397461\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 61.2\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.3659396767616272\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 63.5\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.3391084671020508\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 61.1\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.23301169276237488\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 63.7\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.26705634593963623\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.22003912925720215\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.31345558166503906\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.1971360445022583\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 63.9\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.3048337697982788\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.08571436256170273\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 63.7\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.13489091396331787\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 65.0\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.3997429311275482\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 64.6\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.3402985632419586\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.3199918866157532\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.32210084795951843\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.4004676938056946\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 62.0\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.3398170471191406\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.39220550656318665\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.5660046339035034\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.13944536447525024\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.7985541820526123\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 63.4\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.4057217836380005\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 61.8\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.1382928192615509\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 62.7\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.3084338307380676\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 64.4\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.3067103922367096\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.1598389595746994\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.5283986330032349\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 63.5\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.28997406363487244\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 61.4\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.14876720309257507\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.23939567804336548\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 63.6\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.3193954825401306\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 63.2\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.0879262313246727\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.1309899538755417\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 64.2\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.23778069019317627\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.11931636184453964\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 63.2\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.20245040953159332\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 63.3\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.19942313432693481\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.34830909967422485\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 63.3\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.38400378823280334\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 63.9\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.32210537791252136\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 61.7\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.1422443389892578\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 59.6\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.22869135439395905\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.22886574268341064\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.20763438940048218\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.232171893119812\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 61.3\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.2085210531949997\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.14061099290847778\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 61.0\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.21541041135787964\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 60.9\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.09030834585428238\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.15853187441825867\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.2166357934474945\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 63.9\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.2074507772922516\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 63.9\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.286092072725296\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 63.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "66.1\n",
      "135\n",
      "hidden_size             512\n",
      "kernel_size               3\n",
      "interaction_type     concat\n",
      "train_loss_hist        None\n",
      "val_acc_hist           None\n",
      "max_val_acc            None\n",
      "max_val_acc_epoch      None\n",
      "Name: 4, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 0.9704232215881348\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 52.7\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 0.8816180229187012\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 53.1\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 0.9342094659805298\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 57.1\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 0.8410672545433044\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 55.6\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.841260552406311\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 59.7\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.8510046601295471\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 0.9084119200706482\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 59.9\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.8237226009368896\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 1.1050992012023926\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.7581140995025635\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 61.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.7445151209831238\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.8877547979354858\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 62.2\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.8553110361099243\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.8366382122039795\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 64.6\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.7447705268859863\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 63.2\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.7982133626937866\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 63.7\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.7374877333641052\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.9397774338722229\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 62.4\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.837943971157074\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 63.1\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.9149953126907349\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.9679306149482727\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7094701528549194\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 63.2\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.7472810745239258\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.9245411157608032\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 63.7\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.9571068286895752\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 64.0\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.8960444927215576\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 63.0\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.7609596848487854\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 63.8\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.7914137244224548\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 63.0\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.815031111240387\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 63.4\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.7534430623054504\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.732833981513977\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 63.0\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.7017883658409119\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.5509900450706482\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 63.3\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.5392782688140869\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 62.7\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.48094186186790466\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.756639838218689\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.8515887260437012\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.8068480491638184\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.6068832278251648\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.5109652280807495\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 65.5\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.6700677871704102\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.7007222175598145\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.8083106279373169\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.7911085486412048\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.8332008123397827\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 66.8\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.6364081501960754\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 65.9\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.6436067223548889\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 66.7\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.7440414428710938\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 64.9\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.7482324838638306\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.6578739881515503\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.7436067461967468\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 67.3\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.9049428105354309\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 67.1\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.6680230498313904\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 66.9\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.7411232590675354\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 66.9\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.6902433037757874\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 67.7\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.7344405651092529\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 69.8\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.7270761132240295\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.8310729265213013\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 69.8\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.7067388892173767\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 70.1\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.6548072695732117\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 68.5\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.6787920594215393\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 68.3\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.8146597146987915\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 67.1\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 68.4\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.6289386749267578\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 68.6\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.8607664108276367\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 68.3\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.7738093137741089\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.8111180663108826\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 67.3\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.5929726958274841\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 66.3\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.6038966774940491\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.7582720518112183\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.7050091028213501\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 68.8\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.48053795099258423\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.6181271076202393\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 67.3\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.5051093101501465\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.5153613090515137\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.5295346975326538\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 69.6\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.5832073092460632\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.5808860659599304\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 67.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.7329577803611755\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 68.4\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.6278986930847168\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 67.6\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.806607186794281\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 69.7\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.7533426284790039\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6217280030250549\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.7\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.6225863695144653\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 67.6\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.5531108379364014\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 68.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.8181281685829163\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 70.9\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.5917066335678101\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.670306921005249\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.9441414475440979\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 68.5\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.6463699340820312\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.4690819978713989\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 69.2\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.5378488302230835\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 66.7\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.7154197096824646\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 68.4\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.8374269604682922\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 68.0\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.771072506904602\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.5309295058250427\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 69.5\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.47182372212409973\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.5418498516082764\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 69.1\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.39098626375198364\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.7374314069747925\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 68.0\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.6731486916542053\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.41915690898895264\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.6524755954742432\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.8037853240966797\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.5608987212181091\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 68.5\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.39969778060913086\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 67.4\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.5014350414276123\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.6654701828956604\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 70.1\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.4229336678981781\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 69.7\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.3677007555961609\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.5573340654373169\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.7667699456214905\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.5773468017578125\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 68.7\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.5281328558921814\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 69.8\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.7839922308921814\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 69.3\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.4340256452560425\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 69.1\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.4052391052246094\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.5324242115020752\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.6729121804237366\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.4489758014678955\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.6395941972732544\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.5389823913574219\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 70.3\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.6678041815757751\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 70.1\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.7960562705993652\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 68.6\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.5264012813568115\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 70.9\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 70.6\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.4392838776111603\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 69.9\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.7093787789344788\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 68.9\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.19276835024356842\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.5930426120758057\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 68.9\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.42215627431869507\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 68.4\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.2674030661582947\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 69.3\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.464130699634552\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.34203076362609863\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.4187352657318115\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.637190043926239\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.5803605318069458\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.3364707827568054\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 70.8\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.37232130765914917\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 68.9\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.6233169436454773\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 69.3\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.5147718787193298\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.20949774980545044\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 69.1\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.5747489929199219\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 69.2\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.6556635499000549\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 68.6\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.5390534400939941\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 70.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.516187310218811\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.6696739792823792\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.49908190965652466\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 69.5\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.527518093585968\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.4164397120475769\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.45591774582862854\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 67.4\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.5953698754310608\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 68.0\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.37665075063705444\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 68.6\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.5835419297218323\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 69.1\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.4316047728061676\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.46138525009155273\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 67.4\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.5967762470245361\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 69.5\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.4959295988082886\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 68.8\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.20513704419136047\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.34920772910118103\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.1943776160478592\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 68.5\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.5250849723815918\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.4327262043952942\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.3833307921886444\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.5835253596305847\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.29600486159324646\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 69.6\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.43662941455841064\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 69.4\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.2557641267776489\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.5016727447509766\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.2554326057434082\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.7240322232246399\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 68.0\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.24698340892791748\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 68.4\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.36370420455932617\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 68.1\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.23192434012889862\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 69.1\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.33551645278930664\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 68.3\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.4920872747898102\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.4538121223449707\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 68.8\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.5093212127685547\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.3513367474079132\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 68.7\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.25268808007240295\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 68.8\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.3669919967651367\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 68.5\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.33880218863487244\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 69.2\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.2058301717042923\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.573337972164154\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 68.7\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.4337082505226135\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.41324180364608765\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 69.1\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.4275924563407898\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 70.6\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.4370018243789673\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 69.7\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 69.8\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.22224533557891846\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 69.7\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.20327240228652954\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.28876763582229614\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.38883307576179504\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.38390710949897766\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.2954460382461548\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 69.6\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.18028081953525543\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 69.0\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.2601102292537689\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 69.0\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.23451630771160126\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.2605215907096863\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 69.5\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.21055114269256592\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 69.0\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.17754091322422028\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.08939161896705627\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 69.6\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.16039976477622986\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.38939327001571655\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 69.4\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.30662330985069275\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.20717337727546692\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 68.8\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.43740028142929077\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.3072182238101959\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.538180410861969\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.4171377122402191\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.3046315312385559\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.192058265209198\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 68.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.21406078338623047\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.3702787160873413\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 68.4\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.22988061606884003\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.6769160032272339\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 68.8\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.25983926653862\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.23668932914733887\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.35714396834373474\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 69.3\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.3968829810619354\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 69.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 68.3\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.14546014368534088\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 70.0\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.10101834684610367\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 69.8\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.1989109367132187\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.12349934130907059\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.08613114804029465\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.4622642695903778\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 68.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.2359607219696045\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 68.7\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.3472234606742859\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.06805997341871262\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 69.1\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.1270562708377838\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.4108702540397644\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.3157821595668793\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 68.7\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.3128831088542938\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.24936063587665558\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 69.1\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.29523783922195435\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.10365577042102814\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.33297282457351685\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 1.0213875770568848\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 69.2\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.32326656579971313\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 70.3\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.21048741042613983\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.12822696566581726\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.176614448428154\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 69.4\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.10956345498561859\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.26434239745140076\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.46685051918029785\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 68.9\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.387953519821167\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.2917281985282898\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.18793471157550812\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.2768261432647705\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.2976156771183014\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 68.7\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.24967971444129944\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.2139526605606079\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 68.5\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.12650728225708008\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 70.2\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.08996559679508209\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.2269316464662552\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.5865383148193359\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 67.5\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.06987151503562927\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.11580230295658112\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.04935256019234657\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.06133156642317772\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.36392727494239807\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.0695606991648674\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 68.7\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.055388323962688446\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.0895378366112709\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.22872421145439148\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.10451696068048477\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 69.0\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.4439897835254669\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.13943445682525635\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.12329307943582535\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 70.2\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.1565828025341034\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 69.2\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.5051973462104797\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.1371268928050995\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.37404900789260864\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.572677493095398\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.25961700081825256\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.16521139442920685\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.17572422325611115\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 69.0\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.15814651548862457\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.3234059810638428\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.1335100680589676\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.15935415029525757\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.1785789430141449\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 69.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 68.9\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.06321727484464645\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.20675799250602722\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.03291372209787369\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 69.5\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.280093252658844\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.10916519910097122\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 68.5\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.17725494503974915\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.1757168471813202\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 70.0\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.06562186032533646\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 69.0\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.20352639257907867\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 67.5\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.1890007108449936\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 69.8\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.06392588466405869\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.15094304084777832\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 70.1\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.10846328735351562\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.22362613677978516\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 70.4\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.12708966434001923\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.19797657430171967\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 69.0\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.07795251905918121\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.09381268918514252\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 70.1\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.12756170332431793\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 69.6\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.048626095056533813\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 69.8\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.22255486249923706\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.12704826891422272\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.093032106757164\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 69.3\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.05524527281522751\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.09881611913442612\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.18364810943603516\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.08199262619018555\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 70.0\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.44505444169044495\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 70.1\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.08228380233049393\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.10756546258926392\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 68.3\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.19981732964515686\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 68.1\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 68.0\n",
      "70.9\n",
      "86\n",
      "hidden_size           512\n",
      "kernel_size             3\n",
      "interaction_type      mul\n",
      "train_loss_hist      None\n",
      "val_acc_hist         None\n",
      "max_val_acc          None\n",
      "max_val_acc_epoch    None\n",
      "Name: 5, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.0960865020751953\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 35.8\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.1024248600006104\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 40.3\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 0.9777548313140869\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 45.0\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.0464565753936768\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 48.8\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 0.9754226803779602\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 1.0168100595474243\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 50.9\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.0743061304092407\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 52.0\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.9738883376121521\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 53.4\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.9508887529373169\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.9147385358810425\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 54.9\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.9404531717300415\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 54.7\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.9987666010856628\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 51.9\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.9640526175498962\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 53.8\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 1.005137324333191\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 56.3\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 1.0739554166793823\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.9257087111473083\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 57.5\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.7963675260543823\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 57.2\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.840461254119873\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 58.1\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.9563426971435547\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 59.1\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.8528516292572021\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.9552279114723206\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.8364022970199585\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 59.1\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.8626892566680908\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 60.0\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.9495235085487366\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.8920220732688904\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 60.3\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.832793116569519\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 62.4\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.9832074642181396\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.77398282289505\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 61.4\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.8872703909873962\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 60.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.8070165514945984\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8724780082702637\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.9243180155754089\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 59.9\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.9085099697113037\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 60.5\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.7184205055236816\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 63.0\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.9313076734542847\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.918367862701416\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.7952067255973816\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 61.2\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.6760746240615845\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.7175746560096741\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.9429757595062256\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.8643702864646912\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 65.6\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.6494402885437012\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.6659456491470337\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.860865592956543\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.7080385684967041\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.7797059416770935\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 61.6\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.8329801559448242\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.9861682057380676\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.9654557704925537\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.6030622720718384\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 64.9\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.7799420356750488\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.7718328237533569\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.6265586018562317\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 65.6\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.8827783465385437\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.8209729194641113\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 64.9\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.7417534589767456\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.6851851344108582\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.8636837601661682\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.8647783994674683\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 66.0\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.8646018505096436\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 62.2\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.9879962205886841\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 63.3\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.7138485908508301\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 62.4\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.7738396525382996\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.6467499136924744\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.6822102665901184\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.7892242670059204\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.7298949956893921\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.560226559638977\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 65.1\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.7176584005355835\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6608457565307617\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.545161247253418\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.5467263460159302\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.6736530661582947\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.6870090961456299\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 65.8\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.6241994500160217\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.6113549470901489\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 64.2\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.5608692169189453\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.6349881887435913\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.6692056059837341\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 65.8\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.6893516778945923\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.6223304271697998\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6786714792251587\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.7835285663604736\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.6475708484649658\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.5724105834960938\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 67.7\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.6464524269104004\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 65.4\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.7120371460914612\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.8846133351325989\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.5721790790557861\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.5029479265213013\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.5846033096313477\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.47031158208847046\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 65.7\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.6698963642120361\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.5809841752052307\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.5182948112487793\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 66.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.7213506698608398\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.7795091271400452\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.46231991052627563\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.6382578015327454\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.5422007441520691\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.6628296375274658\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.459025502204895\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.8276776671409607\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.38529735803604126\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.5043524503707886\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.45965132117271423\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.6372872591018677\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.6359091997146606\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.6677896976470947\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.527524471282959\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.6324155926704407\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 65.3\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.44693616032600403\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.4857977032661438\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.4758819043636322\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 68.1\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.6515456438064575\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.4744894206523895\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.6351848840713501\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 70.1\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.7031890749931335\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.7919440865516663\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.8143582344055176\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6522958874702454\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.5735458135604858\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.6824270486831665\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 69.1\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.5295755863189697\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 68.9\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.47770875692367554\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 67.9\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.4376891553401947\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 66.4\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.4771839380264282\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 66.8\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.6933073997497559\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 67.5\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.446331650018692\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 66.9\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.5429712533950806\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 67.3\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.3661617040634155\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.5795567035675049\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 67.9\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.5543886423110962\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.27749672532081604\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.3495200276374817\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 67.5\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.3848804235458374\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 67.9\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.45482856035232544\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.5582395792007446\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.4697535037994385\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.7428483963012695\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.6365757584571838\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 67.1\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.5054305791854858\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 66.9\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.4943568706512451\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.46091026067733765\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 68.5\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.6077761650085449\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.41763412952423096\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.5176371335983276\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 68.4\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.3886827528476715\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 67.7\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.4144632816314697\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.5331332087516785\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 67.5\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.4010073244571686\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 68.6\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.363550066947937\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.5350107550621033\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.5833984613418579\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.5167949795722961\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.4460827112197876\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.2614537179470062\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.3733522593975067\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.21864676475524902\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 68.0\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.29652488231658936\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.23470216989517212\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.5915995836257935\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 65.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.27287396788597107\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 65.4\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.6721771955490112\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.3439188301563263\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.27193683385849\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.6010063886642456\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.2655787467956543\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.3553316593170166\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.4718402028083801\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 68.7\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.3710592985153198\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 69.8\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.5730160474777222\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.3216850459575653\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.36370328068733215\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.3288349509239197\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.3880360424518585\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.2928502857685089\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 67.0\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.2923245131969452\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.5685387849807739\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.3265978991985321\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.5748280882835388\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 67.0\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.5222411155700684\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 68.3\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.2624589800834656\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.2836781144142151\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.4946248531341553\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5192194581031799\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.3117103576660156\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.2584208846092224\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.28272587060928345\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.3431673049926758\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.2742542624473572\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.20435649156570435\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.2522300183773041\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.27653875946998596\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 68.8\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.2605322599411011\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 69.7\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.29153579473495483\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 69.6\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.38049986958503723\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.3849761486053467\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.3336676359176636\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.4014962315559387\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.5093732476234436\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.24730344116687775\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.3384590148925781\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.4637441039085388\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.2820076048374176\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.5698261857032776\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 65.6\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.30201098322868347\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 69.4\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.45037317276000977\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.5594104528427124\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.37642714381217957\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.285999596118927\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.44742074608802795\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 65.9\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.31361404061317444\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.4021092355251312\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 65.8\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.25373584032058716\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 66.5\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.19829924404621124\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.2901384234428406\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.4049733877182007\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.17131362855434418\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.09766453504562378\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.1879044473171234\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.33599644899368286\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 66.4\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.20972087979316711\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 66.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.15225225687026978\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 65.9\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.482805073261261\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.16474607586860657\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.23745667934417725\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.2522098422050476\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 67.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.3535829782485962\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.37373459339141846\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.1906854510307312\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 65.5\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.23669490218162537\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 65.4\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.27808135747909546\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 66.8\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.17098470032215118\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.17398062348365784\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 66.2\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.24503032863140106\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.3574824631214142\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 64.7\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.2083229124546051\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.23954817652702332\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 65.4\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.3783835768699646\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 65.6\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.23920311033725739\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.19054925441741943\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 66.1\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.15756098926067352\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.2429681271314621\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.26032257080078125\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.3491220772266388\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.18176104128360748\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 66.2\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.13027477264404297\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.15921002626419067\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.14576874673366547\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.09976667165756226\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 68.5\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.20425322651863098\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.15501779317855835\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.24795883893966675\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.09433644264936447\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 65.3\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.21082442998886108\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.24654069542884827\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.16157959401607513\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.3164820671081543\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.13879409432411194\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.23265695571899414\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.13483260571956635\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.24191537499427795\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.16524338722229004\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.20098504424095154\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 66.7\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.07625725120306015\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.0779232531785965\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 65.6\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.2733680307865143\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 66.3\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.25743240118026733\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.34163275361061096\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.07163441926240921\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 66.1\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.2573067843914032\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 65.3\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.4114314317703247\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.1616472452878952\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.18548192083835602\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 66.2\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.33246466517448425\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.2503129243850708\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 65.8\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.1407097727060318\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.31263989210128784\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.09957987815141678\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.09230448305606842\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.03832293301820755\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.07335792481899261\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.17333947122097015\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 65.9\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.1406194269657135\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.4426303505897522\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 67.3\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.1489832103252411\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.10433126240968704\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 65.8\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.16217362880706787\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 64.5\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.24525032937526703\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.09767838567495346\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.08150406181812286\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.08959950506687164\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 67.9\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.2169070541858673\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 64.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.21051207184791565\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.23742994666099548\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.13118979334831238\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.1503363847732544\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.11843923479318619\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.19352516531944275\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.07209999114274979\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.10297583788633347\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 65.9\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.10594437271356583\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 66.5\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.1249988004565239\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.0507877841591835\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 65.7\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.1005728542804718\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.05923404544591904\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.22390064597129822\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 65.1\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.3152618408203125\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 68.3\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.2769116759300232\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.9\n",
      "70.1\n",
      "119\n",
      "hidden_size             512\n",
      "kernel_size               5\n",
      "interaction_type     concat\n",
      "train_loss_hist        None\n",
      "val_acc_hist           None\n",
      "max_val_acc            None\n",
      "max_val_acc_epoch      None\n",
      "Name: 6, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.1052989959716797\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 49.2\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0449793338775635\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 54.6\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 0.9798136949539185\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 55.8\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 0.9325263500213623\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 58.2\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 1.0231516361236572\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.9328532814979553\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 0.9573867917060852\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 57.9\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.9581176042556763\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 60.3\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 1.0374536514282227\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 59.9\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 0.8277212977409363\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.8745279312133789\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 60.2\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.6576516032218933\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 57.2\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.7571104764938354\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 59.1\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.9255580306053162\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 58.1\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.7781901359558105\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.7645084857940674\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 60.0\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.6717268824577332\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 60.5\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.6801804304122925\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.825567364692688\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.675025463104248\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 1.0714410543441772\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 61.3\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7207797765731812\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 61.5\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.6114540100097656\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 62.7\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.9013987183570862\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 63.5\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.9028974175453186\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 61.8\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.7326607704162598\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 61.2\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 0.9677579402923584\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 62.6\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.652799129486084\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 63.5\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.8899145126342773\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 62.9\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 1.0375837087631226\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.742780327796936\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.717868983745575\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.5492534637451172\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.6546775698661804\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 63.5\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.6361798048019409\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 64.3\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.7238074541091919\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.6093400716781616\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 64.8\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.542489230632782\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 63.4\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.8156877160072327\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.7951955199241638\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.6672106981277466\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.7020946741104126\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.8963170647621155\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 64.9\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.5905052423477173\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.8029724359512329\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 65.9\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.5643304586410522\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.6448172926902771\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.7385076284408569\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 65.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.7806349396705627\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.9725139141082764\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 66.4\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.801888108253479\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 66.3\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.7756314873695374\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 65.7\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.6322757601737976\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.9735460877418518\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 66.8\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.6180745363235474\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 67.2\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.6397939920425415\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 66.7\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.8959238529205322\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.8330637812614441\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 67.3\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.8624299168586731\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 66.5\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.6141868233680725\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 67.7\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.5969610214233398\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 67.3\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.6557496190071106\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.6445755958557129\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.7638415694236755\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.7194056510925293\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.6560930013656616\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.5569984316825867\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.8890639543533325\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.6786590814590454\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6635106205940247\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 67.6\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.6937242746353149\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.7267633676528931\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.1\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.4899255633354187\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.49324366450309753\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.7385169267654419\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 66.2\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.6843523979187012\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 65.8\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.5702555179595947\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.7357593178749084\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.7459492683410645\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.4422685503959656\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.5831235647201538\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 66.4\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.821726381778717\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.5090851783752441\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.5217544436454773\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.5376565456390381\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.7241932153701782\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.7645031213760376\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.8776803612709045\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.6505666375160217\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.41472551226615906\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.44583356380462646\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 66.7\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.7119002938270569\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.6614459156990051\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 66.7\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.39514976739883423\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.48153913021087646\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 67.5\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.5391154289245605\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.26062896847724915\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.5952971577644348\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.5623074173927307\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.725732147693634\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.3418065905570984\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.6355195045471191\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.5364388227462769\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 65.9\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.4539494514465332\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.7211688756942749\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.4522404074668884\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.2867579460144043\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.545513927936554\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.7407699227333069\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.3456863760948181\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 66.3\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.6701858639717102\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.7009621262550354\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 66.2\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.31754210591316223\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.48414871096611023\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 67.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.7943367958068848\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.3917333781719208\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 66.0\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.45872944593429565\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 65.4\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.6579786539077759\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 66.9\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.5094728469848633\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 66.1\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.7638616561889648\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 65.8\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6398367881774902\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.43728694319725037\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 65.2\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.3831630051136017\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.3372224271297455\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 67.0\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.38273847103118896\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.3426313102245331\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 66.4\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.3458811640739441\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.20410314202308655\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 66.3\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.46790122985839844\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.5389813184738159\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.2480863481760025\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.294046014547348\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.35534167289733887\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.29745155572891235\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.36551904678344727\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.24995175004005432\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.5195533037185669\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.6029254198074341\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.2852553129196167\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.3324437439441681\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.3143075406551361\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.21655382215976715\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.37767288088798523\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.3567819595336914\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.38472193479537964\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.3702124059200287\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.5231693387031555\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.3892599940299988\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.21064572036266327\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.5480738878250122\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.3829476535320282\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.32216155529022217\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.7099717259407043\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.5488969087600708\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.28868335485458374\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.3754815459251404\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.15834689140319824\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.3513360321521759\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.1607758104801178\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.17561018466949463\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.22915233671665192\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.1003243550658226\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.25233837962150574\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.34342557191848755\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.11639601737260818\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.23433658480644226\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.11543843895196915\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.22019436955451965\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.33786478638648987\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 65.6\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.5180829763412476\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.12157431989908218\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.44379621744155884\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.30757471919059753\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.42909038066864014\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.3850599527359009\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.2859213948249817\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.3349047303199768\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 63.7\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.2140287160873413\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.2610597610473633\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.28552624583244324\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 64.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.403083860874176\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.2835376560688019\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 64.7\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.39948907494544983\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 62.1\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.22600626945495605\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.3750884234905243\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.3482931852340698\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 63.9\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 62.6\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.20412448048591614\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 65.1\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.08179338276386261\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 63.6\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.2176036238670349\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 64.0\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.09642712771892548\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.12166735529899597\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.17239828407764435\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.1242871880531311\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 64.9\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.25221383571624756\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.12869377434253693\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.12994104623794556\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.29888030886650085\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 65.7\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.0783386081457138\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.33483314514160156\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.10249096155166626\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.4770985245704651\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 66.0\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.24791720509529114\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 65.7\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.3720223903656006\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 66.5\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.46317362785339355\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.23199062049388885\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.17706383764743805\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.13404111564159393\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.25756871700286865\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.21657773852348328\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.04995419457554817\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.2965035140514374\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.2957315444946289\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.21745318174362183\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.13244380056858063\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.19840243458747864\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 65.2\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.4314301609992981\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 64.3\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.11659295111894608\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 65.1\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.11670360714197159\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 65.7\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.10014554858207703\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.13499945402145386\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 65.1\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.10012739896774292\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.12046778947114944\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.22017253935337067\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 65.7\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.1333048939704895\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.1875760555267334\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.14333470165729523\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.022285640239715576\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 64.5\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.23051968216896057\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.16429850459098816\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.11294733732938766\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.09890685975551605\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 64.4\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.12303486466407776\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.3585599362850189\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.08260507881641388\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 64.7\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.10851281136274338\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 64.2\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.12512877583503723\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.08341376483440399\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.5041062235832214\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 65.6\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.10285963118076324\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 64.4\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.1714586615562439\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 64.5\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.37895992398262024\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.09523298591375351\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.18686020374298096\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.16051192581653595\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 65.5\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.22794848680496216\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 63.9\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.08479324728250504\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 65.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.19037380814552307\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 63.7\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.06018519774079323\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.04769989103078842\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.15103524923324585\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 65.2\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.06637651473283768\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 65.0\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.07334764301776886\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 64.4\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.22991378605365753\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.06271404027938843\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 63.5\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.0389341339468956\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.1792975813150406\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 64.0\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.11874324083328247\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 64.0\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.036815233528614044\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.16949661076068878\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 64.1\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.21181918680667877\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.0750177726149559\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 64.6\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.23933877050876617\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.20885097980499268\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 64.0\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.07982416450977325\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.21867205202579498\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 64.3\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.2967890501022339\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 62.2\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.08907483518123627\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 64.3\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.0480286180973053\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 62.6\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.1313719004392624\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.11074095219373703\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 63.7\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.07859250158071518\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 63.4\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.08734287321567535\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.12460369616746902\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.21927928924560547\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.1930278241634369\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 62.9\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.05187175050377846\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.04555467516183853\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.16728575527668\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.1013251394033432\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 62.0\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.5\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.026545200496912003\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.022376500070095062\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.06278055906295776\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 62.9\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.038185831159353256\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.06278317421674728\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.044327303767204285\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 61.5\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.04416017234325409\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 62.5\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.09439172595739365\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 63.3\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.1278793215751648\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 62.4\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.08996423333883286\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 64.9\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.12521138787269592\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.0961138978600502\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 64.3\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.15535549819469452\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 64.2\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.02763681858778\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.34017300605773926\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 65.7\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.08284693211317062\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 65.7\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.25203028321266174\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.21939679980278015\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 64.7\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.10833559930324554\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 64.9\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.08807110786437988\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.2121793031692505\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 64.3\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.40936505794525146\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 64.5\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.07099110633134842\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 63.9\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.26020723581314087\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 64.9\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.07321610301733017\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 64.8\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.16223250329494476\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 64.5\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.04600323736667633\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 64.7\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.1705995351076126\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 64.3\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.05598630756139755\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 64.6\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.1251915693283081\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 64.4\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.24318917095661163\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 65.9\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 66.1\n",
      "68.2\n",
      "89\n",
      "hidden_size           512\n",
      "kernel_size             5\n",
      "interaction_type      mul\n",
      "train_loss_hist      None\n",
      "val_acc_hist         None\n",
      "max_val_acc          None\n",
      "max_val_acc_epoch    None\n",
      "Name: 7, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.0696094036102295\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 39.8\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0499444007873535\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 39.0\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.046463966369629\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 44.6\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 0.8904687166213989\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 45.0\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 1.1183472871780396\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 49.6\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 1.0601534843444824\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 52.1\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.0433036088943481\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 1.1123273372650146\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 55.3\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.8627541065216064\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 54.6\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 1.0529999732971191\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 55.5\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.899333655834198\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 54.2\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.8816832304000854\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 56.1\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.6943933963775635\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 56.4\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.955711305141449\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 57.2\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.8075287342071533\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 57.4\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 0.9599506855010986\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 55.1\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 1.0542429685592651\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 57.0\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.9889556169509888\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 58.3\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.9877364635467529\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 56.0\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 0.9446580410003662\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 59.5\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 1.023700475692749\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 58.0\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.7100467085838318\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 58.1\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.7616018056869507\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 58.9\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.7945913672447205\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 58.7\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.9758657813072205\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 56.7\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.8793081045150757\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 57.7\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 1.0577563047409058\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 58.4\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 0.9127728343009949\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.6949072480201721\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.9283214807510376\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 58.3\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8559607863426208\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 60.6\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 60.9\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.6581117510795593\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 60.8\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.8700847029685974\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 59.0\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.7692307829856873\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 60.9\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.7714623212814331\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.6538803577423096\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 61.5\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.6366233229637146\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 58.5\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.8477871417999268\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 58.1\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.9197346568107605\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 60.2\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.6389086246490479\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 1.010115385055542\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 59.9\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.7293602824211121\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 61.0\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.8716797232627869\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 61.3\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.832193911075592\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 61.6\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.6632964611053467\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 62.7\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.7722890973091125\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 61.8\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.7896490693092346\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.6624534130096436\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.5328602194786072\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.7821453213691711\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 62.7\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.8404821157455444\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 62.2\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.6479586362838745\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 62.6\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.8805196285247803\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 61.4\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.6212847232818604\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.6383301019668579\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 63.6\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.8790118098258972\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.8742371201515198\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 63.9\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 1.1397418975830078\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.5815632343292236\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 61.0\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.8849149346351624\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.7621583938598633\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.8808456063270569\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.5779617428779602\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 1.0350489616394043\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 62.1\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.4213848114013672\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 63.3\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.5703969597816467\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.9042360782623291\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 60.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.6212116479873657\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 62.1\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.7828437685966492\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.5846081972122192\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.8454771637916565\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 61.9\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.7785950303077698\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 59.5\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.7738522291183472\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 63.5\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.6788663864135742\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.7604480385780334\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 63.7\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.6325148344039917\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.7217541933059692\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.6816790699958801\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 64.5\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.5913055539131165\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.858456552028656\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 63.4\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.7211328744888306\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 63.9\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.6720173954963684\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.6953400373458862\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 63.1\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.7948594689369202\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 62.8\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.7995919585227966\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.7676084041595459\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 61.6\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.6525499820709229\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 64.9\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.6129312515258789\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 61.4\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.5967445969581604\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 64.4\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 0.8314291834831238\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.8249462246894836\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.5940937995910645\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.6083468198776245\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 64.0\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.5486294627189636\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.5291378498077393\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.6947069764137268\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.6348239183425903\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.6508471369743347\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.43782633543014526\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.5173550844192505\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.6647514700889587\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.568733811378479\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 64.7\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.5645464658737183\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.7456960082054138\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.5465754866600037\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.5432872772216797\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 64.4\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.6392170190811157\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 65.0\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.6544977426528931\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 62.0\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.6768931150436401\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.6308264136314392\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 1.010392665863037\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.5967578887939453\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 64.2\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.7033302783966064\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 67.1\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.6186226606369019\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.8694671988487244\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.5596715211868286\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 64.8\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.5215415954589844\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.6424589157104492\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.6051325798034668\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.6579374074935913\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 65.6\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6377871036529541\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.6944249868392944\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 64.9\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.6837431788444519\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 61.1\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.5609086751937866\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 60.7\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.5335059762001038\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 61.5\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.48890700936317444\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 62.5\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.49744153022766113\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.2769874632358551\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.5010101199150085\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.36208826303482056\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 62.4\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.33049118518829346\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.6075606942176819\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 61.6\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.566887617111206\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 63.1\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.4238693118095398\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 65.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.48693278431892395\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.5080539584159851\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.5400339365005493\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 62.7\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.4695316553115845\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 62.0\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.705337643623352\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.3434630334377289\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.3929605185985565\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 63.1\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.39134684205055237\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.4932117760181427\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.3744311034679413\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.5371963977813721\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.6409752368927002\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.6517921090126038\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.5261839628219604\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.3564877510070801\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.43331968784332275\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.3966989815235138\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 65.1\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.46574732661247253\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.5281695127487183\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.4928576946258545\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.4509461522102356\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.25734809041023254\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.3950161039829254\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.14879003167152405\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.441213995218277\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 63.9\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.43954116106033325\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.11176269501447678\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.30736881494522095\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.5314106941223145\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 62.9\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.337698370218277\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 63.0\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.31754663586616516\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 63.1\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.1561540961265564\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 62.1\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.3860340416431427\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.3481113314628601\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.410022109746933\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 64.1\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.3575894236564636\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.3748384714126587\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 64.3\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.4042608439922333\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.22826260328292847\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 63.3\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.3905014097690582\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 63.4\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.48965317010879517\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 62.5\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.49773749709129333\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.2025030106306076\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.3745794892311096\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.12924042344093323\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.4706052541732788\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.38108059763908386\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.5337687730789185\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 65.6\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.3394243121147156\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.2448491007089615\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.42172086238861084\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5418986082077026\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 64.5\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.35211503505706787\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.3611355125904083\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 64.9\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.3369002640247345\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.22140716016292572\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.16451656818389893\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 64.6\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.2580392360687256\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 64.0\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.26704683899879456\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 65.3\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.23642697930335999\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 61.6\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.12795981764793396\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.4562729597091675\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.22106879949569702\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 62.8\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.2765781879425049\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 66.1\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.15667909383773804\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.3142426609992981\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 63.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.25175511837005615\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.3762384057044983\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 64.1\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.21051424741744995\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.3169766664505005\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 64.5\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.36879923939704895\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 63.2\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.4000057876110077\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 63.3\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.4481079578399658\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.2826695442199707\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 63.4\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.31254470348358154\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 63.9\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.43649953603744507\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 64.9\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.2583838999271393\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 60.9\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.4902866780757904\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 63.7\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.6123657822608948\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 63.0\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.5554345846176147\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.19056165218353271\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 61.6\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.35746127367019653\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 64.2\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.14952844381332397\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 64.2\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.06586559861898422\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 64.1\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.04908958822488785\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.014632180333137512\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.09726139903068542\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 62.0\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.2301003634929657\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.09371189028024673\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 64.5\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.2003348469734192\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 64.0\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.24885423481464386\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.13129442930221558\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 62.3\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.3314582109451294\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 64.8\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.09338509291410446\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 63.2\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.09983473271131516\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.11852222681045532\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 0.1799624115228653\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.08234894275665283\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.08298978209495544\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 63.6\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.46860113739967346\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 62.4\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.1574842482805252\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 64.6\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.09389349818229675\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 64.8\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.08378798514604568\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.32662320137023926\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 65.4\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.1840386986732483\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 63.8\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.3230814039707184\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 64.1\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.24298682808876038\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.23114284873008728\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 63.1\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.16735169291496277\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 63.3\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.11112357676029205\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 62.1\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.24866974353790283\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 63.5\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.13613568246364594\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 63.0\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.3039260506629944\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.10077597200870514\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 62.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.1347825527191162\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.03592090681195259\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 62.8\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.17212779819965363\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 63.8\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 0.08385380357503891\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 64.7\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.16248047351837158\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 64.4\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.05324149131774902\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 65.5\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.026604924350976944\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 63.9\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.1783607006072998\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 64.2\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.06953386217355728\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.36170971393585205\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 63.4\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.18999293446540833\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 62.0\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.6620902419090271\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 63.3\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.08093336969614029\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 63.7\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.18571408092975616\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 63.0\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.07792671024799347\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 63.2\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.036890316754579544\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.19118908047676086\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.16743208467960358\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 62.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.14498183131217957\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 61.9\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.21536755561828613\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 64.7\n",
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.1838529109954834\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 65.2\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.09544853121042252\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 62.4\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.218521386384964\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 63.9\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.21979166567325592\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 63.1\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.1318407654762268\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 62.1\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.13349178433418274\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 64.3\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.0529453419148922\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.14033639430999756\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 63.6\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.26982006430625916\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 62.3\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.13839635252952576\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 62.7\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.18385285139083862\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 65.8\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.031107261776924133\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 64.7\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.22756895422935486\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 64.6\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.1060144305229187\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 64.6\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.09608665853738785\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 64.5\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.08961153030395508\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.13823667168617249\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.08667777478694916\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 63.1\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.03184272348880768\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.06641735136508942\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 64.4\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.030943922698497772\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.23545408248901367\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 65.6\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.19721418619155884\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 64.7\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.11217936873435974\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.08315708488225937\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.08987554162740707\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 63.4\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.08327972888946533\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 62.3\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.08815789222717285\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.025125574320554733\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 62.0\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.17703989148139954\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 64.3\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.14773333072662354\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.3473418354988098\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 63.6\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.09437032788991928\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 63.3\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.08276496827602386\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 63.8\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.17206905782222748\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 64.9\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.1937864124774933\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 64.4\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.19600602984428406\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 65.5\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.09307347238063812\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 64.1\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.3267863094806671\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 63.7\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.1659557968378067\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.09265176951885223\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 65.2\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.16127169132232666\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 64.0\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "67.1\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "CNN_HIDDEN_SIZES = [200,512]\n",
    "INTERACT_TYPES = ['concat','mul']\n",
    "KERNEL_SIZES = [3,5]\n",
    "somelists = [CNN_HIDDEN_SIZES,KERNEL_SIZES,INTERACT_TYPES]\n",
    "LIN_HIDDEN_SIZE = 256\n",
    "result = list(itertools.product(*somelists))\n",
    "df_param = pd.DataFrame(result,columns=['hidden_size','kernel_size','interaction_type'])\n",
    "df_param['train_loss_hist'] = None\n",
    "df_param['val_acc_hist'] = None\n",
    "df_param['max_val_acc'] = None\n",
    "df_param['max_val_acc_epoch'] = None\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "for param_i in range(len(df_param)):\n",
    "    print(df_param.iloc[param_i])\n",
    "    CNN_HIDDEN_SIZE = df_param.iloc[param_i]['hidden_size']\n",
    "    KERNEL_SIZE = df_param.iloc[param_i]['kernel_size']\n",
    "    INTERACT_TYPE = df_param.iloc[param_i]['interaction_type']\n",
    "    \n",
    "    model = CNN(hidden_size=CNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat,kernel_size=KERNEL_SIZE).to(DEVICE)\n",
    "    classification_network = ClassificationNetwork(num_inputs=CNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=1,interact_type=INTERACT_TYPE).to(DEVICE)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters())+list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    train_loss_hist = []\n",
    "    val_acc_hist = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "            x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "            outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "            outputs = classification_network(outputs_x1,outputs_x2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_hist.append(loss.item())\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(epoch+1,num_epochs,i+1,len(train_loader),loss.item()))\n",
    "                val_acc_hist.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                # validate\n",
    "        val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "        val_acc_hist.append(val_acc)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                   epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "\n",
    "    val_acc_hist = np.array(val_acc_hist)\n",
    "    max_val_acc = np.max(val_acc_hist)\n",
    "    max_val_acc_epoch = np.argmax(val_acc_hist)\n",
    "    #df_param.set_value(i,'train_loss_hist',train_loss_hist)\n",
    "    df_param.at[param_i,'train_loss_hist'] = np.array(train_loss_hist)\n",
    "    \n",
    "    df_param.at[param_i,'val_acc_hist'] = val_acc_hist\n",
    "    df_param.at[param_i,'max_val_acc'] = max_val_acc\n",
    "    df_param.at[param_i,'max_val_acc_epoch'] = max_val_acc_epoch+1\n",
    "    print(max_val_acc)\n",
    "    print(max_val_acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(df_param,open('df_cnn_cv_correct.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size             200\n",
      "interaction_type     concat\n",
      "train_loss_hist        None\n",
      "val_acc_hist           None\n",
      "max_val_acc            None\n",
      "max_val_acc_epoch      None\n",
      "Name: 0, dtype: object\n",
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.4171068668365479\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 38.3\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.0441713333129883\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 35.6\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.2311501502990723\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 36.1\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.143058180809021\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 39.8\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 1.2052719593048096\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 37.1\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 1.1479912996292114\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 42.1\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.0620074272155762\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 41.9\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 0.9420973062515259\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 40.8\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 0.9963425993919373\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 42.6\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 1.1227309703826904\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 44.7\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 1.0966073274612427\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 45.3\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.9165311455726624\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 44.7\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.9403874278068542\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 47.1\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.8985233902931213\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 50.9\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 0.9453263878822327\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 48.3\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 1.0734436511993408\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 49.5\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 1.0227725505828857\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 50.1\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.908526599407196\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 49.5\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 0.8206273913383484\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 49.8\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 1.0770572423934937\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.9307624697685242\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 52.0\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 0.9988939166069031\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 53.9\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 0.9962031245231628\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 51.6\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 1.0146888494491577\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 52.9\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 0.8862332701683044\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 52.8\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 1.0853023529052734\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 1.0724843740463257\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 1.1474435329437256\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 52.0\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 0.9984208941459656\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 53.5\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.9887376427650452\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 0.8746212124824524\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 54.4\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 55.4\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.9232903718948364\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 53.4\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 1.047502875328064\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 50.8\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.973526120185852\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 53.2\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 1.141765832901001\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 55.2\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 1.0064010620117188\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 52.5\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.7851270437240601\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 55.4\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.837958812713623\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 53.9\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.874629557132721\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.8892581462860107\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 54.6\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.9683003425598145\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 53.9\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.7425669431686401\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 1.0130876302719116\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.8901498913764954\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.9268513917922974\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 53.9\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.9370170831680298\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.6699596047401428\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 54.0\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 0.8579763174057007\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 53.4\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 1.0226114988327026\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 55.4\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.7808858156204224\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 54.3\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 0.8396570682525635\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 56.1\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.8667938709259033\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 55.6\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 0.9507032632827759\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 0.7874652147293091\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 56.5\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.9393066763877869\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 57.0\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.8119447231292725\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 57.8\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.7943585515022278\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 57.7\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 0.8187298774719238\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 57.5\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.8119794726371765\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 57.4\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.8636269569396973\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 58.7\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.7901408672332764\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 59.5\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.9816270470619202\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 58.7\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 58.5\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.7788804173469543\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 58.4\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.8219171166419983\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 58.6\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.7617418766021729\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 59.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [401/3125], Training Loss: 1.1430144309997559\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 58.8\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.8388009667396545\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 59.6\n",
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.9113153219223022\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 58.7\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.9049214124679565\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 58.7\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.9867337942123413\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 58.7\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.8704606294631958\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 59.0\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.9325309991836548\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 59.2\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.845345139503479\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 59.5\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 1.0316963195800781\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 59.9\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.7994769215583801\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 58.5\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.7057408094406128\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 59.4\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 1.0027433633804321\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 59.4\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.7869382500648499\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 57.3\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.8384599685668945\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 59.0\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.779136598110199\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 59.7\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.8686658143997192\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 58.7\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.7500922679901123\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 59.3\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.9014884233474731\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 59.5\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.9054150581359863\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 60.1\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 1.0141853094100952\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 60.7\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.7472626566886902\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 60.3\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.835035502910614\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 60.1\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.9199557900428772\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 59.9\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.7950683236122131\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 60.4\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 1.0779112577438354\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 61.3\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.900574803352356\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.7409119009971619\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 61.1\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 1.0418158769607544\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 58.8\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.7955560684204102\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 60.8\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.8009843826293945\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 58.6\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.84770667552948\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 61.0\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.8454529047012329\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.715778112411499\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 60.7\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 0.9441700577735901\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.803901731967926\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 61.1\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.8925074338912964\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 59.3\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 1.0173507928848267\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.8547224998474121\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.8018085360527039\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 61.4\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.7474403381347656\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 61.7\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.7971341609954834\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 1.1154018640518188\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 60.2\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.8105341196060181\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 61.1\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.8040125370025635\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 60.4\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.9778720140457153\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 61.8\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.8025740385055542\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.734771192073822\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.7733954191207886\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 62.1\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.7838953137397766\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 62.4\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.7762234807014465\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.8783360719680786\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 62.8\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.7922726273536682\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.7861497402191162\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.8088440299034119\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.9228817820549011\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 62.4\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.6956614851951599\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.6844863891601562\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 62.9\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.787743330001831\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.9232056140899658\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.7699888944625854\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.7868385910987854\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.9759712815284729\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.8365308046340942\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 62.6\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.5959560871124268\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.7618985176086426\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.6456156969070435\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 62.8\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.7215867638587952\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 62.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.6167958378791809\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.870089590549469\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 63.2\n",
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.7834392189979553\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.982855498790741\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.8549488186836243\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.7748283743858337\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.6787644028663635\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.8384294509887695\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.6566789150238037\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.8681350350379944\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.8626158833503723\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.6244716048240662\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 65.9\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.7278220057487488\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 66.7\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.76670241355896\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.8990617394447327\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.6634209156036377\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.672057032585144\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.9550038576126099\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 64.7\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.7458641529083252\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.7694810032844543\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.8301029205322266\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.6849462389945984\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 66.2\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.856914222240448\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 64.0\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.7212820053100586\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 63.6\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.7968658804893494\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 63.8\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.5864254236221313\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 64.8\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.6515041589736938\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.7429429292678833\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.6866104602813721\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 67.5\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.6001747846603394\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.8587020635604858\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.5693615078926086\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.7108491063117981\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.9171442985534668\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.7039093971252441\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.871777355670929\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.8303249478340149\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.7284479141235352\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 64.6\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 0.7395760416984558\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.6766656637191772\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.7527486085891724\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.5962010622024536\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.7887782454490662\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.632575511932373\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.9202495813369751\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.8608814477920532\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.6804875135421753\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 65.0\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.8017921447753906\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 67.0\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.5829649567604065\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.9638192653656006\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.5970205664634705\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.4956744313240051\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 65.1\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.7792177796363831\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.6137647032737732\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.8360856771469116\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.7758819460868835\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.8682683110237122\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.9619364738464355\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.6385560631752014\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 65.9\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.7322520613670349\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.6817741394042969\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 66.6\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.5708772540092468\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.9540432691574097\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.7781676054000854\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.6767479777336121\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.8322429656982422\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.7354512214660645\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 67.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.7283430099487305\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.5376429557800293\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 66.2\n",
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.6461762189865112\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.6749982833862305\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.7948722243309021\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.5664923787117004\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.6454054713249207\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.8307227492332458\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 69.4\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.6857758164405823\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 69.7\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.8360754251480103\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.6524309515953064\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 65.9\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.7104584574699402\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.8193697929382324\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.6052377223968506\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 68.8\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.5656070709228516\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.9043994545936584\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.5677833557128906\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.7335218191146851\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.6419474482536316\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.5358983874320984\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 69.2\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.6341133713722229\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.8009127974510193\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.6633501052856445\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.609616756439209\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.809467613697052\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 70.3\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.5786259770393372\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 69.7\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.594315767288208\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.6544175744056702\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 68.2\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.5780084133148193\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 70.8\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.798999547958374\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 67.9\n"
     ]
    }
   ],
   "source": [
    "RNN_HIDDEN_SIZES = [200,512]\n",
    "INTERACT_TYPES = ['concat','mul']\n",
    "KERNEL_SIZES = [3,5]\n",
    "somelists = [RNN_HIDDEN_SIZES,INTERACT_TYPES]\n",
    "LIN_HIDDEN_SIZE = 256\n",
    "result = list(itertools.product(*somelists))\n",
    "df_param_rnn = pd.DataFrame(result,columns=['hidden_size','interaction_type'])\n",
    "df_param_rnn['train_loss_hist'] = None\n",
    "df_param_rnn['val_acc_hist'] = None\n",
    "df_param_rnn['max_val_acc'] = None\n",
    "df_param_rnn['max_val_acc_epoch'] = None\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "for param_i in range(len(df_param_rnn)):\n",
    "    print(df_param_rnn.iloc[param_i])\n",
    "    RNN_HIDDEN_SIZE = int(df_param_rnn.iloc[param_i]['hidden_size'])\n",
    "    INTERACT_TYPE = df_param_rnn.iloc[param_i]['interaction_type']\n",
    "    \n",
    "    model = RNN(hidden_size=RNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat, bidirectional = True).to(DEVICE)\n",
    "    classification_network = ClassificationNetwork(num_inputs=RNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=NUM_DIRECTIONS,interact_type=INTERACT_TYPE).to(DEVICE)\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    train_loss_hist = []\n",
    "    val_acc_hist = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "            x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "            outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "            outputs = classification_network(outputs_x1,outputs_x2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_hist.append(loss.item())\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(epoch+1,num_epochs,i+1,len(train_loader),loss.item()))\n",
    "                val_acc_hist.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                # validate\n",
    "        val_acc = test_model(val_loader, model,classification_network=classification_network)\n",
    "        val_acc_hist.append(val_acc)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                   epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "\n",
    "    val_acc_hist = np.array(val_acc_hist)\n",
    "    max_val_acc = np.max(val_acc_hist)\n",
    "    max_val_acc_epoch = np.argmax(val_acc_hist)\n",
    "    #df_param.set_value(i,'train_loss_hist',train_loss_hist)\n",
    "    df_param_rnn.at[param_i,'train_loss_hist'] = np.array(train_loss_hist)\n",
    "    \n",
    "    df_param_rnn.at[param_i,'val_acc_hist'] = val_acc_hist\n",
    "    df_param_rnn.at[param_i,'max_val_acc'] = max_val_acc\n",
    "    df_param_rnn.at[param_i,'max_val_acc_epoch'] = max_val_acc_epoch+1\n",
    "    print(max_val_acc)\n",
    "    print(max_val_acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(df_param_rnn,open('df_rnn_cv_correct.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refitting on the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 50.6\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 56.2\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 57.9\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 57.0\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 59.1\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 59.4\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 60.9\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 62.0\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 61.2\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 62.1\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 60.8\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 59.0\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 61.6\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 62.1\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 63.2\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 62.8\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 63.3\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 61.7\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 63.6\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 64.9\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 64.0\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 61.6\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 65.6\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 66.3\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 65.3\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 65.1\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 66.0\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 65.7\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 65.2\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 65.5\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 64.7\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 65.6\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 66.6\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 65.8\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 66.8\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 65.6\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 67.0\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 65.7\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 67.0\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 67.5\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 64.2\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 66.5\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 65.7\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 67.9\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 69.2\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 69.6\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 66.2\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 67.7\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 67.1\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 68.3\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 68.3\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 67.5\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 67.8\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 67.0\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 67.2\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 69.5\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 69.5\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 67.9\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 68.8\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 68.8\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 67.4\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 69.2\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 69.3\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 69.0\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 69.0\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 68.8\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 68.9\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 69.7\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 70.6\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 68.8\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 67.4\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 69.4\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 70.0\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 69.4\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 69.8\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 68.2\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 69.5\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 70.3\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 69.7\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 70.3\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 69.0\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 68.8\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 68.6\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 69.2\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 70.9\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 70.1\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 69.6\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 69.1\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 69.4\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 69.9\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 69.8\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 70.5\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 71.3\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 71.3\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 70.4\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 68.5\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 68.3\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 69.2\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 67.0\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 67.3\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 68.0\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 66.5\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 67.8\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 69.2\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 68.5\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 68.0\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 69.6\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 69.4\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 70.4\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 69.3\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 68.6\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 67.4\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 67.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 67.3\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 67.2\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 68.8\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 70.1\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 67.5\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 68.5\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 66.8\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 66.4\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 67.0\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 69.4\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 69.2\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 68.7\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 69.4\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 69.9\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 70.2\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 68.9\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 68.5\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 68.0\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 69.7\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 69.8\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 69.4\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 66.9\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 66.9\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 66.8\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 68.4\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 67.2\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 66.4\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 67.1\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 67.9\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 66.0\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 66.9\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 68.8\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 67.2\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 68.0\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 67.6\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 68.6\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 66.6\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 65.2\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 67.3\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 68.9\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 67.4\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 68.7\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 67.8\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 68.3\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 67.5\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 67.6\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 69.1\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 67.2\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 66.8\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 66.9\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 68.4\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 66.0\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 66.1\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 67.3\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 65.4\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 65.7\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 67.0\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 67.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 68.4\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 67.8\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 66.7\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 66.8\n",
      "71.3\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model,classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x1,x2,length_x1,length_x2,x1_mask,x2_mask,label in loader:\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = F.softmax(classification_network(outputs_x1,outputs_x2),dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "CNN_HIDDEN_SIZE = 512\n",
    "\n",
    "best_cnn_model = CNN(hidden_size=CNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat).to(DEVICE)\n",
    "classification_network = ClassificationNetwork(num_inputs=CNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=1,interact_type='concat').to(DEVICE)\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(best_cnn_model.parameters())+list(classification_network.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        best_cnn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_x1 = best_cnn_model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = best_cnn_model(x2,length_x2,x2_mask)\n",
    "        outputs = classification_network(outputs_x1,outputs_x2)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_hist.append(loss.item())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            val_acc = test_model(val_loader, best_cnn_model,classification_network=classification_network)\n",
    "            val_acc_hist.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            # validate\n",
    "    \n",
    "            \n",
    "\n",
    "val_acc_hist = np.array(val_acc_hist)\n",
    "max_val_acc = np.max(val_acc_hist)\n",
    "max_val_acc_epoch = np.argmax(val_acc_hist)\n",
    "print(max_val_acc)\n",
    "print(max_val_acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dicts_cnn = {\n",
    "    'embedding_network': best_cnn_model.state_dict(),\n",
    "    'classification_network': classification_network.state_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Training Loss: 1.1995611190795898\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 37.6\n",
      "Epoch: [1/10], Step: [201/3125], Training Loss: 1.1100894212722778\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 37.4\n",
      "Epoch: [1/10], Step: [301/3125], Training Loss: 1.0222305059432983\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 38.4\n",
      "Epoch: [1/10], Step: [401/3125], Training Loss: 1.0443081855773926\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 38.9\n",
      "Epoch: [1/10], Step: [501/3125], Training Loss: 1.1561411619186401\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 37.7\n",
      "Epoch: [1/10], Step: [601/3125], Training Loss: 0.9454556107521057\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 38.7\n",
      "Epoch: [1/10], Step: [701/3125], Training Loss: 1.0953350067138672\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 41.1\n",
      "Epoch: [1/10], Step: [801/3125], Training Loss: 1.1802767515182495\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 42.0\n",
      "Epoch: [1/10], Step: [901/3125], Training Loss: 1.0460729598999023\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 42.7\n",
      "Epoch: [1/10], Step: [1001/3125], Training Loss: 1.026662826538086\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 44.1\n",
      "Epoch: [1/10], Step: [1101/3125], Training Loss: 0.9988728761672974\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 43.2\n",
      "Epoch: [1/10], Step: [1201/3125], Training Loss: 0.9938322305679321\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 44.9\n",
      "Epoch: [1/10], Step: [1301/3125], Training Loss: 0.9833756685256958\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 44.8\n",
      "Epoch: [1/10], Step: [1401/3125], Training Loss: 0.9415769577026367\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 46.8\n",
      "Epoch: [1/10], Step: [1501/3125], Training Loss: 1.0581355094909668\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 47.3\n",
      "Epoch: [1/10], Step: [1601/3125], Training Loss: 1.0243324041366577\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 48.3\n",
      "Epoch: [1/10], Step: [1701/3125], Training Loss: 0.9148005843162537\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 48.5\n",
      "Epoch: [1/10], Step: [1801/3125], Training Loss: 0.9319711327552795\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 48.8\n",
      "Epoch: [1/10], Step: [1901/3125], Training Loss: 1.0466331243515015\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 47.1\n",
      "Epoch: [1/10], Step: [2001/3125], Training Loss: 1.048247218132019\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 50.4\n",
      "Epoch: [1/10], Step: [2101/3125], Training Loss: 0.8838427066802979\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 51.9\n",
      "Epoch: [1/10], Step: [2201/3125], Training Loss: 1.0315513610839844\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 52.3\n",
      "Epoch: [1/10], Step: [2301/3125], Training Loss: 1.104310154914856\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 52.7\n",
      "Epoch: [1/10], Step: [2401/3125], Training Loss: 0.9215936660766602\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 51.7\n",
      "Epoch: [1/10], Step: [2501/3125], Training Loss: 1.0740798711776733\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 51.8\n",
      "Epoch: [1/10], Step: [2601/3125], Training Loss: 0.9598546028137207\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 51.9\n",
      "Epoch: [1/10], Step: [2701/3125], Training Loss: 1.0044208765029907\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 52.6\n",
      "Epoch: [1/10], Step: [2801/3125], Training Loss: 1.1792913675308228\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 49.1\n",
      "Epoch: [1/10], Step: [2901/3125], Training Loss: 1.0124086141586304\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 53.4\n",
      "Epoch: [1/10], Step: [3001/3125], Training Loss: 0.828241765499115\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 52.4\n",
      "Epoch: [1/10], Step: [3101/3125], Training Loss: 1.1390715837478638\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 53.3\n",
      "Epoch: [1/10], Step: [3125/3125], Validation Acc: 53.0\n",
      "Epoch: [2/10], Step: [101/3125], Training Loss: 0.8905879855155945\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 53.7\n",
      "Epoch: [2/10], Step: [201/3125], Training Loss: 0.9290416836738586\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 55.7\n",
      "Epoch: [2/10], Step: [301/3125], Training Loss: 0.9685216546058655\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 54.6\n",
      "Epoch: [2/10], Step: [401/3125], Training Loss: 0.9706236720085144\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 52.7\n",
      "Epoch: [2/10], Step: [501/3125], Training Loss: 0.9437462091445923\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [601/3125], Training Loss: 0.9634215235710144\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [701/3125], Training Loss: 0.8717381358146667\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 51.9\n",
      "Epoch: [2/10], Step: [801/3125], Training Loss: 0.9017841219902039\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 54.9\n",
      "Epoch: [2/10], Step: [901/3125], Training Loss: 0.934329628944397\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 55.0\n",
      "Epoch: [2/10], Step: [1001/3125], Training Loss: 0.8795228004455566\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 53.5\n",
      "Epoch: [2/10], Step: [1101/3125], Training Loss: 0.6825541853904724\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 54.4\n",
      "Epoch: [2/10], Step: [1201/3125], Training Loss: 0.8519583940505981\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 53.5\n",
      "Epoch: [2/10], Step: [1301/3125], Training Loss: 0.7620090246200562\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 56.1\n",
      "Epoch: [2/10], Step: [1401/3125], Training Loss: 0.8084999322891235\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 55.1\n",
      "Epoch: [2/10], Step: [1501/3125], Training Loss: 0.8231768012046814\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 54.1\n",
      "Epoch: [2/10], Step: [1601/3125], Training Loss: 0.938345193862915\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 54.7\n",
      "Epoch: [2/10], Step: [1701/3125], Training Loss: 1.1185779571533203\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 55.8\n",
      "Epoch: [2/10], Step: [1801/3125], Training Loss: 0.9862683415412903\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 56.3\n",
      "Epoch: [2/10], Step: [1901/3125], Training Loss: 0.8064968585968018\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 56.1\n",
      "Epoch: [2/10], Step: [2001/3125], Training Loss: 1.0541678667068481\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 55.6\n",
      "Epoch: [2/10], Step: [2101/3125], Training Loss: 0.9642474055290222\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 57.4\n",
      "Epoch: [2/10], Step: [2201/3125], Training Loss: 1.0738517045974731\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 55.6\n",
      "Epoch: [2/10], Step: [2301/3125], Training Loss: 1.0226988792419434\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 53.6\n",
      "Epoch: [2/10], Step: [2401/3125], Training Loss: 0.9022632837295532\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 57.2\n",
      "Epoch: [2/10], Step: [2501/3125], Training Loss: 0.9933794140815735\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 54.5\n",
      "Epoch: [2/10], Step: [2601/3125], Training Loss: 0.9098056554794312\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 55.7\n",
      "Epoch: [2/10], Step: [2701/3125], Training Loss: 1.097651720046997\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 55.8\n",
      "Epoch: [2/10], Step: [2801/3125], Training Loss: 0.7940296530723572\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 57.1\n",
      "Epoch: [2/10], Step: [2901/3125], Training Loss: 0.9140162467956543\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 57.3\n",
      "Epoch: [2/10], Step: [3001/3125], Training Loss: 0.9123303294181824\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 57.3\n",
      "Epoch: [2/10], Step: [3101/3125], Training Loss: 0.9640801548957825\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 55.4\n",
      "Epoch: [2/10], Step: [3125/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [101/3125], Training Loss: 0.7851980328559875\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [201/3125], Training Loss: 0.9075306057929993\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 55.5\n",
      "Epoch: [3/10], Step: [301/3125], Training Loss: 0.8484281897544861\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 55.1\n",
      "Epoch: [3/10], Step: [401/3125], Training Loss: 0.898023247718811\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 57.0\n",
      "Epoch: [3/10], Step: [501/3125], Training Loss: 0.863684356212616\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 60.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/10], Step: [601/3125], Training Loss: 0.8916212320327759\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 57.1\n",
      "Epoch: [3/10], Step: [701/3125], Training Loss: 0.8576177954673767\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 58.3\n",
      "Epoch: [3/10], Step: [801/3125], Training Loss: 0.6413313150405884\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 58.8\n",
      "Epoch: [3/10], Step: [901/3125], Training Loss: 0.8267204761505127\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 57.5\n",
      "Epoch: [3/10], Step: [1001/3125], Training Loss: 0.7518324255943298\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 59.5\n",
      "Epoch: [3/10], Step: [1101/3125], Training Loss: 0.9723003506660461\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 56.9\n",
      "Epoch: [3/10], Step: [1201/3125], Training Loss: 0.7372439503669739\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 57.8\n",
      "Epoch: [3/10], Step: [1301/3125], Training Loss: 0.9425835013389587\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 58.6\n",
      "Epoch: [3/10], Step: [1401/3125], Training Loss: 0.8530013561248779\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 58.4\n",
      "Epoch: [3/10], Step: [1501/3125], Training Loss: 0.8145497441291809\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 57.0\n",
      "Epoch: [3/10], Step: [1601/3125], Training Loss: 0.9182418584823608\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 57.9\n",
      "Epoch: [3/10], Step: [1701/3125], Training Loss: 0.8845045566558838\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 61.0\n",
      "Epoch: [3/10], Step: [1801/3125], Training Loss: 0.8240212798118591\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 61.2\n",
      "Epoch: [3/10], Step: [1901/3125], Training Loss: 0.8678100109100342\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 57.2\n",
      "Epoch: [3/10], Step: [2001/3125], Training Loss: 0.8325749039649963\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 59.9\n",
      "Epoch: [3/10], Step: [2101/3125], Training Loss: 0.7764977812767029\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 60.7\n",
      "Epoch: [3/10], Step: [2201/3125], Training Loss: 0.9396049380302429\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 60.5\n",
      "Epoch: [3/10], Step: [2301/3125], Training Loss: 0.9428114891052246\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 61.7\n",
      "Epoch: [3/10], Step: [2401/3125], Training Loss: 0.9914459586143494\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 59.3\n",
      "Epoch: [3/10], Step: [2501/3125], Training Loss: 0.6779068112373352\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 61.6\n",
      "Epoch: [3/10], Step: [2601/3125], Training Loss: 0.917716383934021\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [2701/3125], Training Loss: 0.7525437474250793\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 62.9\n",
      "Epoch: [3/10], Step: [2801/3125], Training Loss: 1.0284819602966309\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 61.3\n",
      "Epoch: [3/10], Step: [2901/3125], Training Loss: 0.6843025088310242\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 61.8\n",
      "Epoch: [3/10], Step: [3001/3125], Training Loss: 0.9730829000473022\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 63.3\n",
      "Epoch: [3/10], Step: [3101/3125], Training Loss: 0.6324224472045898\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 61.3\n",
      "Epoch: [3/10], Step: [3125/3125], Validation Acc: 61.3\n",
      "Epoch: [4/10], Step: [101/3125], Training Loss: 0.648061990737915\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 63.5\n",
      "Epoch: [4/10], Step: [201/3125], Training Loss: 0.8380857706069946\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 63.4\n",
      "Epoch: [4/10], Step: [301/3125], Training Loss: 0.784365713596344\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 62.5\n",
      "Epoch: [4/10], Step: [401/3125], Training Loss: 0.6880674958229065\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [501/3125], Training Loss: 0.8483243584632874\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 61.4\n",
      "Epoch: [4/10], Step: [601/3125], Training Loss: 1.1330368518829346\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 62.1\n",
      "Epoch: [4/10], Step: [701/3125], Training Loss: 0.9339157342910767\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [801/3125], Training Loss: 0.8344167470932007\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [901/3125], Training Loss: 0.7165783643722534\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [1001/3125], Training Loss: 0.6175284385681152\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 60.1\n",
      "Epoch: [4/10], Step: [1101/3125], Training Loss: 0.8222144246101379\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [1201/3125], Training Loss: 0.7034990787506104\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [1301/3125], Training Loss: 0.8753055930137634\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 62.9\n",
      "Epoch: [4/10], Step: [1401/3125], Training Loss: 0.8647952675819397\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 62.7\n",
      "Epoch: [4/10], Step: [1501/3125], Training Loss: 0.6451159715652466\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 61.4\n",
      "Epoch: [4/10], Step: [1601/3125], Training Loss: 0.6514217853546143\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 62.9\n",
      "Epoch: [4/10], Step: [1701/3125], Training Loss: 0.8741819262504578\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 62.2\n",
      "Epoch: [4/10], Step: [1801/3125], Training Loss: 0.9187807440757751\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [1901/3125], Training Loss: 0.7842385768890381\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 64.0\n",
      "Epoch: [4/10], Step: [2001/3125], Training Loss: 0.7686618566513062\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 62.5\n",
      "Epoch: [4/10], Step: [2101/3125], Training Loss: 0.9582570791244507\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [2201/3125], Training Loss: 0.9616758227348328\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 61.6\n",
      "Epoch: [4/10], Step: [2301/3125], Training Loss: 0.816309928894043\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 63.1\n",
      "Epoch: [4/10], Step: [2401/3125], Training Loss: 0.7463990449905396\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 61.9\n",
      "Epoch: [4/10], Step: [2501/3125], Training Loss: 0.5509339570999146\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [2601/3125], Training Loss: 0.937086820602417\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [2701/3125], Training Loss: 0.7332866787910461\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 63.0\n",
      "Epoch: [4/10], Step: [2801/3125], Training Loss: 0.806435227394104\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 62.6\n",
      "Epoch: [4/10], Step: [2901/3125], Training Loss: 0.8335981369018555\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [3001/3125], Training Loss: 0.8205819129943848\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 63.2\n",
      "Epoch: [4/10], Step: [3101/3125], Training Loss: 0.8541887402534485\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 64.1\n",
      "Epoch: [4/10], Step: [3125/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [101/3125], Training Loss: 0.7846415638923645\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [201/3125], Training Loss: 0.9434962272644043\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [301/3125], Training Loss: 0.690544605255127\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 63.7\n",
      "Epoch: [5/10], Step: [401/3125], Training Loss: 0.7393637895584106\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 63.1\n",
      "Epoch: [5/10], Step: [501/3125], Training Loss: 0.8173755407333374\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 63.3\n",
      "Epoch: [5/10], Step: [601/3125], Training Loss: 0.7043222784996033\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [701/3125], Training Loss: 0.7845461368560791\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 63.5\n",
      "Epoch: [5/10], Step: [801/3125], Training Loss: 0.7529712319374084\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [901/3125], Training Loss: 0.9908238649368286\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [1001/3125], Training Loss: 0.9212733507156372\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 62.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [1101/3125], Training Loss: 0.9302197694778442\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [1201/3125], Training Loss: 0.7532354593276978\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 65.0\n",
      "Epoch: [5/10], Step: [1301/3125], Training Loss: 0.7656674981117249\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [1401/3125], Training Loss: 0.7302685379981995\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [1501/3125], Training Loss: 0.6648122668266296\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 63.9\n",
      "Epoch: [5/10], Step: [1601/3125], Training Loss: 0.6083792448043823\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 64.0\n",
      "Epoch: [5/10], Step: [1701/3125], Training Loss: 0.8468359708786011\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [1801/3125], Training Loss: 0.896939754486084\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 63.6\n",
      "Epoch: [5/10], Step: [1901/3125], Training Loss: 0.7192823886871338\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 66.1\n",
      "Epoch: [5/10], Step: [2001/3125], Training Loss: 0.9018048048019409\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 65.2\n",
      "Epoch: [5/10], Step: [2101/3125], Training Loss: 0.8154064416885376\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 64.2\n",
      "Epoch: [5/10], Step: [2201/3125], Training Loss: 0.6804164052009583\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2301/3125], Training Loss: 0.7021273374557495\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 64.3\n",
      "Epoch: [5/10], Step: [2401/3125], Training Loss: 0.7970207333564758\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [2501/3125], Training Loss: 0.7503057718276978\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 64.5\n",
      "Epoch: [5/10], Step: [2601/3125], Training Loss: 0.5497094988822937\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2701/3125], Training Loss: 0.6828689575195312\n",
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [2801/3125], Training Loss: 0.804843544960022\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 64.8\n",
      "Epoch: [5/10], Step: [2901/3125], Training Loss: 0.7041116952896118\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 65.4\n",
      "Epoch: [5/10], Step: [3001/3125], Training Loss: 0.789949893951416\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 65.6\n",
      "Epoch: [5/10], Step: [3101/3125], Training Loss: 0.7491825222969055\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [3125/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [101/3125], Training Loss: 0.7033717632293701\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 65.3\n",
      "Epoch: [6/10], Step: [201/3125], Training Loss: 0.6564381718635559\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [301/3125], Training Loss: 0.781783938407898\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 66.2\n",
      "Epoch: [6/10], Step: [401/3125], Training Loss: 0.8072962164878845\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 64.9\n",
      "Epoch: [6/10], Step: [501/3125], Training Loss: 0.6699519753456116\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 65.6\n",
      "Epoch: [6/10], Step: [601/3125], Training Loss: 0.6310651898384094\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [701/3125], Training Loss: 0.7559283375740051\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [801/3125], Training Loss: 0.6256413459777832\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 66.4\n",
      "Epoch: [6/10], Step: [901/3125], Training Loss: 0.575259804725647\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [1001/3125], Training Loss: 0.833411693572998\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [1101/3125], Training Loss: 0.8055464625358582\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1201/3125], Training Loss: 0.6730772256851196\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [1301/3125], Training Loss: 0.8213016986846924\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [1401/3125], Training Loss: 0.6981769800186157\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1501/3125], Training Loss: 0.8417980074882507\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 67.4\n",
      "Epoch: [6/10], Step: [1601/3125], Training Loss: 1.0588958263397217\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 66.0\n",
      "Epoch: [6/10], Step: [1701/3125], Training Loss: 0.8502658009529114\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [1801/3125], Training Loss: 0.9309647083282471\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 65.9\n",
      "Epoch: [6/10], Step: [1901/3125], Training Loss: 0.8418020009994507\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [2001/3125], Training Loss: 0.7766615152359009\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 67.7\n",
      "Epoch: [6/10], Step: [2101/3125], Training Loss: 0.8392701745033264\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [2201/3125], Training Loss: 0.8850737810134888\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 67.5\n",
      "Epoch: [6/10], Step: [2301/3125], Training Loss: 0.6444165706634521\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 67.6\n",
      "Epoch: [6/10], Step: [2401/3125], Training Loss: 0.9520681500434875\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 66.5\n",
      "Epoch: [6/10], Step: [2501/3125], Training Loss: 0.8405159115791321\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [2601/3125], Training Loss: 0.678885817527771\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [2701/3125], Training Loss: 0.6846542358398438\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [2801/3125], Training Loss: 0.8606518507003784\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [2901/3125], Training Loss: 0.8707857131958008\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [3001/3125], Training Loss: 0.6256257891654968\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 68.6\n",
      "Epoch: [6/10], Step: [3101/3125], Training Loss: 0.5290624499320984\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 67.2\n",
      "Epoch: [6/10], Step: [3125/3125], Validation Acc: 66.3\n",
      "Epoch: [7/10], Step: [101/3125], Training Loss: 0.7475182414054871\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [201/3125], Training Loss: 0.6443029046058655\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [301/3125], Training Loss: 0.5677659511566162\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [401/3125], Training Loss: 0.7144067287445068\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 69.2\n",
      "Epoch: [7/10], Step: [501/3125], Training Loss: 0.8952122330665588\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [601/3125], Training Loss: 0.668269693851471\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [701/3125], Training Loss: 0.8181119561195374\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [801/3125], Training Loss: 0.8210365772247314\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 67.3\n",
      "Epoch: [7/10], Step: [901/3125], Training Loss: 0.9381013512611389\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [1001/3125], Training Loss: 0.6714085340499878\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [1101/3125], Training Loss: 0.5223870277404785\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 69.0\n",
      "Epoch: [7/10], Step: [1201/3125], Training Loss: 0.701823890209198\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [1301/3125], Training Loss: 0.7017317414283752\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [1401/3125], Training Loss: 0.7319447994232178\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 68.9\n",
      "Epoch: [7/10], Step: [1501/3125], Training Loss: 0.6608896851539612\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 69.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7/10], Step: [1601/3125], Training Loss: 0.6373110413551331\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 69.1\n",
      "Epoch: [7/10], Step: [1701/3125], Training Loss: 0.7456009984016418\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [1801/3125], Training Loss: 0.6611131429672241\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 69.5\n",
      "Epoch: [7/10], Step: [1901/3125], Training Loss: 0.7957515716552734\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [2001/3125], Training Loss: 0.7345466613769531\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 68.3\n",
      "Epoch: [7/10], Step: [2101/3125], Training Loss: 0.6303751468658447\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [2201/3125], Training Loss: 0.6335746645927429\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [2301/3125], Training Loss: 0.7813224792480469\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [2401/3125], Training Loss: 0.6629168391227722\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 70.1\n",
      "Epoch: [7/10], Step: [2501/3125], Training Loss: 0.7319347262382507\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [2601/3125], Training Loss: 0.6824117302894592\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [2701/3125], Training Loss: 0.7748427391052246\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 70.2\n",
      "Epoch: [7/10], Step: [2801/3125], Training Loss: 0.8553587198257446\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 69.9\n",
      "Epoch: [7/10], Step: [2901/3125], Training Loss: 0.6510311365127563\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 70.0\n",
      "Epoch: [7/10], Step: [3001/3125], Training Loss: 0.6226828098297119\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [3101/3125], Training Loss: 0.9464101791381836\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 69.3\n",
      "Epoch: [7/10], Step: [3125/3125], Validation Acc: 68.6\n",
      "Epoch: [8/10], Step: [101/3125], Training Loss: 0.6823558807373047\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 69.4\n",
      "Epoch: [8/10], Step: [201/3125], Training Loss: 0.8044520020484924\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [301/3125], Training Loss: 0.47262513637542725\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 67.5\n",
      "Epoch: [8/10], Step: [401/3125], Training Loss: 0.45948854088783264\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [501/3125], Training Loss: 0.6488707065582275\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 69.9\n",
      "Epoch: [8/10], Step: [601/3125], Training Loss: 0.5495980381965637\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 70.0\n",
      "Epoch: [8/10], Step: [701/3125], Training Loss: 0.8316308259963989\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 70.5\n",
      "Epoch: [8/10], Step: [801/3125], Training Loss: 0.604185163974762\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [901/3125], Training Loss: 0.8262978792190552\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 71.4\n",
      "Epoch: [8/10], Step: [1001/3125], Training Loss: 0.7471675872802734\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 69.9\n",
      "Epoch: [8/10], Step: [1101/3125], Training Loss: 0.722307026386261\n",
      "Epoch: [8/10], Step: [1101/3125], Validation Acc: 71.7\n",
      "Epoch: [8/10], Step: [1201/3125], Training Loss: 0.8246108293533325\n",
      "Epoch: [8/10], Step: [1201/3125], Validation Acc: 70.2\n",
      "Epoch: [8/10], Step: [1301/3125], Training Loss: 0.5441799163818359\n",
      "Epoch: [8/10], Step: [1301/3125], Validation Acc: 69.5\n",
      "Epoch: [8/10], Step: [1401/3125], Training Loss: 1.0010647773742676\n",
      "Epoch: [8/10], Step: [1401/3125], Validation Acc: 70.7\n",
      "Epoch: [8/10], Step: [1501/3125], Training Loss: 0.9006257057189941\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 70.9\n",
      "Epoch: [8/10], Step: [1601/3125], Training Loss: 0.49837401509284973\n",
      "Epoch: [8/10], Step: [1601/3125], Validation Acc: 71.0\n",
      "Epoch: [8/10], Step: [1701/3125], Training Loss: 0.6407197117805481\n",
      "Epoch: [8/10], Step: [1701/3125], Validation Acc: 70.4\n",
      "Epoch: [8/10], Step: [1801/3125], Training Loss: 0.48857229948043823\n",
      "Epoch: [8/10], Step: [1801/3125], Validation Acc: 70.0\n",
      "Epoch: [8/10], Step: [1901/3125], Training Loss: 0.6476386785507202\n",
      "Epoch: [8/10], Step: [1901/3125], Validation Acc: 70.7\n",
      "Epoch: [8/10], Step: [2001/3125], Training Loss: 0.8500445485115051\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 69.7\n",
      "Epoch: [8/10], Step: [2101/3125], Training Loss: 0.5216463208198547\n",
      "Epoch: [8/10], Step: [2101/3125], Validation Acc: 70.3\n",
      "Epoch: [8/10], Step: [2201/3125], Training Loss: 0.6933960914611816\n",
      "Epoch: [8/10], Step: [2201/3125], Validation Acc: 70.1\n",
      "Epoch: [8/10], Step: [2301/3125], Training Loss: 0.7063809633255005\n",
      "Epoch: [8/10], Step: [2301/3125], Validation Acc: 71.4\n",
      "Epoch: [8/10], Step: [2401/3125], Training Loss: 0.6091426610946655\n",
      "Epoch: [8/10], Step: [2401/3125], Validation Acc: 70.4\n",
      "Epoch: [8/10], Step: [2501/3125], Training Loss: 0.6438820958137512\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 71.2\n",
      "Epoch: [8/10], Step: [2601/3125], Training Loss: 0.5834913849830627\n",
      "Epoch: [8/10], Step: [2601/3125], Validation Acc: 69.4\n",
      "Epoch: [8/10], Step: [2701/3125], Training Loss: 0.6173500418663025\n",
      "Epoch: [8/10], Step: [2701/3125], Validation Acc: 71.5\n",
      "Epoch: [8/10], Step: [2801/3125], Training Loss: 0.7412534356117249\n",
      "Epoch: [8/10], Step: [2801/3125], Validation Acc: 70.5\n",
      "Epoch: [8/10], Step: [2901/3125], Training Loss: 0.6157843470573425\n",
      "Epoch: [8/10], Step: [2901/3125], Validation Acc: 70.7\n",
      "Epoch: [8/10], Step: [3001/3125], Training Loss: 0.5838068723678589\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 70.6\n",
      "Epoch: [8/10], Step: [3101/3125], Training Loss: 0.7657738924026489\n",
      "Epoch: [8/10], Step: [3101/3125], Validation Acc: 69.7\n",
      "Epoch: [8/10], Step: [3125/3125], Validation Acc: 71.1\n",
      "Epoch: [9/10], Step: [101/3125], Training Loss: 0.46013349294662476\n",
      "Epoch: [9/10], Step: [101/3125], Validation Acc: 70.4\n",
      "Epoch: [9/10], Step: [201/3125], Training Loss: 0.9596469402313232\n",
      "Epoch: [9/10], Step: [201/3125], Validation Acc: 70.3\n",
      "Epoch: [9/10], Step: [301/3125], Training Loss: 0.625747799873352\n",
      "Epoch: [9/10], Step: [301/3125], Validation Acc: 70.7\n",
      "Epoch: [9/10], Step: [401/3125], Training Loss: 1.1441190242767334\n",
      "Epoch: [9/10], Step: [401/3125], Validation Acc: 71.0\n",
      "Epoch: [9/10], Step: [501/3125], Training Loss: 0.6604305505752563\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 71.2\n",
      "Epoch: [9/10], Step: [601/3125], Training Loss: 0.6974055767059326\n",
      "Epoch: [9/10], Step: [601/3125], Validation Acc: 72.2\n",
      "Epoch: [9/10], Step: [701/3125], Training Loss: 0.7756686210632324\n",
      "Epoch: [9/10], Step: [701/3125], Validation Acc: 70.3\n",
      "Epoch: [9/10], Step: [801/3125], Training Loss: 0.5858110785484314\n",
      "Epoch: [9/10], Step: [801/3125], Validation Acc: 70.5\n",
      "Epoch: [9/10], Step: [901/3125], Training Loss: 0.7487635016441345\n",
      "Epoch: [9/10], Step: [901/3125], Validation Acc: 71.3\n",
      "Epoch: [9/10], Step: [1001/3125], Training Loss: 0.637927770614624\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 69.9\n",
      "Epoch: [9/10], Step: [1101/3125], Training Loss: 0.8919788599014282\n",
      "Epoch: [9/10], Step: [1101/3125], Validation Acc: 69.6\n",
      "Epoch: [9/10], Step: [1201/3125], Training Loss: 0.560748279094696\n",
      "Epoch: [9/10], Step: [1201/3125], Validation Acc: 70.3\n",
      "Epoch: [9/10], Step: [1301/3125], Training Loss: 0.749193549156189\n",
      "Epoch: [9/10], Step: [1301/3125], Validation Acc: 69.5\n",
      "Epoch: [9/10], Step: [1401/3125], Training Loss: 0.5647457838058472\n",
      "Epoch: [9/10], Step: [1401/3125], Validation Acc: 71.0\n",
      "Epoch: [9/10], Step: [1501/3125], Training Loss: 0.47663402557373047\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 71.5\n",
      "Epoch: [9/10], Step: [1601/3125], Training Loss: 0.7363229990005493\n",
      "Epoch: [9/10], Step: [1601/3125], Validation Acc: 71.4\n",
      "Epoch: [9/10], Step: [1701/3125], Training Loss: 0.890855073928833\n",
      "Epoch: [9/10], Step: [1701/3125], Validation Acc: 70.1\n",
      "Epoch: [9/10], Step: [1801/3125], Training Loss: 0.6328812837600708\n",
      "Epoch: [9/10], Step: [1801/3125], Validation Acc: 69.6\n",
      "Epoch: [9/10], Step: [1901/3125], Training Loss: 0.45911723375320435\n",
      "Epoch: [9/10], Step: [1901/3125], Validation Acc: 71.3\n",
      "Epoch: [9/10], Step: [2001/3125], Training Loss: 0.6590282917022705\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 69.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9/10], Step: [2101/3125], Training Loss: 0.5182740688323975\n",
      "Epoch: [9/10], Step: [2101/3125], Validation Acc: 71.8\n",
      "Epoch: [9/10], Step: [2201/3125], Training Loss: 0.5161539912223816\n",
      "Epoch: [9/10], Step: [2201/3125], Validation Acc: 69.8\n",
      "Epoch: [9/10], Step: [2301/3125], Training Loss: 0.6029080152511597\n",
      "Epoch: [9/10], Step: [2301/3125], Validation Acc: 72.5\n",
      "Epoch: [9/10], Step: [2401/3125], Training Loss: 0.7643371224403381\n",
      "Epoch: [9/10], Step: [2401/3125], Validation Acc: 70.4\n",
      "Epoch: [9/10], Step: [2501/3125], Training Loss: 0.5227996110916138\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 70.5\n",
      "Epoch: [9/10], Step: [2601/3125], Training Loss: 0.6224384307861328\n",
      "Epoch: [9/10], Step: [2601/3125], Validation Acc: 70.8\n",
      "Epoch: [9/10], Step: [2701/3125], Training Loss: 0.6552159786224365\n",
      "Epoch: [9/10], Step: [2701/3125], Validation Acc: 69.9\n",
      "Epoch: [9/10], Step: [2801/3125], Training Loss: 0.7933003306388855\n",
      "Epoch: [9/10], Step: [2801/3125], Validation Acc: 72.2\n",
      "Epoch: [9/10], Step: [2901/3125], Training Loss: 0.7495298385620117\n",
      "Epoch: [9/10], Step: [2901/3125], Validation Acc: 71.6\n",
      "Epoch: [9/10], Step: [3001/3125], Training Loss: 0.6073340773582458\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 72.0\n",
      "Epoch: [9/10], Step: [3101/3125], Training Loss: 0.5885307192802429\n",
      "Epoch: [9/10], Step: [3101/3125], Validation Acc: 72.0\n",
      "Epoch: [9/10], Step: [3125/3125], Validation Acc: 71.3\n",
      "Epoch: [10/10], Step: [101/3125], Training Loss: 0.4830646514892578\n",
      "Epoch: [10/10], Step: [101/3125], Validation Acc: 71.7\n",
      "Epoch: [10/10], Step: [201/3125], Training Loss: 0.8107175230979919\n",
      "Epoch: [10/10], Step: [201/3125], Validation Acc: 71.0\n",
      "Epoch: [10/10], Step: [301/3125], Training Loss: 0.8063649535179138\n",
      "Epoch: [10/10], Step: [301/3125], Validation Acc: 71.1\n",
      "Epoch: [10/10], Step: [401/3125], Training Loss: 0.3728921413421631\n",
      "Epoch: [10/10], Step: [401/3125], Validation Acc: 71.4\n",
      "Epoch: [10/10], Step: [501/3125], Training Loss: 0.7439448833465576\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 71.6\n",
      "Epoch: [10/10], Step: [601/3125], Training Loss: 0.7187772393226624\n",
      "Epoch: [10/10], Step: [601/3125], Validation Acc: 71.4\n",
      "Epoch: [10/10], Step: [701/3125], Training Loss: 0.7589698433876038\n",
      "Epoch: [10/10], Step: [701/3125], Validation Acc: 70.8\n",
      "Epoch: [10/10], Step: [801/3125], Training Loss: 0.6696374416351318\n",
      "Epoch: [10/10], Step: [801/3125], Validation Acc: 71.7\n",
      "Epoch: [10/10], Step: [901/3125], Training Loss: 0.6303595304489136\n",
      "Epoch: [10/10], Step: [901/3125], Validation Acc: 70.7\n",
      "Epoch: [10/10], Step: [1001/3125], Training Loss: 0.8813392519950867\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 70.1\n",
      "Epoch: [10/10], Step: [1101/3125], Training Loss: 0.6715788245201111\n",
      "Epoch: [10/10], Step: [1101/3125], Validation Acc: 70.3\n",
      "Epoch: [10/10], Step: [1201/3125], Training Loss: 0.7754335403442383\n",
      "Epoch: [10/10], Step: [1201/3125], Validation Acc: 70.3\n",
      "Epoch: [10/10], Step: [1301/3125], Training Loss: 0.8881557583808899\n",
      "Epoch: [10/10], Step: [1301/3125], Validation Acc: 71.7\n",
      "Epoch: [10/10], Step: [1401/3125], Training Loss: 0.5160120725631714\n",
      "Epoch: [10/10], Step: [1401/3125], Validation Acc: 71.3\n",
      "Epoch: [10/10], Step: [1501/3125], Training Loss: 0.5911683440208435\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [1601/3125], Training Loss: 0.5650759339332581\n",
      "Epoch: [10/10], Step: [1601/3125], Validation Acc: 70.6\n",
      "Epoch: [10/10], Step: [1701/3125], Training Loss: 0.7649050951004028\n",
      "Epoch: [10/10], Step: [1701/3125], Validation Acc: 69.9\n",
      "Epoch: [10/10], Step: [1801/3125], Training Loss: 0.6212729215621948\n",
      "Epoch: [10/10], Step: [1801/3125], Validation Acc: 69.1\n",
      "Epoch: [10/10], Step: [1901/3125], Training Loss: 0.3750684857368469\n",
      "Epoch: [10/10], Step: [1901/3125], Validation Acc: 72.1\n",
      "Epoch: [10/10], Step: [2001/3125], Training Loss: 0.6814126968383789\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 71.0\n",
      "Epoch: [10/10], Step: [2101/3125], Training Loss: 0.6677287817001343\n",
      "Epoch: [10/10], Step: [2101/3125], Validation Acc: 69.6\n",
      "Epoch: [10/10], Step: [2201/3125], Training Loss: 0.5435558557510376\n",
      "Epoch: [10/10], Step: [2201/3125], Validation Acc: 69.9\n",
      "Epoch: [10/10], Step: [2301/3125], Training Loss: 0.558901846408844\n",
      "Epoch: [10/10], Step: [2301/3125], Validation Acc: 68.8\n",
      "Epoch: [10/10], Step: [2401/3125], Training Loss: 0.6743214726448059\n",
      "Epoch: [10/10], Step: [2401/3125], Validation Acc: 70.4\n",
      "Epoch: [10/10], Step: [2501/3125], Training Loss: 0.6071290969848633\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 70.6\n",
      "Epoch: [10/10], Step: [2601/3125], Training Loss: 0.7306950688362122\n",
      "Epoch: [10/10], Step: [2601/3125], Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [2701/3125], Training Loss: 0.532691240310669\n",
      "Epoch: [10/10], Step: [2701/3125], Validation Acc: 70.4\n",
      "Epoch: [10/10], Step: [2801/3125], Training Loss: 0.7202917337417603\n",
      "Epoch: [10/10], Step: [2801/3125], Validation Acc: 71.3\n",
      "Epoch: [10/10], Step: [2901/3125], Training Loss: 0.5353025794029236\n",
      "Epoch: [10/10], Step: [2901/3125], Validation Acc: 71.0\n",
      "Epoch: [10/10], Step: [3001/3125], Training Loss: 0.5535668134689331\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [3101/3125], Training Loss: 0.5460687279701233\n",
      "Epoch: [10/10], Step: [3101/3125], Validation Acc: 71.8\n",
      "Epoch: [10/10], Step: [3125/3125], Validation Acc: 71.1\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model,classification_network):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    classification_network.eval()\n",
    "    for x1,x2,length_x1,length_x2,x1_mask,x2_mask,label in loader:\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        outputs_x1 = model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = model(x2,length_x2,x2_mask)\n",
    "        outputs = F.softmax(classification_network(outputs_x1,outputs_x2),dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "RNN_HIDDEN_SIZE = 512\n",
    "INTERACT_TYPE = 'concat'\n",
    "\n",
    "best_rnn_model = RNN(hidden_size=RNN_HIDDEN_SIZE, num_layers=1, vocab_size=len(token2id),weights=weights_mat, bidirectional = True).to(DEVICE)\n",
    "classification_network_rnn = ClassificationNetwork(num_inputs=RNN_HIDDEN_SIZE, hidden_size=LIN_HIDDEN_SIZE, num_outputs=NUM_CLASSES,num_directions=NUM_DIRECTIONS,interact_type=INTERACT_TYPE).to(DEVICE)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(best_rnn_model.parameters()) + list(classification_network_rnn.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "train_loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader):\n",
    "        x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "        best_rnn_model.train()\n",
    "        classification_network_rnn.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_x1 = best_rnn_model(x1, length_x1,x1_mask)\n",
    "        outputs_x2 = best_rnn_model(x2,length_x2,x2_mask)\n",
    "        outputs = classification_network_rnn(outputs_x1,outputs_x2)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_hist.append(loss.item())\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            val_acc = test_model(val_loader, best_rnn_model,classification_network=classification_network_rnn)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(epoch+1,num_epochs,i+1,len(train_loader),loss.item()))\n",
    "            val_acc_hist.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            # validate\n",
    "    val_acc = test_model(val_loader, best_rnn_model,classification_network=classification_network_rnn)\n",
    "    val_acc_hist.append(val_acc)\n",
    "    print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "               epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(val_acc_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dicts_rnn = {\n",
    "    'embedding_network': best_rnn_model.state_dict(),\n",
    "    'classification_network': classification_network_rnn.state_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_data = pd.read_csv('hw2_data/mnli_train.tsv',sep='\\t')\n",
    "mnli_val_data = pd.read_csv('hw2_data/mnli_val.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Not entirely , ' I snapped , harsher than int...</td>\n",
       "      <td>I spoke more harshly than I wanted to .</td>\n",
       "      <td>entailment</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cook and then the next time it would be my tur...</td>\n",
       "      <td>I would cook and then the next turn would be h...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The disorder hardly seemed to exist before the...</td>\n",
       "      <td>The disorder did n't seem to be as common when...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Report and Order , in large part , adopts ...</td>\n",
       "      <td>The Report and Order ignores recommendations f...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IDPA 's OIG 's mission is to prevent , detect ...</td>\n",
       "      <td>IDPA 's OIG 's mission is clear and cares abou...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  'Not entirely , ' I snapped , harsher than int...   \n",
       "1  cook and then the next time it would be my tur...   \n",
       "2  The disorder hardly seemed to exist before the...   \n",
       "3  The Report and Order , in large part , adopts ...   \n",
       "4  IDPA 's OIG 's mission is to prevent , detect ...   \n",
       "\n",
       "                                           sentence2          label  \\\n",
       "0            I spoke more harshly than I wanted to .     entailment   \n",
       "1  I would cook and then the next turn would be h...  contradiction   \n",
       "2  The disorder did n't seem to be as common when...     entailment   \n",
       "3  The Report and Order ignores recommendations f...  contradiction   \n",
       "4  IDPA 's OIG 's mission is clear and cares abou...     entailment   \n",
       "\n",
       "        genre  \n",
       "0     fiction  \n",
       "1   telephone  \n",
       "2       slate  \n",
       "3  government  \n",
       "4  government  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'contradiction':0, 'entailment':2, 'neutral':1}\n",
    "mnli_train_data.replace({'label':label_map},inplace=True)\n",
    "mnli_val_data.replace({'label':label_map},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_data = prepare_data(mnli_train_data)\n",
    "mnli_val_data = prepare_data(mnli_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Not, entirely, ,, ', I, snapped, ,, harsher,...</td>\n",
       "      <td>[I, spoke, more, harshly, than, I, wanted, to, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[cook, and, then, the, next, time, it, would, ...</td>\n",
       "      <td>[I, would, cook, and, then, the, next, turn, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, disorder, hardly, seemed, to, exist, bef...</td>\n",
       "      <td>[The, disorder, did, n't, seem, to, be, as, co...</td>\n",
       "      <td>2</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, Report, and, Order, ,, in, large, part, ...</td>\n",
       "      <td>[The, Report, and, Order, ignores, recommendat...</td>\n",
       "      <td>0</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[IDPA, 's, OIG, 's, mission, is, to, prevent, ...</td>\n",
       "      <td>[IDPA, 's, OIG, 's, mission, is, clear, and, c...</td>\n",
       "      <td>2</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  ['Not, entirely, ,, ', I, snapped, ,, harsher,...   \n",
       "1  [cook, and, then, the, next, time, it, would, ...   \n",
       "2  [The, disorder, hardly, seemed, to, exist, bef...   \n",
       "3  [The, Report, and, Order, ,, in, large, part, ...   \n",
       "4  [IDPA, 's, OIG, 's, mission, is, to, prevent, ...   \n",
       "\n",
       "                                           sentence2  label       genre  \n",
       "0  [I, spoke, more, harshly, than, I, wanted, to, .]      2     fiction  \n",
       "1  [I, would, cook, and, then, the, next, turn, w...      0   telephone  \n",
       "2  [The, disorder, did, n't, seem, to, be, as, co...      2       slate  \n",
       "3  [The, Report, and, Order, ignores, recommendat...      0  government  \n",
       "4  [IDPA, 's, OIG, 's, mission, is, clear, and, c...      2  government  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres=mnli_val_data['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_rnn = []\n",
    "val_acc_cnn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data loader and the vocab remains same for the mnli dataset\n",
    "\n",
    "train_dataset = SNLIDataset(train_data, token2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genres)):\n",
    "    val_dataset_mnli = SNLIDataset(mnli_val_data[(mnli_val_data['genre'] == genres[i])][['sentence1','sentence2','label']], token2id)\n",
    "    val_loader_mnli = torch.utils.data.DataLoader(dataset=val_dataset_mnli,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "    val_acc_cnn.append(test_model(val_loader_mnli, best_cnn_model,classification_network=classification_network))\n",
    "    val_acc_rnn.append(test_model(val_loader_mnli, best_rnn_model,classification_network=classification_network_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.92462311557789,\n",
       " 41.791044776119406,\n",
       " 41.616766467065865,\n",
       " 41.732283464566926,\n",
       " 41.24236252545825]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.120603015075375,\n",
       " 44.17910447761194,\n",
       " 42.21556886227545,\n",
       " 47.539370078740156,\n",
       " 43.686354378818734]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mnli_acc = pd.DataFrame({'Genre':genres , 'accuracy_cnn':val_acc_cnn, 'accuracy_rnn':val_acc_rnn}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>accuracy_cnn</th>\n",
       "      <th>accuracy_rnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiction</td>\n",
       "      <td>44.924623</td>\n",
       "      <td>44.120603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>41.791045</td>\n",
       "      <td>44.179104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>41.616766</td>\n",
       "      <td>42.215569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government</td>\n",
       "      <td>41.732283</td>\n",
       "      <td>47.539370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>41.242363</td>\n",
       "      <td>43.686354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre  accuracy_cnn  accuracy_rnn\n",
       "0     fiction     44.924623     44.120603\n",
       "1   telephone     41.791045     44.179104\n",
       "2       slate     41.616766     42.215569\n",
       "3  government     41.732283     47.539370\n",
       "4      travel     41.242363     43.686354"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mnli_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "      Genre &  accuracy\\_cnn &  accuracy\\_rnn \\\\\n",
      "\\midrule\n",
      "    fiction &     44.924623 &     44.120603 \\\\\n",
      "  telephone &     41.791045 &     44.179104 \\\\\n",
      "      slate &     41.616766 &     42.215569 \\\\\n",
      " government &     41.732283 &     47.539370 \\\\\n",
      "     travel &     41.242363 &     43.686354 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_mnli_acc.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mnli_ft_val_acc = pd.DataFrame({'Genre':genres})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genres)):\n",
    "    df_mnli_ft_val_acc[genres[i]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>fiction</th>\n",
       "      <th>telephone</th>\n",
       "      <th>slate</th>\n",
       "      <th>government</th>\n",
       "      <th>travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiction</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre fiction telephone slate government travel\n",
       "0     fiction    None      None  None       None   None\n",
       "1   telephone    None      None  None       None   None\n",
       "2       slate    None      None  None       None   None\n",
       "3  government    None      None  None       None   None\n",
       "4      travel    None      None  None       None   None"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mnli_ft_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_network_cnn = copy.deepcopy(classification_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: fiction\n",
      "Epoch: [1/2], Step: [101/120], Training Loss: 0.8693899512290955\n",
      "Epoch: [1/2], Step: [101/120], Validation Acc: 52.76381909547739\n",
      "Epoch: [1/2], Step: [120/120], Validation Acc: 52.96482412060301\n",
      "Epoch: [2/2], Step: [101/120], Training Loss: 0.7448746562004089\n",
      "Epoch: [2/2], Step: [101/120], Validation Acc: 55.778894472361806\n",
      "Epoch: [2/2], Step: [120/120], Validation Acc: 53.969849246231156\n",
      "Genre: telephone\n",
      "Epoch: [1/2], Step: [101/134], Training Loss: 0.905422031879425\n",
      "Epoch: [1/2], Step: [101/134], Validation Acc: 54.527363184079604\n",
      "Epoch: [1/2], Step: [134/134], Validation Acc: 56.417910447761194\n",
      "Epoch: [2/2], Step: [101/134], Training Loss: 0.7695083618164062\n",
      "Epoch: [2/2], Step: [101/134], Validation Acc: 56.91542288557214\n",
      "Epoch: [2/2], Step: [134/134], Validation Acc: 55.52238805970149\n",
      "Genre: slate\n",
      "Epoch: [1/2], Step: [101/126], Training Loss: 1.0679054260253906\n",
      "Epoch: [1/2], Step: [101/126], Validation Acc: 50.99800399201597\n",
      "Epoch: [1/2], Step: [126/126], Validation Acc: 50.89820359281437\n",
      "Epoch: [2/2], Step: [101/126], Training Loss: 0.6547098755836487\n",
      "Epoch: [2/2], Step: [101/126], Validation Acc: 50.199600798403196\n",
      "Epoch: [2/2], Step: [126/126], Validation Acc: 51.097804391217565\n",
      "Genre: government\n",
      "Epoch: [1/2], Step: [101/122], Training Loss: 0.8574967384338379\n",
      "Epoch: [1/2], Step: [101/122], Validation Acc: 56.79133858267716\n",
      "Epoch: [1/2], Step: [122/122], Validation Acc: 57.874015748031496\n",
      "Epoch: [2/2], Step: [101/122], Training Loss: 0.7829476594924927\n",
      "Epoch: [2/2], Step: [101/122], Validation Acc: 55.610236220472444\n",
      "Epoch: [2/2], Step: [122/122], Validation Acc: 56.10236220472441\n",
      "Genre: travel\n",
      "Epoch: [1/2], Step: [101/125], Training Loss: 0.5286865830421448\n",
      "Epoch: [1/2], Step: [101/125], Validation Acc: 52.64765784114053\n",
      "Epoch: [1/2], Step: [125/125], Validation Acc: 54.378818737270876\n",
      "Epoch: [2/2], Step: [101/125], Training Loss: 0.5858780145645142\n",
      "Epoch: [2/2], Step: [101/125], Validation Acc: 53.66598778004073\n",
      "Epoch: [2/2], Step: [125/125], Validation Acc: 52.74949083503055\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for genre_i in range(len(genres)):\n",
    "    print('Genre: {}'.format(genres[genre_i]))\n",
    "    train_dataset_mnli = SNLIDataset(mnli_train_data[(mnli_train_data['genre'] == genres[genre_i])][['sentence1','sentence2','label']], token2id)\n",
    "    train_loader_mnli = torch.utils.data.DataLoader(dataset=train_dataset_mnli,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=snli_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset_mnli = SNLIDataset(mnli_val_data[(mnli_val_data['genre'] == genres[genre_i])][['sentence1','sentence2','label']], token2id)\n",
    "    val_loader_mnli = torch.utils.data.DataLoader(dataset=val_dataset_mnli,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "    \n",
    "    best_rnn_model.load_state_dict(original_state_dicts_rnn['embedding_network'])\n",
    "    classification_network_rnn.load_state_dict(original_state_dicts_rnn['classification_network'])\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(best_rnn_model.parameters()) + list(classification_network_rnn.parameters()), lr=learning_rate)\n",
    "    \n",
    "    train_loss_hist = []\n",
    "    val_acc_hist = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x1,x2,length_x1,length_x2,x1_mask,x2_mask,label) in enumerate(train_loader_mnli):\n",
    "            x1,x2,x1_mask,x2_mask,label = x1.to(DEVICE),x2.to(DEVICE),x1_mask.to(DEVICE),x2_mask.to(DEVICE),label.to(DEVICE)\n",
    "            best_rnn_model.train()\n",
    "            classification_network_rnn.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs_x1 = best_rnn_model(x1, length_x1,x1_mask)\n",
    "            outputs_x2 = best_rnn_model(x2,length_x2,x2_mask)\n",
    "            outputs = classification_network_rnn(outputs_x1,outputs_x2)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_hist.append(loss.item())\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                val_acc = test_model(val_loader_mnli, best_rnn_model,classification_network=classification_network_rnn)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format(epoch+1,num_epochs,i+1,len(train_loader_mnli),loss.item()))\n",
    "                val_acc_hist.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(epoch+1, num_epochs, i+1, len(train_loader_mnli), val_acc))\n",
    "                # validate\n",
    "        val_acc = test_model(val_loader_mnli, best_rnn_model,classification_network=classification_network_rnn)\n",
    "        val_acc_hist.append(val_acc)\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                   epoch+1, num_epochs, i+1, len(train_loader_mnli), val_acc))\n",
    "        \n",
    "    for genre_j in range(len(genres)):\n",
    "        val_dataset_mnli = SNLIDataset(mnli_val_data[(mnli_val_data['genre'] == genres[genre_j])][['sentence1','sentence2','label']], token2id)\n",
    "        val_loader_mnli = torch.utils.data.DataLoader(dataset=val_dataset_mnli,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=snli_collate_func,\n",
    "                                               shuffle=True)\n",
    "        df_mnli_ft_val_acc.at[genre_i,genres[genre_j]] = test_model(val_loader_mnli, best_rnn_model,classification_network=classification_network_rnn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3836"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_mnli.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>fiction</th>\n",
       "      <th>telephone</th>\n",
       "      <th>slate</th>\n",
       "      <th>government</th>\n",
       "      <th>travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiction</td>\n",
       "      <td>55.1759</td>\n",
       "      <td>54.9254</td>\n",
       "      <td>49.8004</td>\n",
       "      <td>54.0354</td>\n",
       "      <td>50.2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephone</td>\n",
       "      <td>55.3769</td>\n",
       "      <td>57.1144</td>\n",
       "      <td>50.7984</td>\n",
       "      <td>54.2323</td>\n",
       "      <td>51.6293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>54.9749</td>\n",
       "      <td>53.3333</td>\n",
       "      <td>51.2974</td>\n",
       "      <td>55.4134</td>\n",
       "      <td>51.3238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government</td>\n",
       "      <td>53.9698</td>\n",
       "      <td>54.2289</td>\n",
       "      <td>50.6986</td>\n",
       "      <td>56.6929</td>\n",
       "      <td>52.0367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>52.8643</td>\n",
       "      <td>55.1244</td>\n",
       "      <td>50.1996</td>\n",
       "      <td>56.7913</td>\n",
       "      <td>53.4623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Genre  fiction telephone    slate government   travel\n",
       "0     fiction  55.1759   54.9254  49.8004    54.0354  50.2037\n",
       "1   telephone  55.3769   57.1144  50.7984    54.2323  51.6293\n",
       "2       slate  54.9749   53.3333  51.2974    55.4134  51.3238\n",
       "3  government  53.9698   54.2289  50.6986    56.6929  52.0367\n",
       "4      travel  52.8643   55.1244  50.1996    56.7913  53.4623"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mnli_ft_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "      Genre &  fiction & telephone &    slate & government &   travel \\\\\n",
      "\\midrule\n",
      "    fiction &  55.1759 &   54.9254 &  49.8004 &    54.0354 &  50.2037 \\\\\n",
      "  telephone &  55.3769 &   57.1144 &  50.7984 &    54.2323 &  51.6293 \\\\\n",
      "      slate &  54.9749 &   53.3333 &  51.2974 &    55.4134 &  51.3238 \\\\\n",
      " government &  53.9698 &   54.2289 &  50.6986 &    56.6929 &  52.0367 \\\\\n",
      "     travel &  52.8643 &   55.1244 &  50.1996 &    56.7913 &  53.4623 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_mnli_ft_val_acc.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
